
# Capitolo 4: Validazione degli Algoritmi di Analisi dell'Immagine Biomedica
Un punto fondamentale nello sviluppo di algoritmi per l’analisi dell’immagine biomedica è la validazione di detti algoritmi. Per validazione si intende un processo che assicuri il “buon funzionamento” dell’algoritmo sviluppato nella pratica clinica, limitandosi ovviamente all’insieme di casi per cui l’algoritmo è stato sviluppato. Un buon algoritmo di analisi dell’immagine:

1. Deve dare risultati **corretti**, cioè misurare correttamente i parametri oggetto dell’analisi.  
2. Deve dare risultati **riproducibili**, cioè se applicato più volte allo stesso set di immagini deve dare risultati molto simili tra loro.

Per quanto riguarda il primo punto, nel processo di validazione si confrontano i risultati dell’algoritmo con un **“gold standard”**, **“ground truth”**, o **“reference standard”**, che rappresenta il risultato corretto da raggiungere. Le recenti CLAIM guidelines (https://pubs.rsna.org/doi/10.1148/ryai.240300) sviluppate nel campo dell’intelligenza artificiale applicata alla radiologia suggeriscono fortemente l’uso del termine **“reference standard”**. La motivazione è che “ground truth” and “gold standard” suggeriscono erroneamente che il riferimento (reference) sia noto con precisione, cosa che in medicina è estremamente raro. Il termine reference standard è quindi più appropriato. 

Meno il risultato fornito dall’algoritmo si discosta dal reference standard, migliore è il suo funzionamento. L’errore accettabile dipende dal quesito clinico. Per la misura della temperatura corporea un errore del 10% è incettabile, in quanto a un paziente con temperatura normale (37 C°) verrebbe diagnosticato uno stato patologico (40.7 C°) che richiede l’adozione di terapie salvavita. Nel caso della misura del volume ventricolare sinistro in un paziente maschio (range di normalità 67-155 ml), un errore del 10% in un paziente bordeline (155 ml) porta ad un valore di 170 ml che corrisponde ad una diagnosi di lieve anomalia (range 156-178 ml), che può essere accettabile. Infatti un errore intorno al 10% è considerato accettabile in ecografia cardiaca. Quindi non è possibile dare una soglia accettabile di errore ma questa va stabilita sulla base del quesito clinico al quale la procedura deve rispondere. Tipicamente, come sarà dettagliato nel seguito, un software può essere utilizzato clinicamente se l’errore commesso dal software è comparabile a quello commesso da un operatore esperto seguendo le linee guida approvate.          

Un punto fondamentale nella valutazione dell’errore è quindi la definizione del “reference standard”. In teoria il reference standard dovrebbe rappresentare una misura oggettiva e certa della quantità da misurare. Tuttavia, nella maggioranza dei casi detta quantità non sarà nota con precisione ma con un certo errore sperimentale. 

Un primo approccio per risolvere il problema è quello di utilizzare per la validazione dell’algoritmo delle immagini ottenuta da oggetti noti, cioè **fantocci (phantom)**, oppure generare immagini sintetiche simili alle immagini reali (**fantoccio sintetico o in silico**). Nel primo caso le caratteristiche fisiche del fantoccio saranno note dai suoi dati costruttivi, nel secondo dai parametri noti della procedura di simulazione.
Il limite di questo approccio è evidentemente il fatto che un fantoccio reale o sintetico non sarà in grado di rappresentare pienamente immagini reali. L’uso di fantocci è comunque estremamente utile nella fase di sviluppo dell’algoritmo per affinarne le potenzialità. 

Nel caso di immagini reali, possiamo utilizzare come riferimento una misura ottenuta con la tecnica che rappresenta il meglio della tecnologia disponibile. Ad esempio, se lo scopo dell’algoritmo sotto esame è diagnosticare l'estensione dell'infarto miocardico, potremmo utilizzare come riferimento una misura istologica dell'estensione dell'infarto sul cuore espiantato (Figura 4.1). 

<img src="./images/image-1.png" alt="Esempio DE vs istologia" style="width:100%;">

*Figura 4.1. Esempio di confronto tra dati MRI (LGE-CMR) e istologia nel topo.*

In Figura 4.1 (Bohl S et al Am J Physiol Heart Circ Physiol. 2009 April; 296(4): H1200–H1208) è mostrato un esempio di confronto tra dati MRI (LGE-CMR) e istologia nel topo.
Questo tipo di valutazione comporta una serie di problemi di tipo tecnico, quali la contrarietà del paziente a farsi espiantare l’organo oggetto dell’esame e la difficoltà di individuare una corrispondenza precisa tra localizzazione dei tessuti ex-vivo ed in-vivo. Il suo utilizzo è di solito limitato al modello animale, per quanto siano possibili applicazioni sull'uomo, ad esempio nell'imaging di organi o tessuti che vengono poi rimossi per via chirurgica.

Il reference standard può essere anche rappresentato da una modalità di imaging “invasiva” rispetto ad una modalità di imaging “non invasiva”. Un esempio tipico è la valutazione della stenosi coronarica attraverso misure di perfusione miocardica. Le misure di perfusione rappresentano una valutazione indiretta del rischio coronarico, in quando una coronaria stenotica è associata ad una riduzione del flusso e quindi della perfusione miocardica, soprattutto in condizione di stress indotto. In questo caso i pazienti che vengono valutati con un significativo livello di rischio nella modalità di imaging non invasiva (MRI, SPECT, PET) che valuta la perfusione vengono sottoposti ad angiografia digitale ed eventualmente se la diagnosi è confermata ad angioplastica, ed è quindi disponibile un set di dati comparati dove l'angiografia rappresenta il reference standard. 
Infine, il reference standard può essere rappresentato da una modalità di imaging non invasiva che allo stato dell'arte fornisce la migliore qualità diagnostica. Ad esempio, può essere di interesse valutare la capacità della ecocardiografia di ottenere a costi significativamente minori la stessa qualità diagnostica nella valutazione dei parametri di funzione cardiaca della risonanza magnetica. 
Infine, è possibile prendere come riferimento l’**analisi manuale** fatta da un operatore esperto, tipicamente un radiologo. In questo scenario, il radiologo effettua un’analisi di tipo manuale ad esempio definendo i contorni di un organo con il mouse su di una workstation. La regione segmentata viene presa come riferimento per la valutazione dell’algoritmo di segmentazione sviluppato. Il confronto con l’analisi manuale è la tecnica di validazione più diffusa per la sua facilità di implementazione, infatti spesso basta usare come riferimento l’analisi manuale fatta a fini diagnostici che è a costo zero. Ha tuttavia due limiti fondamentali: 

1. L’analisi non è veramente manuale, visto che comunque viene utilizzato un software che condiziona l’operato dell’utente esperto; 
2. I risultati dell’analisi manuale, e quindi il reference standard, dipendono dall’operatore.

Quindi, se il reference standard è rappresentato da una segmentazione manuale, esso dipenderà dell’osservatore che compie la misura. Il reference standard avrà quindi una variabilità dovuta alla variabilità inter- ed intra-osservatore. Supponiamo di avere un indice $SIM$ che calcola la similarità (o la differenza) tra due misure. Avendo due osservatori avremo quindi due “reference standard” $G_1$ e $G_2$, per i quali sarà definito ad esempio un indice $SIM(G_1,G_2)$, che rappresenta una misura della variabilità inter-osservatore. Confrontando il risultato della misura automatica S con il reference standard avremo due valori $SIM(S,G_1)$ e  $SIM(S,G_2)$ che rappresentano la differenza tra l’algoritmo automatico e i due osservatori umani. L’algoritmo di segmentazione sarà efficace se i tre indici sono tra loro simili, cioè se la procedura automatica non è distinguibile da un osservatore umano. Vedendo la cosa dal punto di vista grafico, considerando l’indice $SIM$ come una distanza spaziale l’algoritmo e i due osservatori devono trovarsi ai vertici di un triangolo equilatero (Figura 4.2). 
La validazione viene sempre effettuata su base statistica, quindi non su di una sola immagine ma su di una serie di immagini dello stesso tipo, in modo da estendere al valutazione stessa ad un set di immagini rappresentativo della pratica clinica in cui ci si aspetta di utilizzare l’algoritmo.

<img src="./images/image-2.png" alt="Triangolo algoritmo/osservtori" style="width:100%;">

*Figura 4.2. Rappresentazione grafica di un'indice di similarità.*

## Valutazione degli algoritmi di segmentazione
Come detto in precedenza, per valutare l’efficacia di un algoritmo di segmentazione il risultato dell’algoritmo, tipicamente una maschera, viene confrontato con un riferimento “reference standard”. Il reference standard è tipicamente ottenuto attraverso una segmentazione manuale operata da un utente esperto. Date le due maschere da confrontare, è possibile valutare degli indici numerici che misurano la bontà della segmentazione. Gli indici più usati sono l’indice di **Dice**, introdotto da Lee Raymond Dice nel 1945 nel campo dell’ecologia, e l’indice di **Jaccard** (1901).   
Detta S la maschera ottenuta dalla segmentazione e G la maschera che rappresenta il reference standard, l’indice di Jaccard è definito come:

$$
J = \frac{|S \cap G|}{|S \cup G|}
$$

$|S \cap G|$ indica la numerosità dell’intersezione tra le due maschere, ed è quindi uguale al numero di pixel in comune tra le due maschere. È detta anche overlapping area ($OA$). $|S \cup G|$ è il numero di pixel comuni ottenuto dall’unione delle due maschere.  
L’indice di Jaccard è uguale a 1 se le due maschere sono perfettamente coincidenti e 0 se le due maschere sono disgiunte. L’indice può essere naturalmente esteso al caso 3D in modo immediato.
L’indice di Dice è definito come:

$$
DSC = \frac{2|S \cap G|}{|S| + |G|}
$$

ed è quindi uguale al numero di pixel in comune tra le due maschere, diviso per la media della numerosità delle maschere. Anche in questo caso si ha $DSC=1$ per la perfetta segmentazione e $DCS=0$ per $OA=0$. La relazione tra i due indici è definita come: 

$$
J = \frac{DSC}{2-DSC}
$$

$$
DSC = \frac{2J}{1+J}
$$

I due indici si possono anche esprimere in termini delle quattro categorie:
1. $TP/VP$ (Veri Positivi). 	Un pixel è incluso sia in $S$ che in $G$
2. $TN/VN$ (Veri negativi). 	Un pixel non è incluso sia in $S$ che in $$G$ 
3. $FP/FP$ (Falsi Positivi). 	Un pixel è incluso in $S$ ma non in $G$
4. $FP/FN$ (Falsi Negativi). 	Un pixel non è incluso in $S$ ma è incluso in $G$

$$
J = \frac{TP}{TP+FP+FN}
$$

$$
DSC = \frac{2TP}{2TP+FP+FN}
$$

Come illustrato in Figura 4.3, l’indice di Jaccard è più “severo” rispetto all’indice di Dice, in pratica l’indice di Dice assegna un valore doppio ai pixel True Positive (TP) dove l’algoritmo sotto esame ed il Gold Standard coincidono. La cosa ha senso perché l’utente tipicamente concentra la sua attenzione sull’oggetto da segmentare e non sullo sfondo.

<img src="./images/image-3.png" alt="Confronto Dice Jaccard" style="width:100%;">

*Figura 4.3. Confronto tra l'indice di Jaccard e l'indice di Dice.*

Gli indici così definiti sono validi per una singola maschera. Se l’obiettivo della segmentazione è individuare più pattern, è possibile utilizzare l’object-level DICE/Jaccard index. Supponiamo di voler segmentare $N_G$ oggetti di cui sono note le maschere “gold standard” $G_k \quad (k=1,...,N_G)$ e che l’algoritmo di segmentazione restituisca $N_S$ maschere $S_q \quad (q=1,...,N_S)$. Per ogni $S_i$ definiamo $G_i$ come la maschera appartenente all’insieme $G$ di tutte le maschere gold standard che ha il massimo overlap con $S_i$.  In pratica $G_i$ è la maschera gold standard che assomiglia di più a $S_i$. Definiamo poi in modo similare $\hat{S}_i$  come l’elemento di $S$ che ha il massimo overlap con $\hat{G}_i$. Si ha quindi:

$$
DICE_{object}(G,S) = \frac{1}{2} \left[ \sum_{i=1}^{N_S} \omega_i DICE(G_i, S_i) + \sum_{i=1}^{N_G} \varphi_i DICE(\hat{G}_i, \hat{S}_i) \right]
$$

L’indice DICE object-level è ottenuto come media della somma pesata degli indici DICE tra gli oggetti segmentati e gli oggetti gold standard più somiglianti e la somma pesata degli indici DICE tra gli oggetti gold standard e gli oggetti segmentati più somiglianti. Si noti che se $N_S=N_G$ , cioè se l’algoritmo di segmentazione riconosce il giusto numero di oggetti, la formula diventa:

$$
DICE_{object}(G,S) = \frac{1}{2} \left[ \sum_{i=1}^{N_S} \omega_i DICE(G_i, S_i) + \sum_{i=1}^{N_G} \varphi_i DICE(\hat{G}_i, \hat{S}_i) \right]
$$

i pesi $\omega _i$ vengono calcolati come:

$$
\omega_i = \frac{|S_i|}{|S|} \qquad \varphi_i = \frac{|\widehat{G}_i|}{|G|}
$$

Cioè il peso è il numero di pixel dell’oggetto diviso il totale dei pixel da segmentare, in modo da pesare maggiormente l’indice DICE degli oggetti più grandi. La formulazione per $J$ è analoga.
In MATLAB gli indici di Dice e Jaccard possono essere calcolati con le funzioni `dice` e `jaccard`.
In python il calcolo degli indici può essere fatto con le funzioni `dice` e `jaccard` nel pacchetto **scipy.spatial.distance**.

Per una dimostrazione delle funzioni Python: {doc}`dice-jaccard-demonstration.ipynb`

In alternativa al confronto delle maschere, è possibile confrontare i contorni delle regioni segmentate. In questo caso, detto CS il contorno prodotto dall’algoritmo di segmentazione e CG il contorno preso come gold standard, si definiscono la precisione ($P$, precision) e il richiamo ($R$, recall) come:

$$
P = \frac{1}{|C_S|} \sum_{i=1}^{N_S} \llbracket d(C_S(i), C_G) < \vartheta \rrbracket, \quad R = \frac{1}{|C_G|} \sum_{i=1}^{N_G} \llbracket d(C_G(i), C_S) < \vartheta \rrbracket
$$


dove $d()$ è la distanza euclidea tra un punto $i$ del contorno (ad esempio $C_S$) e l’altro contorno (ad esempio $C_G$), intesa come il minimo delle distanze tra il punto $C_S(i)$ e tutti i punti di $C_G$.  $\vartheta$ è un valore di soglia, pari allo $0.75\%$ della diagonale dell’immagine nella formulazione originale dell’algoritmo (Csurka et al Proceedings of the British Machine Vision Conference, 2013, pp. 32.1-32.11). In pratica $P$ è la percentuale di punti del contorno segmentato con una distanza dal gold standard minore di $\vartheta$, mentre $R$ è la percentuale di punti del contorno gold standard con una distanza dal contorno segmentato minore di $\vartheta$. La misura $F_1$ che definisce la simiglianza dei due contorni è:

$$
F_1 = \frac{2PR}{P + R}
$$

Questa metrica detta $BF$ (Boundary $F_1$) contour matching score è implementata in MATLAB dalla funzione `bfscore`. L’implementazione python è disponibile a https://github.com/minar09/bfscore_python. 

Come si vedrà in seguito, gli indici di $DICE$, $Jaccard$ e $F_1$ trovano uso negli algoritmi di machine learning supervisionati come funzioni loss e come indici di valutazione della bontà del modello sviluppato. 

## Misure statistiche di precisione e riproducibilità
Gli indici definiti in precedenza valutano la qualità della segmentazione, che però nella pratica clinica rappresenta quasi sempre uno step intermedio per valutare un indice diagnostico. Ad esempio la segmentazione del ventricolo sinistro del cuore da immagini US, MR o CT permette di valutare il volume ventricolare in diastole e sistole e quindi la frazione di eiezione (EF) che misura l’efficienza della pompa cardiaca. E’ quindi necessario valutare l’efficacia dell’algoritmo di segmentazione nel misurare questi indici.
Come detto in precedenza, la valutazione di un algoritmo di segmentazione si basa su prove ripetute su di un certo numero di immagini considerate rappresentative delle immagini su cui verrà utilizzato l’algoritmo. Supponiamo di voler valutare la congruenza di due metodi nell’analisi di un certo tipo di immagini. Prendiamo $K$ immagini e utilizziamo il metodo 1 per estrarre dalle $K$ immagini $K$ indici ed un secondo metodo per effettuare la stessa procedura senza conoscere i risultati del primo metodo. Avremo una prima serie di indici $x_1,x_2,...,x_K$ ed una seconda serie $y_1,y_2,...,y_K$. In teoria, gli indici dovrebbero essere uguali a coppie $(x_1=y_1, x_2=y_2,...)$.
I due metodi possono corrispondere a due operatori umani, ed in questo caso il confronto delle misure fornisce una stima della **variabilità** (o **riproducibilità**) **inter-osservatore**. Se i due metodi rappresentano le misure fatte in tempi diversi dallo stesso osservatore umano viene stimata la variabilità **intra-osservatore**. Se il confronto è tra un algoritmo ed un reference standard il confronto delle misure fornisce una stima della **correttezza** o **precisione** dell’algoritmo. 

1. Osservatore 1 vs Osservatore 1	--> Riproducibilità Intra-osservatore
2. Osservatore 1 vs Osservatore 2	--> Riproducibilità Inter-osservatore
3. Algoritmo vs Osservatore	--> Precisione dell’algoritmo
4. Algoritmo (test 1) vs Algoritmo (test 2)	-- > Riproducibilità dell’algoritmo

Se il funzionamento dell’algoritmo dipende da un input dell’osservatore (algoritmo semi-automatico) o da fattori casuali (condizioni iniziali random) si definisce la riproducibilità dell’algoritmo confrontando i risultati di due o più test successivi sugli stessi dati di ingresso. Un algoritmo totalmente automatico (**unsupervised**), cioè che non dipende in nessun modo dall’input dell’utente può essere perfettamente riproducibile. 
Per confrontare le misure si possono utilizzare diversi indici.

Il primo indice utilizzato è il cosiddetto **Coefficente di Variazione** (Coefficient of Variation, $CoV$ o $CV$). Il $CoV$ valuta quanto pesa percentualmente la differenza tra due misure ed è misurato come la deviazione standard del vettore degli errori percentuali assoluti sulle singole misure.  
Valuteremo quindi l'errore percentuale assoluto come $epi = 100*|x_i-y_i|/MEAN(x_i,y_i)$ e poi calcoleremo la deviazione standard del vettore $epi$. $MEAN(x_i,y_i)$ rappresenta una stima del valore “vero” dell’indice che viene misurato. Se una delle due misure è un reference standard, al posto della media delle misure avremo la misura effettuata con il reference standard che si considera quella più vicina al valore “vero”. Una $CoV< 10\%$ è di solito considerata accettabile in campo medico.
Questo tipo di misura ci fornisce l’errore percentuale atteso che commette un osservatore o un algoritmo quando esamina due volte la stessa immagine. Non ci dice però quanto è grave l’errore da un punto di vista diagnostico, in quanto ad esempio un errore del 10% può essere trascurabile su di un indice che varia del 100% tra un caso normale ed uno patologico mentre è grave se la variazione è ad esempio del 20%. Inoltre la $CoV$ non tiene conto della differenza tra errore casuale ed errore sistematico. Per errore sistematico intendiamo il fatto che uno dei due operatori sottostimi o sovrastimi sempre l’indice estratto. L’errore sistematico è meno grave dell’errore casuale, in quanto mantiene l’ordine degli indici estratti che è un fattore diagnostico importante. 

Un altro indice spesso utilizzato nella comparazione di indici diagnostici è la **correlazione** tra le due misure. Ipotizziamo che esista una relazione lineare tra le osservazioni $X=(x_1,x_2,...,x_K)$ e $Y=(y_1,y_2,...,y$K)$.  
Avremo quindi:

$$
y = a + bx_i + \epsilon _i
$$

Quindi la variabile dipendente $y$ viene "spiegata" attraverso una relazione lineare della variabile indipendente $x$ (cioè: $a+bx$) e da una quantità casuale $\epsilon _i$  che rappresenta il rumore della misura. L’indice a rappresenta il fatto che un operatore aggiunge una certa quantità rispetto all’altro, il fattore $b$ il fatto che un operatore sovrastima l’indice in una certa misura. Il ben noto problema della regressione si traduce nella determinazione di $a$ e $b$ in modo da esprimere al ‘meglio’ la relazione funzionale tra $Y$ e $X$. Tipicamente si calcolano i coefficienti $a$ e $b$ secondo il metodo dei minimi quadrati utilizzando opportuni software statistici. E’ possibile estendere il metodo a più variabili (caso multivariato).

Si deriva quindi una retta che interpola uno scatter di punti minimizzando la somma dei quadrati delle distanze  $\epsilon _i$ dei punti stessi dalla retta (Figura 4.4).

<img src="./images/image-4.png" alt="Correlazione" style="width:100%;">

*Figura 4.4. Esempio di retta di regressione. *

In questo esempio il confronto è tra due osservatori, uno umano (manual) ed uno automatico (auto). Il fatto che le misure siano vicine alla retta di regressione da una informazione visuale sul fatto che i due osservatori forniscono risultati simili. In questo caso la misura manuale funge da “gold standard”. La variabilità inter- e intra-osservatore si calcolano in modo analogo. Chiaramente oltre alla rappresentazione grafica è utile avere un indice numerico della bontà del fitting. Il coefficiente di determinazione ($R^2$), è una misura della bontà dell'adattamento (in inglese fitting) della regressione lineare stimata ai dati osservati. $R^2$ misura la frazione della variabilità delle osservazioni yi che siamo in grado di spiegare tramite il modello lineare. Bisogna notare che $R^2$ non misura se effettivamente sussista una relazione (di qualsiasi tipo) tra le $y_i$ e i regressori, ma soltanto fino a che punto un modello lineare consente di approssimare la realtà dei dati osservati; un modello non lineare, ad esempio, potrebbe meglio rappresentare la relazione tra variabile dipendente e regressori, e presentare un buon potere esplicativo, anche in presenza di un $R^2$ prossimo allo zero. Nella retta di regressione in Figura 4.4 ad esempio $r=0.93$. Un altro parametro importante è la pendenza della retta $b$, che idealmente deve essere uguale ad 1. Se $b$ è diverso da uno, uno dei due metodi sottostimerà o sovrastimerà la misura rispetto all’altro. L’intercetta a dovrebbe essere nulla, se non lo è esiste una differenza sistematica tra le misure.  

Un'altra rappresentazione grafica molto popolare è il cosiddetto **Bland-Altman** plot. In Figura 4.5 vediamo il grafico per le due serie di dati per cui era stata tracciata la retta di regressione. Nel Bland-Altmann plot, in ascisse abbiamo la media delle due misure, ed in ordinata la differenza delle stesse. La media delle differenze è riportata come una riga continua, e permette di stimare se una delle due metodiche sottostima o sovrastima l’indice rispetto all’altra (bias). In assenza di bias, la riga continua dovrebbe coincidere con lo zero dell’asse y. In questo caso vediamo che la metodica automatica da una stima inferiore rispetto a quella manuale. Le due righe indicate con $1.96SD$ e $-1.96SD$ sono ottenute calcolando la deviazione standard della differenza tra le misure. Se la distribuzione delle differenze è gaussiana il $95\%$ dei dati cade nell’area tra le due linee. In questo caso (errore gaussiano) ci aspettiamo che l’errore cada nell’intervallo di confidenza. La distanza tra le due linee da una misura della congruenza delle due misure fatta la correzione per l’errore sistematico. Minore è la distanza tra le due linee, migliore è la concordanza tra le due misure.

<img src="./images/image-5.png" alt="BA Plot" style="width:100%;">

*Figura 4.5. Bland-Altman plot.*

Il Bland-Altman plot può essere usato solo per confrontare misure della stessa natura. Nel caso il confronto sia fatto con un reference standard, sull’asse $x$ viene riportato il reference standard e non la stima fatta con la media delle misure. 

## Test di significatività

Di larghissimo uso nella valutazione delle metodiche per l'estrazione di indici clinici è l'indice $p$, che rappresenta la probabilità di accettazione dell'ipotesi nulla, e cioè che non esista alcuna differenza tra le due misure riguardo al parametro considerato. In altre parole, secondo l'ipotesi nulla le due misure sono fra loro uguali e le eventuali differenze osservate vanno attribuite al solo caso. Si comprende l'importanza da un punto di vista diagnostico di questa misura, in quando due misure statisticamente uguali possono essere usate in modo interscambiabile nella valutazione clinica.
L'ipotesi zero può essere accettata o respinta applicando un **test statistico di significatività**, il cui risultato - in genere - va confrontato con un valore critico tabulato in apposite tabelle. Se il risultato del test di significatività supera il valore critico, allora la differenza fra i gruppi viene dichiarata **statisticamente significativa** e, quindi, l'ipotesi zero viene respinta. In caso contrario l'ipotesi zero viene accettata. Questo fatto viene espresso dal parametro $p$ che esprime la probabilità che l’ipotesi zero sia falsa. Se $p$ è molto basso (valori tipici sono $p<0.01$, $p<0.05$) si può concludere che i due gruppi sono equivalenti. Nel caso delle misure di riproducibilità e di validazione, l’ipotesi zero sarà che la differenza tra gli indici misurati dai due osservatori sia dovuta solo al caso, e questa ipotesi sarà confermata da un $p<0.05$ o $p<0.01$ a seconda della soglia scelta. 
Numerosi test statistici vengono usati per determinare con un certo grado di probabilità l'esistenza (o l'assenza) di differenze significative nei dati in esame o meglio, più in generale, di accettare o rigettare una ipotesi zero. 

Un test molto usato è il **paired-t-test**, che si applica quando la misura è effettuata da due osservatori sulla stessa immagine (o sullo stesso paziente). Il paired-t-test valuta se la differenza tra le due misure è statisticamente significativa rispetto al valore della misura stessa. I casi visti fino a ora (valutazione vs. gold standard e valutazione dell'intra- e inter-observer variability) possono essere oggetto di una analisi con il paired-t-test.    

In Figura 4.6 i risultati di un paired-t-test su due set di dati (misura della variabilità tra due programmi software per la misura dell'accumulo di ferro nel paziente talassemico). 

<img src="./images/image-6.png" alt="Paired t-test" style="width:100%;">

*Figura 4.6. Esempio di analisi tramite paired t-test.*

Le misure sono state effettuate su un set di 20 immagini con i due programmi, quindi viene applicato il paired-t-test. Nei pazienti senza accumulo $p=0.0776$, quindi non c'è differenza significativa tra le due misure. Notiamo che il paired-t-test è molto sensibile, in quanto una differenza di 0.55 su valori medi di 38.5 (1.5%) rischia di creare una significatività. Questo perché il test è sensibile all'errore sistematico. Per confronto la correlazione lineare tra gli stessi dati da $r=0.98, \quad p< 0.0001$. Nei pazienti con accumulo severo c'è differenza significativa ($p<0.0001$).  

## Misura dell’efficacia diagnostica

La validità di un metodo di analisi può anche essere valutata in base all’efficacia nell’identificare una malattia. In questo caso avremo $K$ immagini, acquisite da $K$ pazienti di cui un certo numero $M$ presentano una certa malattia e gli altri ($S$) sono sani. L’operatore esaminerà le varie immagini e fornirà un giudizio binario sulla presenza o meno della malattia. Avremo quattro possibilità:

1. VP (Veri Positivi). 	Viene identificata la malattia ed il soggetto è effettivamente malato.
2. VN (Veri negativi). 	Non viene identificata la malattia ed il soggetto è sano.
3. FP (Falsi Positivi). 	Viene identificata la malattia ma il soggetto è sano.
4. FN (Falsi Negativi). 	Non viene identificata la malattia ma il soggetto è malato.

O come in Figura 4.7, attraverso una **confusion matrix**. In questo esempio abbimo $VP=100$, $VN=80$, $FP=21$, $FN=11$.

<img src="./images/image-15.png" alt="Confuzion Matrix" style="width:100%;">

*Figura 4.7. Confusion matrix.*

Nel caso ideale dovremmo avere $VP=M$, $VN=S$, $FN=FP=0$.
La rappresentazione tabellare è anche detta “**confusion matrix**” o “**matrice di confusione**” ed è correntemente usata per la valutazione di decisori sviluppati con tecniche AI di machine learning.  

Si definiscono:

$$
Sensitività = \frac{VP}{VP+FN}
$$ 

E’ la capacità del metodo di scoprire la patologia. In altri termini è la percentuale di diagnosi giuste fatte sui soggetti malati.

$$
Specificità = \frac{VN}{VN+FP}
$$ 

E’ la capacità dell’immagine di fare una diagnosi corretta sui pazienti sani. Infine:

$$
Accuratezza = \frac{VN+VP}{VN+VP+FN+FP}
$$ 
 
Descrive il numero di diagnosi corrette.

Un modo alternativo di visualizzare queste misure sono le curve ROC (Figura 4.8). Una curva ROC è una curva che lega la sensitività alla specificità in base ad una soglia scelta per l’indice considerato. 

<img src="./images/image-7.png" alt="Curva ROC" style="width:100%;">

*Figura 4.8. Curva ROC.*

Per tracciare una curva ROC prenderemo un vettore $X=(x_1,x_2,...,x_N)$ continuo. Ad esempio $x$ potrebbe essere una misura di perfusione nel miocardio. Avremo poi un indice $I=(i_1,i_2,...,i_N)$ binario, che indica se il soggetto ha o non ha una certa patologia (ad esempio una stenosi coronarica). Facciamo variare una soglia $T$ e definiamo patologici i soggetti con $X<T$ e sani quelli con $X>T$. Avremo dei valori di sensitività e specificità che riporteremo nel grafico tracciando la curva ROC. Notiamo che avremo sempre un punto nell’origine, corrispondente a sensitività 0 e specificità 1. Questo punto equivale ad un osservatore che risponde sempre $N_O$. L’osservatore che risponde sempre $S_I$ corrisponde al punto in alto a destra. Tutti i punti della curva corrispondono ad un compromesso tra sensitività e specificità, il punto migliore in generale dipende dal quesito clinico e dal rapporto costo/beneficio. E’ possibile comunque calcolare il punto con la massima accuratezza. Una curva corrispondente alla diagonale indica che l’indice non dà informazioni sulla patologia. L’area della curva può essere usato come fattore di merito per la metodologia, e quindi in questo caso per il metodo di analisi dell’immagine.    
Anche in questo caso il ground truth, cioè le diagnosi sano/malato potranno essere affette da variabilità inter- ed intra-osservatore. I valori di accuratezza, sensitività e specificità ottenuti da un software automatico dovranno quindi essere confrontati con la riproducibilità del riferimento. 

## Valutazione di un software ad uso diagnostico

Come detto in precedenza, la validazione di un metodo di analisi automatico o semi-automatico comporta il confronto delle misure ottenute con il metodo in oggetto con un “gold standard” clinico, tipicamente la segmentazione manuale effettuata da un operatore esperto. Un altro punto importante nella valutazione di un software di analisi dell'immagine biomedica è la misura della riproducibilità, cioè della capacità del software di assicurare un bassa variabilità inter- e intra-osservatore. Se il software è completamente automatico la variabilità inter e intra-osservatore è zero, a meno della possibile variabilità indotta dalla componente random dell'algoritmo, ad esempio a causa dell’uso di un algoritmo di clustering. 

Come esempio consideriamo il programma HIPPO FAT per la valutazione del grasso addominale. Il programma permette di misurare in modo automatico i valori di grasso viscerale (VAT) e subcutaneo (SAT) da immagini di risonanza T1-pesate (Positano V, JMRI 2004). Per la validazione sono stati selezionati 20 pazienti con vario livello di obesità. Per ogni paziente sono state acquisite 32 slice assiali con una sequenza pesata T1. I volumi di SAT e VAT sono stati valutati con il software e manualmente da un operatore esperto. In tutto quindi sono state valutate 640 immagini (32x20). L'analisi con Bland-Altman plot (Figura 4.9) mostra come la correlazione tra misura automatica e manuale sia buona. La correlazione risulta $r=0.99, \quad p<0.0001$ per il SAT e $r=0.96, \quad p<0.0001$ per il VAT.


<img src="./images/image-8.png" alt="BA Plot HIPPO FAT" style="width:100%;">

*Figura 4.9. Bland-Altman plot della stima dei volumi di SAT e VAT.*

Dai BA plot si nota come ci sia una leggera sovrastima del SAT (intorno al 6%). Questo può essere dovuto all'inclusione errata di parti di VAT da parte dello snake interno o al fatto che lo snake esterno converge leggermente all'esterno del SAT per lo smoothing dei contorni introdotto dall'edge map. Per il VAT abbiamo una sottostima intorno al 8% ma una maggiore ampiezza dell'intervallo di confidenza, che conferma come la misura del VAT sia più critica. La sottostima è ragionevolmente dovuta al fatto che l'algoritmo automatico non tiene conto dell'effetto volume parziale, in quanto il fitting dell'istogramma viene effettuato con una gaussiana. Il problema potrebbe essere risolto introducendo l'effetto PV nel modello di fitting.
Una cosa positiva è che l'errore non è dipendente dal valore del VAT o del SAT. 
La validazione effettuata presenta alcuni limiti intrinseci:
1. È effettuata dagli autori del metodo, che possono avere un bias nella valutazione
2. È limitata ad immagini acquisite in un singolo centro e con una singola macchina MR, non c'è garanzia che i risultati ottenuti siano validi su altri tipi di immagini. 

È quindi opportuno procedere ad una valutazione che sia indipendente, cioè effettuata da valutatori esterni, e inter-centro, cioè che coinvolga immagini acquisite in centri diversi e con apparecchiature diverse. Una validazione di questo tipo è richiesta per la certificazione del software per uso clinico. 

Esistono varie validazioni indipendenti del software.  In quella eseguita presso la Washington University School of Medicine, St. Louis, Missouri, USA [^1] sono state esaminate le immagini di 10 soggetti, analizzate con HIPPO FAT e ANALYZE (Figura 4.10). Le immagini sono state acquisite con scanner Siemens. ANALYZE è un tool generico per l'analisi delle immagini biomediche che permette di definire manualmente le regioni di interesse e di effettuare all'interno delle stesse una segmentazione a soglia modificando la soglia stessa e osservando la maschera risultante. L'analisi è stata effettuata da due operatori per valutare la variabilità inter- e intra-osservatore. Se necessario, i risultati della segmentazione tramite HIPPO sono stati corretti manualmente. 

<img src="./images/image-9.png" alt="Analyze vs. HIPPO FAT" style="width:100%;">

*Figura 4.10. Confronto HIPPO vs ANALYZE (1).*

[^1]: Arif, Hassan, Susan B. Racette, Dennis T. Villareal, John O. Holloszy, And Edward P. Weiss. Comparison of methods for assessing abdominal adipose tissue from magnetic resonance images. Obesity. 2007;15:2240 –2244.

Un primo set di risultati mostra la variabilità inter-osservatore (qui chiamata  inter-rater agreement) per i due software, valutata con l'$ICC$ (inter-class correlation coefficient). L'$ICC$ sostanzialmente misura la correlazione tra l'ordinamento delle misure fatto dai due osservatori, piuttosto che la correlazione tra le misure stesse. Un $ICC=1$ indica che l'ordinamento delle misure è identico. Vediamo come l'$ICC$ sia abbastanza alto ($> 0.9$) in tutti i casi tranne la misura del VAT con ANALYZE, mentre l'$ICC$ dell'HIPPO FAT nella valutazione del VAT è molto vicino ad uno. Questo è un risultato abbastanza generale, nel senso che tipicamente un programma con un livello alto di automazione riduce la variabilità inter e intra osservatore, in quanto funge da “guida” per l'operatore. Un programma manuale lascia più libero l'operatore e quindi aumenta la variabilità tra operatori.
Viene poi valutata la riproducibilità intra-operatore (intra rater agreement), che risulta sempre elevata ($ICC>0.98$). Anche questo è un risultato atteso, in quanto la variabilità intra-osservatore è tipicamente minore della variabilità inter-osservatore (Figura 4.11).

<img src="./images/image-10.png" alt="Analyze vs. HIPPO FAT 2" style="width:100%;">

*Figura 4.11. Confronto HIPPO vs ANALYZE (2).*

Infine viene valutata la concordanza tra i due software. Dalla tabella si vede come per il SAT e per il VAT l'HIPPO presenti una sottostima rispetto ad ANALYZE del 10% circa. Le misure effettuate con i due software non sono comunque diverse da un punto di vista statistico.  
Il risultato di questo studio indipendente conferma la valutazione precedente per il VAT, mentre da un risultato di segno opposto per il SAT.  Un risultato importante è che il tempo di analisi con HIPPO FAT è la metà di quello richiesto da ANALYZE, cosa che porta gli autori a suggerirne l'uso. 

Un altro studio dell'università dell'Ohio, USA è stato condotto su 40 pazienti [^2]. Oltre al numero di pazienti più elevato, che assicura una migliore potenza statistica, è interessante notare che il confronto è fatto con il programma Slice-O-Matic, uno strumento commerciale che utilizza un algoritmo di tipo *region growing*. Il programma Slice-O-Matic è stato validato per la misura del grasso addominale in molti studi anche rispetto a campioni istologici, e rappresenta quindi un ottimo “gold standard”. 
Come si osserva dalla figura, i risultati dello studio confermano quelli della validazione con una leggera sovrastima del SAT (4%) e una sottostima più rilevante del VAT (9.4%). Anche qui il tempo di analisi è circa la metà di quello richiesto utilizzando Slice-O-Matic (Figura 4.12).

[^2]: Demerath EW, Ritter KJ, Couch WA, et al. Validity of a new automated software program for visceral adipose tissue estimation. Int J Obes (Lond). 2007;31:285–91

<img src="./images/image-11.png" alt="Analyze vs. HIPPO FAT 3" style="width:100%;">

*Figura 4.12. Confronto HIPPO vs ANALYZE (3).*

Infine uno studio della Johns Hopkins University di Baltimora (USA)  [^3] confronta cinque software per la misura del grasso viscerale: NIHImage, slice-O-matic, Analyze, Philips Easy Vision e Hippo Fat.  

NIHImage è il precursore di imagej, il software di analisi dell'immagine sviluppato presso NIH (National Institute of Health) degli USA, Philips Easy Vision è un software proprietario installato nelle workstation Philips, tipicamente disponibili insieme agli scanner MR Philips. 
Abbastanza interessante è la tabella comparativa tra i software (Figura 4.13), che rappresenta un buon esempio dei parametri da tenere in conto nella valutazione clinica di questo tipo di strumenti. Vediamo i principali: 

[^3]:Bonekamp S, Ghosh P, Crawford S, et al. Quantitative comparison and evaluation of software packages for assessment of abdominal adipose tissue distribution by magnetic resonance imaging. Int J Obes (Lond) 2008;32:100–111.


<img src="./images/image-12.png" alt="Bonekamp" style="width:100%;">

*Figura 4.13. Confronto fra diversi software.*

**Avalaibility**: Qui viene riportato il costo del software. L'uso dell'Easy Vision è limitato agli utenti di uno scanner Philips. SliceOMatic e Analyze sono software commerciali. Gli ultimi due sono gratuiti per uso di ricerca. Importante anche valutare i sistemi operativi supportati, in particolare Easy Vision è in bundle con la workstation Philips, gli altri sono multi piattaforma con l'eccezione di HIHimage (in realtà la versione corrente imagej è multipiattaforma) e SliceOMatic. Questa ultima informazione è importante per valutare l'uso del software nella configurazione della rete informatica disponibile in un data centro. Viene anche valutata la facilità di installazione.
        
**Documentation**: Viene poi valutata la qualità della manualistica e la facilità di apprendimento del software. Questi parametri sono importanti per definire la curva di apprendimento, cioè il tempo previsto per un uso a regime del software nel centro.

**Data Input**: ad oggi è necessaria e sufficiente la compatibilità DICOM, qui si poteva utilmente aggiungere la presenza o meno di un DICOM conformance statement. Software più “antichi” accedono ad altri formati in uso precedente al DICOM.

**Average time**: un altro parametro importante è il tempo di analisi richiesto. Questo permette di valutare il costo in ore/uomo per il centro per analizzare i dati.

Al solito viene poi valutata la riproducibilità inter-osservatore (Figura 4.14) e inter-software (Figura 4.15). 


<img src="./images/image-13.png" alt="Bonekamp 2" style="width:100%;">

*Figura 4.14. Variabilità inter-osservatore.*

<img src="./images/image-14.png" alt="Bonekamp 3" style="width:100%;">

*Figura 4.15. Variabilità inter-software.*

Qui l’$ICC$ risulta elevato per tutti i software. Per quanto riguarda il SAT i risultati dei software sono sovrapponibili. Per quanto riguarda il VAT NIHImage, SliceOmatic e Analyze danno risultati simili, mentre HippoFat e EasyVision divergono.  Probabilmente questo è dovuto all’approccio simile per la segmentazione del VAT seguito da  NIHImage, SliceOmatic e Analyze che utilizzano un approccio di tipo topologico mentre HippoFat usa un approccio basato sull’analisi dell’istogramma.

Il caso di studio esaminato permette di trarre alcune conclusioni di carattere generale. La valutazione di un pacchetto software per l'analisi di immagini biomediche deve basarsi su una serie di considerazioni, che possono essere schematizzate come segue.

1.	Accessibilità al software. Si valuta la possibilità di utilizzare il software presso il proprio centro. I parametri di interesse sono:\
1a. Costo (Compatibilità economica). Il costo deve comprendere il costo di acquisto e quello stimato per gli aggiornamenti periodici nel tempo.\
1b. Compatibilità software. Compatibilità del software con l'hardware e il/i sistemi operativi disponibili nel centro. Tempo di installazione.\
1c. Supporto DICOM. Certificato da conformance statement.

2.	Usabilità. La possibilità di utilizzare in modo proficuo il software. Questo comprende: \
2a. Curva di apprendimento: il tempo che un utente medio impiega a divenire produttivo nell'uso del software. Dipende dalla semplicità d'uso del software, dalla qualità della manualistica, dal servizio di assistenza (helpdesk, corsi di formazione, etc).\
2b. Tempo di analisi delle immagini a regime. 

3.	Riproducibilità.  La capacità del software di assicurare risultati riproducibili in modo indipendente dall'operatore. Valutata attraverso la variabilità inter- e intra- osservatore.

4.	Validazione clinica. La capacità del software di produrre risultati corretti. Si valuta attraverso gli studi di validazione disponibili. I parametri di interesse sono:\
4a. Numero di pazienti e range di patologie esaminate. \
4b. Indipendenza della validazione\
4c. Validazione multi-centro

5.	Certificazione.  Un punto importante che esula dalla componente tecnica è la certificazione del software come dispositivo medico. Un software utilizzato in ambito clinico deve essere a norma di legge certificato da un ente certificatore. Nella concessione della certificazione i punti fondamentali sono la validazione clinica e la presenza di una manualistica adeguata. Inoltre è necessario un documento di gestione del rischio che contempli gli eventuali malfunzionamenti del software e le loro conseguenze.   

Per quanto riguarda la certificazione, il software per l’analisi di immagini mediche è a tutti gli effetti un “dispositivo medico”, e rientra quindi nella normativa corrente che riguarda questi dispositivi. Tale normativa impone al fabbricante del software (cioè a chi commercializza o distribuisce il software) di provvedere ad una particolare marcatura CE, specifica per i dispositivi medici, che deve essere rilasciata da un ente certificato. Il software se è integrato con un dispositivo medico (ad esempio un ecografo o uno scanner MR) di cui è parte imprescindibile viene certificato insieme al dispositivo.  Il software “stand-alone” viene classificato come “dispositivo medico attivo” in quanto il suo funzionamento dipende da una fonte di energia esterna. La certificazione richiede una complessa serie di attività da parte del produttore:

1.	Stabilire se il software è o meno un dispositivo medico (per essere un dispositivo medico il software deve produrre informazioni diagnostiche. Una cartella clinica ad esempio non è di per se un dispositivo medico. La semplice visualizzazione di immagini rende invece un software un dispositivo medico. 
2.	Individuare la destinazione d’uso (quindi quando è appropriato usare il software). Il produttore dovrà quindi individuare ad esempio i valori limite di qualità dell’immagine che permettono l’uso del software stesso.
3.	Attribuire la classe di rischio del dispositivo, in base al danno che un malfunzionamento del software può produrre sul paziente o sugli operatori. La classe di rischio viene assegnata sulla base di tabelle prefissate. Le tre classi principali sono A: nessun danno possibile; B: lesioni non gravi; C: morte o lesioni gravi.
4.	Progettare e fabbricare il dispositivo secondo i parametri stabiliti dalla normativa. Sostanzialmente questo punto implica l’adozione di un controllo interno di qualità secondo le specifiche ISO e la predisposizione di un fascicolo tecnico che documenti la struttura del software.
5.	Attivare una procedura di valutazione di conformità in base alla classe di rischio individuata.
6.	Definire le istruzioni per l’installazione del software e gli eventuali requisiti di competenza /qualifica dell’installatore (manuale di installazione).
7.	Definire e documentare le istruzioni per l’utilizzo del software (manuale d’uso del software).
8.	Definire le procedure per la manutenzione del software (aggiornamenti, etc).


























