{"version":"1","records":[{"hierarchy":{"lvl1":"Elaborazione delle Bioimmagini"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Elaborazione delle Bioimmagini"},"content":"Materiale didattico per il corso di elaborazione di bioimmagini.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Elaborazione delle Bioimmagini","lvl2":"Contatti"},"type":"lvl2","url":"/#contatti","position":2},{"hierarchy":{"lvl1":"Elaborazione delle Bioimmagini","lvl2":"Contatti"},"content":"Per domande o problemi, contatta:\n\nIng. Vincenzo Positano: \n\npositano@ftgm.it\n\nDr. Alejandro Luis Callara: \n\nalejandro​.callara@unipi​.it","type":"content","url":"/#contatti","position":3},{"hierarchy":{"lvl1":"Elaborazione delle Bioimmagini","lvl2":"Indice dei Contenuti"},"type":"lvl2","url":"/#indice-dei-contenuti","position":4},{"hierarchy":{"lvl1":"Elaborazione delle Bioimmagini","lvl2":"Indice dei Contenuti"},"content":"Prerequisiti\n\nIstruzioni di Installazione\n\nmacOS\n\nLinux\n\nWindows\n\nCompilazione del Jupyter Book\n\nStruttura del Progetto\n\nRisoluzione dei Problemi\n\nProblemi con la versione di Python\n\nErrori durante l’installazione\n\nProblemi con l’attivazione dell’ambiente virtuale","type":"content","url":"/#indice-dei-contenuti","position":5},{"hierarchy":{"lvl1":"Elaborazione delle Bioimmagini","lvl2":"Prerequisiti"},"type":"lvl2","url":"/#prerequisiti","position":6},{"hierarchy":{"lvl1":"Elaborazione delle Bioimmagini","lvl2":"Prerequisiti"},"content":"IMPORTANTE: Questo progetto richiede Python 3.12. Assicurati di avere Python 3.12 installato prima di procedere.","type":"content","url":"/#prerequisiti","position":7},{"hierarchy":{"lvl1":"Elaborazione delle Bioimmagini","lvl2":"Istruzioni di Installazione"},"type":"lvl2","url":"/#istruzioni-di-installazione","position":8},{"hierarchy":{"lvl1":"Elaborazione delle Bioimmagini","lvl2":"Istruzioni di Installazione"},"content":"","type":"content","url":"/#istruzioni-di-installazione","position":9},{"hierarchy":{"lvl1":"Elaborazione delle Bioimmagini","lvl3":"macOS","lvl2":"Istruzioni di Installazione"},"type":"lvl3","url":"/#macos","position":10},{"hierarchy":{"lvl1":"Elaborazione delle Bioimmagini","lvl3":"macOS","lvl2":"Istruzioni di Installazione"},"content":"","type":"content","url":"/#macos","position":11},{"hierarchy":{"lvl1":"Elaborazione delle Bioimmagini","lvl3":"macOS","lvl2":"Istruzioni di Installazione"},"type":"lvl3","url":"/#macos-1","position":12},{"hierarchy":{"lvl1":"Elaborazione delle Bioimmagini","lvl3":"macOS","lvl2":"Istruzioni di Installazione"},"content":"Installa Python 3.12 (se non già installato):\n\nUsando Homebrew (consigliato):brew install python@3.12\n\nOppure scarica da \n\npython.org e seleziona Python 3.12.x\n\nVerifica la versione di Python:python3.12 --version\n\nDovrebbe mostrare: Python 3.12.x\n\nNaviga nella directory del progetto:cd /path/to/elab_bioimmagini\n\nCrea un ambiente virtuale:python3.12 -m venv .venv\n\nAttiva l’ambiente virtuale:source .venv/bin/activate\n\nAggiorna pip:pip install --upgrade pip\n\nInstalla le dipendenze:pip install -r requirements.txt\n\n","type":"content","url":"/#macos-1","position":13},{"hierarchy":{"lvl1":"Elaborazione delle Bioimmagini","lvl3":"Linux","lvl2":"Istruzioni di Installazione"},"type":"lvl3","url":"/#linux","position":14},{"hierarchy":{"lvl1":"Elaborazione delle Bioimmagini","lvl3":"Linux","lvl2":"Istruzioni di Installazione"},"content":"","type":"content","url":"/#linux","position":15},{"hierarchy":{"lvl1":"Elaborazione delle Bioimmagini","lvl3":"Linux","lvl2":"Istruzioni di Installazione"},"type":"lvl3","url":"/#linux-1","position":16},{"hierarchy":{"lvl1":"Elaborazione delle Bioimmagini","lvl3":"Linux","lvl2":"Istruzioni di Installazione"},"content":"Installa Python 3.12 (se non già installato):\n\nSu Ubuntu/Debian:sudo apt update\nsudo apt install software-properties-common\nsudo add-apt-repository ppa:deadsnakes/ppa\nsudo apt update\nsudo apt install python3.12 python3.12-venv python3.12-dev\n\nSu Fedora/RHEL:sudo dnf install python3.12 python3.12-pip\n\nOppure scarica da \n\npython.org e compila dai sorgenti\n\nVerifica la versione di Python:python3.12 --version\n\nDovrebbe mostrare: Python 3.12.x\n\nNaviga nella directory del progetto:cd /path/to/elab_bioimmagini\n\nCrea un ambiente virtuale:python3.12 -m venv .venv\n\nAttiva l’ambiente virtuale:source .venv/bin/activate\n\nAggiorna pip:pip install --upgrade pip\n\nInstalla le dipendenze:pip install -r requirements.txt\n\n","type":"content","url":"/#linux-1","position":17},{"hierarchy":{"lvl1":"Elaborazione delle Bioimmagini","lvl3":"Windows","lvl2":"Istruzioni di Installazione"},"type":"lvl3","url":"/#windows","position":18},{"hierarchy":{"lvl1":"Elaborazione delle Bioimmagini","lvl3":"Windows","lvl2":"Istruzioni di Installazione"},"content":"","type":"content","url":"/#windows","position":19},{"hierarchy":{"lvl1":"Elaborazione delle Bioimmagini","lvl3":"Windows","lvl2":"Istruzioni di Installazione"},"type":"lvl3","url":"/#windows-1","position":20},{"hierarchy":{"lvl1":"Elaborazione delle Bioimmagini","lvl3":"Windows","lvl2":"Istruzioni di Installazione"},"content":"Installa Python 3.12 (se non già installato):\n\nScarica Python 3.12.x da \n\npython.org\n\nEsegui l’installer\n\nImportante: Seleziona “Add Python to PATH” durante l’installazione\n\nSeleziona “Install for all users” se hai i diritti di amministratore\n\nVerifica la versione di Python:\n\nApri Command Prompt o PowerShell:python --version\n\nOppure se python3.12 è nel PATH:python3.12 --version\n\nDovrebbe mostrare: Python 3.12.x\n\nNaviga nella directory del progetto:cd C:\\path\\to\\elab_bioimmagini\n\nCrea un ambiente virtuale:python -m venv .venv\n\nOppure se usi python3.12 esplicitamente:python3.12 -m venv .venv\n\nAttiva l’ambiente virtuale:\n\nIn Command Prompt:.venv\\Scripts\\activate\n\nIn PowerShell:.venv\\Scripts\\Activate.ps1\n\nSe ottieni un errore di policy di esecuzione in PowerShell, esegui:Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser\n\nAggiorna pip:python -m pip install --upgrade pip\n\nInstalla le dipendenze:pip install -r requirements.txt","type":"content","url":"/#windows-1","position":21},{"hierarchy":{"lvl1":"Elaborazione delle Bioimmagini","lvl2":"Compilazione del Jupyter Book"},"type":"lvl2","url":"/#compilazione-del-jupyter-book","position":22},{"hierarchy":{"lvl1":"Elaborazione delle Bioimmagini","lvl2":"Compilazione del Jupyter Book"},"content":"Dopo l’installazione delle dipendenze, puoi compilare il libro utilizzando il comando ufficiale di Jupyter Book:jupyter book start\n\n\nvisita il sito \n\nhttp://​localhost:3000 per visualizzare il libro.","type":"content","url":"/#compilazione-del-jupyter-book","position":23},{"hierarchy":{"lvl1":"ELABORAZIONE DELLE BIOIMMAGINI"},"type":"lvl1","url":"/c10-info","position":0},{"hierarchy":{"lvl1":"ELABORAZIONE DELLE BIOIMMAGINI"},"content":"Ing. Vincenzo Positano Dr. Alejandro Luis Callara\n\ncontact: \n\npositano@ftgm.it\n\nalejandro​.callara@unipi​.it","type":"content","url":"/c10-info","position":1},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità"},"type":"lvl1","url":"/c11-imm-biom","position":0},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità"},"content":"","type":"content","url":"/c11-imm-biom","position":1},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl2":"Dal Post-processing all’analisi dell’immagine"},"type":"lvl2","url":"/c11-imm-biom#dal-post-processing-allanalisi-dellimmagine","position":2},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl2":"Dal Post-processing all’analisi dell’immagine"},"content":"Nell’imaging biomedico, storicamente le immagini vengono prodotte dal dispositivo di acquisizione avendo come obiettivo l’analisi visiva da parte del radiologo o medico esperto. L’analisi visiva viene effettuata tradizionalmente attraverso la stampa radiografica e la visualizzazione su diafanoscopio. Oggi più comunemente le immagini vengono visualizzate su schermo, sia per ragioni di costo (la stampa radiografica è estremamente costosa) che di praticità (un moderno esame TAC o MRI può contenere centinaia di immagini). L’analisi visiva da parte dell’operatore esperto conduce alla compilazione di un referto diagnostico. L’analisi visiva è intrinsecamente qualitativa, cioè di tipo descrittivo. Nell’ipotesi di un referto relativo ad un esame MRI cardiovascolare, descrizioni di tipo qualitativo potranno essere del tipo “Ventricolo sinistro di normali dimensioni”, “Funzione cardiaca ridotta”, etc. In realtà nella pratica radiologica viene adottato un criterio semi-quantitativo, secondo il quale i vari livelli di una patologia vengono identificati per classi (ad esempio severo, medio, moderato, lieve, assente). Al medico refertante viene richiesto di associare quanto visualizzato dall’immagine diagnostica ad una classe di patologia. Troveremo quindi nel referto espressioni del tipo “buona cinesi segmentaria”, “severo accumulo di ferro”, etc. Dove le locuzioni “buona”, “severo” non indicano un giudizio puramente qualitativo, ma la classificazione semi-quantitativa su di una opportuna scala. L’analisi qualitativa e semi-quantitativa non richiedono in generale l’uso di algoritmi di elaborazione dell’immagine, se si esclude il possibile uso di algoritmi di filtraggio che hanno lo scopo di ottimizzare la visione dell’immagine stessa.\n\nIl passaggio ad un referto di tipo quantitativo comporta invece l’utilizzo di algoritmi di elaborazione atti a estrarre dall’immagine degli indici numerici e quindi quantitativi. Ad esempio l’uso di un programma di segmentazione delle cavità ventricolari permetterà di ottenere una misura quantitativa dei volumi ventricolari destro e sinistro, misure che potranno essere riportate a referto, ad esempio come: “VTD 66 ml/m2”, dove il Volume Telediastolico è riportato in ml normalizzato alla superficie corporea in m2. I vantaggi di una misura di tipo quantitativo sono la superiore oggettività rispetto all’analisi qualitativa o semi-quantitativa e la maggior precisione rispetto ad un sistema discreto a classi. Notiamo comunque che tipicamente nel referto la classificazione a classi viene comunque conservata, cioè le informazioni quantitative vengono “tradotte” in classi di gravità della patologia. Questo ha lo scopo di rendere il referto più immediatamente comprensibile dal medico richiedente il referto stesso che non è in generale esperto della particolare metodica di imaging utilizzata e ha quindi bisogno della traduzione dell’indice qualitativo in una classe di merito a lui nota. Le soglie di conversione vengono solitamente definite all’interno di linee guida sviluppate dalle varie società scientifiche. In Figura 1.1 è riportato un esempio di referto da immagini MRI.\n\nFigura 1.1. Esempio di referto radiologico (MRI cardiaca per lo studio dell’accumulo di ferro).\n\nA questo livello l’analisi visiva e l’elaborazione dell’immagine vengono effettuate in parallelo, quindi gli stessi insiemi di immagini vengono valutate visivamente ed elaborate al calcolatore. Nella pratica clinica odierna, tuttavia, iniziano ad essere utilizzate procedure di acquisizione destinate a produrre immagini inutili dal punto di vista dell’esame visivo e destinate in modo univoco all’elaborazione. Esempi tipici sono le immagini PC (Phase Contrast) in risonanza per la valutazione algoritmica della velocità di flusso e le immagini T1/T2/T2* multiecho per la caratterizzazione quantitativa dei tessuti in MR. In questi casi la parte di elaborazione cessa di essere un “add-on” del processo diagnostico, e ne diviene parte integrante e insostituibile. Nell’esempio di referto in figura troviamo un esempio di questo tipo nella definizione dell’accumulo di ferro attraverso la misura quantitativa del valore di T2* sul miocardio e sul parenchima del fegato. In questo caso il referto è puramente quantitativo, nel senso che l’analisi visiva non è rilevante. Nella terminologia moderna il termine “elaborazione delle bioimmagini” viene preferibilmente sostituito con il termine “analisi dell’immagine medica” (medical image analysis), proprio per sottolineare la preminenza dell’analisi software su quella visuale.\n\nÈ importante notare come in questo caso l’elaborazione dell’immagine biomedica da “ausilio al processo diagnostico” diviene “parte strutturale del processo diagnostico” e l’affidabilità del processo di elaborazione deve essere equivalente a quella di un dispositivo medico vero e proprio. Il decreto legislativo 37 del 25/01/2010 recepisce in Italia la direttiva europea 93/42/CEE sulla certificazione dei dispositivi medici, chiarendo che il software che implementa algoritmi di elaborazione di immagini se usato a fini diagnostici è da considerare un “dispositivo medico” e come tale richiede una opportuna certificazione. Sostanzialmente il software per l’analisi dell’immagine biomedica è equiparato ad un dispositivo di acquisizione o altro strumento diagnostico.\nSecondo la normativa, un Dispositivo Medico è definito come “qualunque strumento, apparecchio, impianto, software, sostanza o altro prodotto utilizzato da solo o in combinazione, compreso il software destinato dal fabbricante ad essere impiegato specificatamente con finalità diagnostiche e/o terapeutiche….”, quindi possono essere individuate quattro tipologie di software, che comprendono sia software “stand-alone”, cioè software utilizzati in modo autonomo, che software integrati in altri dispositivi medici che concorrono al funzionamento dei dispositivi stessi. Ad esempio, il software di controllo di una macchina per l’analisi di campioni biologici o di una macchina di acquisizione immagini rappresenta un software integrato, mentre un programma per l’elaborazione di immagini è un software stand-alone.\n\nA questo livello l’analisi visiva e l’elaborazione dell’immagine vengono effettuate in parallelo, quindi gli stessi insiemi di immagini vengono valutate visivamente ed elaborate al calcolatore. Nella pratica clinica odierna, tuttavia, iniziano ad essere utilizzate procedure di acquisizione destinate a produrre immagini inutili dal punto di vista dell’esame visivo e destinate in modo univoco all’elaborazione. Esempi tipici sono le immagini PC (Phase Contrast) in risonanza per la valutazione algoritmica della velocità di flusso e le immagini T1/T2/T2* multiecho per la caratterizzazione quantitativa dei tessuti in MR. In questi casi la parte di elaborazione cessa di essere un “add-on” del processo diagnostico, e ne diviene parte integrante e insostituibile. Nell’esempio di referto in figura troviamo un esempio di questo tipo nella definizione dell’accumulo di ferro attraverso la misura quantitativa del valore di T2* sul miocardio e sul parenchima del fegato. In questo caso il referto è puramente quantitativo, nel senso che l’analisi visiva non è rilevante. Nella terminologia moderna il termine “elaborazione delle bioimmagini” viene preferibilmente sostituito con il termine “analisi dell’immagine medica” (medical image analysis), proprio per sottolineare la preminenza dell’analisi software su quella visuale.\n\nÈ importante notare come in questo caso l’elaborazione dell’immagine biomedica da “ausilio al processo diagnostico” diviene “parte strutturale del processo diagnostico” e l’affidabilità del processo di elaborazione deve essere equivalente a quella di un dispositivo medico vero e proprio. Il decreto legislativo 37 del 25/01/2010 recepisce in Italia la direttiva europea 93/42/CEE sulla certificazione dei dispositivi medici, chiarendo che il software che implementa algoritmi di elaborazione di immagini se usato a fini diagnostici è da considerare un “dispositivo medico” e come tale richiede una opportuna certificazione. Sostanzialmente il software per l’analisi dell’immagine biomedica è equiparato ad un dispositivo di acquisizione o altro strumento diagnostico.\nSecondo la normativa, un Dispositivo Medico è definito come “qualunque strumento, apparecchio, impianto, software, sostanza o altro prodotto utilizzato da solo o in combinazione, compreso il software destinato dal fabbricante ad essere impiegato specificatamente con finalità diagnostiche e/o terapeutiche...”, quindi possono essere individuate quattro tipologie di software, che comprendono sia software “stand-alone”, cioè software utilizzati in modo autonomo, che software integrati in altri dispositivi medici che concorrono al funzionamento dei dispositivi stessi. Ad esempio, il software di controllo di una macchina per l’analisi di campioni biologici o di una macchina di acquisizione immagini rappresenta un software integrato, mentre un programma per l’elaborazione di immagini è un software stand-alone.\n\nLa Figura 1.2 descrive le quattro classi di software utilizzati in ambito biomedicale e soggetti a certificazione. Le procedure di elaborazione dell’immagine ricadono nella terza classe (Software sviluppato per analizzare dati acquisiti da dispositivi medici).\nÈ importante notare come il software che costituisce una cartella clinica non è un dispositivo medico, in quanto non contribuisce direttamente alla diagnosi e cura.\n\nFigura 1.2. Tipi di software utilizzati in ambito biomedicale.","type":"content","url":"/c11-imm-biom#dal-post-processing-allanalisi-dellimmagine","position":3},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl2":"Richiami sulla definizione di Immagine Biomedica"},"type":"lvl2","url":"/c11-imm-biom#richiami-sulla-definizione-di-immagine-biomedica","position":4},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl2":"Richiami sulla definizione di Immagine Biomedica"},"content":"","type":"content","url":"/c11-imm-biom#richiami-sulla-definizione-di-immagine-biomedica","position":5},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Livelli di grigio e colormap","lvl2":"Richiami sulla definizione di Immagine Biomedica"},"type":"lvl3","url":"/c11-imm-biom#livelli-di-grigio-e-colormap","position":6},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Livelli di grigio e colormap","lvl2":"Richiami sulla definizione di Immagine Biomedica"},"content":"Dal punto di vista dell’analisi dell’immagine, una immagine biomedica è una rappresentazione spazio/temporale di un fenomeno fisico legato al dispositivo di acquisizione. Le caratteristiche fisiche di una regione di spazio (voxel) rispetto ad una qualche eccitazione endogena o esogena (opacità ai raggi X, risonanza magnetica, emissione di radiazioni, etc.) vengono tradotti in un valore numerico di segnale. Nella maggior parte dei casi viene misurata una singola quantità, e quindi l’immagine presenta un singolo canale (livello di grigio). Il livello di grigio è proporzionale al fenomeno fisico misurato. Oggi sostanzialmente tutte le immagini biomediche sono immagini digitali (quindi sono espresse in forma numerica come matrici di numeri interi). Esiste uno standard di fatto di codifica che è il formato DICOM come verrà dettagliato nel seguito.\nVale la pena di notare che tra le normali immagini digitali (Figura 1.3) e le immagini biomediche (Figura 1.4) esistono alcune fondamentali differenze.\n\nUn immagine digitale è tipicamente a colori, quindi è rappresentata come un array (3,dx,dy), cioè attraverso tre canali tipicamente in formato RGB (Red, Green, Blue). I tre canali possono tipicamente assumere valori tra 0 e 255. Quindi una immagine digitale di dimensioni (dx,dy) non compressa occupa 3dxdy byte più alcuni byte di header, cioè di intestazione dell’immagine.\n\nFigura 1.3. Concetto di immagine a colori (RGB).\n\nUna immagine biomedica può essere invece vista come una immagine a livelli di grigio a singolo canale che può essere caratterizzata dal suo istogramma. Tipicamente i livelli di grigio possono essere più di 255, spesso l’immagine è codificata a 12 bit (212=4096) o a 16 bit (216=65536) nel caso di immagini a valori solo positivi. Se sono codificati valori negativi i valori possibili variano di conseguenza.\nNella figura è rappresentata una immagine di risonanza con valori che vanno dallo 0 (nessun segnale, aria) ai valori più alti relativi al grasso. È possibile visualizzare una immagine a livelli di grigio a colori attraverso una opportuna color map, cioè una trasformazione che fa corrispondere ad un certo livello di grigio una terna di valori RGB. Questa tecnica è spesso utilizzata per migliorare la qualità di visualizzazione, ad esempio, in immagini di medicina nucleare (SPECT o PET), sfruttando il fatto che l’occhio umano (o meglio il sistema occhio-cervello) è molto più efficiente nel riconoscimento dei colori rispetto ai livelli di grigio.\n\nFigura 1.4. Concetto di immagine biomedica (MRI assiale dell’addome, MRI Lab, FTGM, Pisa).\n\nDalla Figura 1.5 Notiamo che l’immagine a colori a destra non ha alcun senso dal punto di vista diagnostico. Mentre l’immagine a livelli di grigio ci informa del fatto che un tessuto (il grasso) produce un segnale MR maggiore di quello del muscolo e dell’aria, nell’immagine a colori non possiamo sapere se il blu rappresenta un segnale maggiore o minore del giallo.\n\nFigura 1.5. Immagine assiale MRI a livello della coscia (MRI Lab, Aarhus, DK) e trasformazione in RGB.\n\nPer dare un senso medico all’immagine dobbiamo associare all’immagine a colori la color map utilizzata (Figura 1.6).\n\nFigura 1.6. Esempio di colormap.\n\nLa color map ci dice che il blu corrisponde a livelli di segnale più basso rispetto al giallo e rende l’immagine utilizzabile in senso diagnostico. L’immagine, quindi, verrà visualizzata come in Figura 1.7.\n\nFigura 1.7. Visualizzazione corretta di una immagine a falsi colori in radiologia.\n\nL’immagine con associata la color map utilizzata per la rappresentazione a colori assume quindi una utilità diagnostica, in quanto consente di associare ad un colore l’intensità di segnale acquisita dalla macchina di acquisizione, in questo caso una MRI. Riassumendo:\n\nLe immagini biomediche tipicamente vengono visualizzate a livelli di grigio (US, MRI, CT). Il significato del livello di grigio dipende dalla fisica di acquisizione. Il fatto che alcuni software non specifici per l’analisi di immagini biomediche, come il MATLAB, visualizzino di default una immagine a livelli di grigio con una color map arbitraria non autorizza a ritenere che tale rappresentazione sia accettabile.\n\nIn alcuni casi (SPECT, PET) si utilizzano delle color map standard per visualizzare l’immagine a falsi colori. La color map utilizzata deve essere sempre associata all’immagine. L’immagine a colori risultante è una rappresentazione dell’immagine originale a livelli di grigio come acquisita dalla macchina di acquisizione, che è l’unica fonte reale di informazione diagnostica. Come si vedrà in seguito il formato standard di memorizzazione delle immagini biomediche consente il salvataggio della color map da utilizzare insieme all’immagine stessa.\n\nIl modo “giusto” di rappresentare una immagine dipende sostanzialmente da una convenzione tra gli utilizzatori. Se il mondo radiologico visualizza le immagini MRI a livelli di grigio è il caso di adeguarsi e non inventare rappresentazioni non standard. Non seguire la convenzione implica rendere l’immagine non utile alla comunità di medici che la utilizza e comporta tipicamente il licenziamento dell’Ingegnere Biomedico responsabile.","type":"content","url":"/c11-imm-biom#livelli-di-grigio-e-colormap","position":7},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl2":"Standard DICOM"},"type":"lvl2","url":"/c11-imm-biom#standard-dicom","position":8},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl2":"Standard DICOM"},"content":"Come già accennato le immagini biomediche sono tipicamente memorizzate in formato DICOM. Le immagini DICOM sono le uniche che conservano il valore diagnostico dell’immagine e hanno valore legale. Rappresentazioni dell’immagine DICOM (immagini a colori, salvataggio in formati a minor numero di livelli di grigio  come TIF o JPEG) alterano il contenuto diagnostico dell’immagine e non vanno utilizzati nella distribuzione o immagazzinamento di immagini mediche.\nUn file DICOM contiene tipicamente un header (intestazione) contenente informazioni sull’immagine e l’immagine vera e propria codificata come interi a 16 bit con o senza segno. Un file DICOM può contenere una singola immagine o più immagini. Le informazioni dell’header DICOM sono codificate tipicamente come:\n\nTAG\n\nDescription\n\nType\n\nLength\n\nValue\n\n0008,0022\n\nAcquisition Date\n\nDA\n\n10\n\n20120211\n\nIl campo TAG contiene un codice composto da due valori esadecimali (gruppo, elemento del gruppo) che identificano in modo univoco un elemento dello standard DICOM. Il campo description contiene la descrizione dell’elemento (in questo caso la data di acquisizione dell’immagine). Il campo type contiene il formato dell’elemento (in questo caso una data), mentre il campo Length contiene la lunghezza dell’elemento. Il campo Value contiene il valore dell’elemento stesso in un formato definito dallo standard. In Figura 1.8 viene riportato un esempio della struttura dei metadati di un file DICOM.\n\nFigura 1.8. Esempio di struttura dei metadati di un file DICOM.\n\nIn Python, l’accesso ai metadati DICOM è possibile tramite la libreria pydicom, che consente di leggere i file DICOM e di accedere ai singoli elementi informativi (tag) attraverso un’interfaccia ad oggetti.","type":"content","url":"/c11-imm-biom#standard-dicom","position":9},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Griglia di acquisizione, risoluzione e FOV","lvl2":"Standard DICOM"},"type":"lvl3","url":"/c11-imm-biom#griglia-di-acquisizione-risoluzione-e-fov","position":10},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Griglia di acquisizione, risoluzione e FOV","lvl2":"Standard DICOM"},"content":"Riepiloghiamo ora i parametri fondamentali di interesse di una immagine biomedica digitale, essendo tipicamente l’immagine memorizzata in formato DICOM nel seguito verranno forniti i nomi dei campi DICOM corrispondenti come sarà meglio dettagliato nel seguito.\nUna immagine biomedica bidimensionale può essere vista come una funzione I(x,y) che trasforma una coppia di coordinate nello spazio cartesiano in un valore di intensità di segnale. Il valore I sarà legato al principio fisico di produzione dell’immagine. Una immagine biomedica digitale (quindi discreta) e le sue coordinate saranno definite in un dominio:x = (0, 1,..., N_{x}-1) \\\\\ny = (0, 1,..., N_{y}-1) \\\\\nI = (0, 1,..., 2^{B}-1) \\\\\n\nDove N_{x} e N_{y} rappresentano il numero di righe e di colonne dell’immagine sui due assi (rows and columns nel formato DICOM di codifica) e B rappresenta la profondità dell’immagine espressa in bit (campo Bits Allocated nel DICOM). Alcune modalità di immagine, come la TAC, permettono di definire valori negativi dell’intensità di segnale. In questo caso il range dei valori possibili sarà  = (-2^{B}/2-1,...,2^{B}/2). Il formato dei dati immagazzinati è definito nel campo pixel representation del DICOM (1 signed, 0 unsigned).","type":"content","url":"/c11-imm-biom#griglia-di-acquisizione-risoluzione-e-fov","position":11},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl4":"Convenzione per gli assi","lvl3":"Griglia di acquisizione, risoluzione e FOV","lvl2":"Standard DICOM"},"type":"lvl4","url":"/c11-imm-biom#convenzione-per-gli-assi","position":12},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl4":"Convenzione per gli assi","lvl3":"Griglia di acquisizione, risoluzione e FOV","lvl2":"Standard DICOM"},"content":"Gli assi x ed y seguono una convenzione diversa da quella usuale, dove lo zero delle coordinate è in alto a sinistra e l’asse y è orientato verso il basso come riportato in Figura 1.9.\nIl numero di pixel dell’immagine sarà dato dal prodotto del numero di righe e del numero di colonne. Il numero di pixel è legato alla risoluzione spaziale dell’immagine, in quanto a parità di campo di vista (FOV) un maggior numero di pixel corrisponde ad una minore superficie rappresentata dal pixel e quindi ad una maggiore risoluzione dell’immagine. La risoluzione dell’immagine, espressa in millimetri per pixel, è un altro importante parametro di qualità non ricavabile direttamente dall’immagine vera e propria ma dalle informazioni associate all’immagine nell’header DICOM (campo Pixel Spacing). Il pixel spacing è legato alla dimensione dell’immagine e al Field of View (cioè la dimensione della regione di spazio oggetto dell’immagine) dall’ovvia relazioneFOVx = Rows*dx \\\\ \nFOVy = Columns*dy \\\\\n\ndove dx e dy rappresentano il pixel spacing. Tipicamente sulla console di acquisizione vengono settati il FOV (che ha implicazioni dirette su parametri clinici, quali la dose assorbita in TAC e gli artefatti da ribaltamento in MRI) e le dimensioni della matrice di acquisizione che ha tipicamente effetto sul tempo di acquisizione. Il pixel spacing viene definito dalla macchina di conseguenza. I pixel sono di solito quadrati (dx=dy) per la simmetria del dispositivo di acquisizione lungo il piano dell’acquisizione stessa. È importante ricordare che l’acquisizione di una immagine planare richiede comunque l’acquisizione di un segnale da una porzione finita di spazio, quindi ogni pixel corrisponde comunque ad un volume spaziale con forma a parallelepipedo con base uguale al pixel stesso e altezza dipendente dalla modalità. Ad esempio, nel caso della MRI l’altezza sarà uguale al thickness della fetta (campo Slice Thickness del DICOM). E’ importante notare che il concetto di risoluzione così espresso non è equivalente al concetto di risoluzione dell’immagine biomedica da un punto di vista fisico, che rappresenta la dimensione minima di un oggetto apprezzabile da un dato sistema di imaging. A livello di acquisizione si può scegliere di avere una risoluzione peggiore di quella potenziale del dispositivo per ottimizzare altri parametri, quali il tempo di acquisizione, la dose irradiata o il rapporto segnale rumore.\n\nFigura 1.9. Convenzione per gli assi, FOV e pixel spacing.\n\nSe l’immagine biomedica descrive un volume tridimensionale, essa sarà descritta da una funzione I(x,y,z). Indipendentemente dalla modalità di acquisizione (slice-by-slice o vero 3D) il volume acquisito viene salvato sotto forma di una serie di immagini 2D parallele perpendicolari all’asse z (Figura 1.10). La distanza tra le fette è uguale alla risoluzione lungo l’asse z (dz). E’ importante notare che mentre i pixel sono di solito quadrati, e quindi la risoluzione lungo gli assi x e y è di solito la stessa, la risoluzione lungo l’asse z è spesso diversa ed in particolare più ridotta (quindi dz > dx,dy). Gli elementi fondamentali del volume dei dati (voxel) saranno quindi in forma di parallelepipedo.\n\nFigura 1.10. Slice thickness e slice gap.\n\nLa risoluzione spaziale lungo z è uguale alla distanza tra due slice consecutive. Tale distanza è la somma dello slice thickness più un eventuale gap tra le slices (gap=0 vuol dire che il volume di interesse viene coperto interamente da slices adiacenti, gap>0 che porzioni del volume non vengono coperte, gap<0 che c’è un overlap tra le slices). Il valore del gap dovrebbe essere contenuto nel campo Spacing Between Slices del DICOM. Tuttavia l’uso di questo campo è fortemente arbitrario (alcune interpretazioni scrivono qui direttamente la distanza inter-slice), e il suo uso è quindi sconsigliato. La scelta più opportuna è valutare la distanza inter-slice come la distanza euclidea tra gli angoli superiori sinistri delle due slices le cui coordinate sono memorizzate  nel campo Image Position Patient.\nInfine, possiamo avere immagini dinamiche acquisite nel tempo per descrivere un certo fenomeno. I questo caso definiremo risoluzione temporale delle immagini la distanza in unità temporali (millisecondi o secondi) tra due immagini successive. Ricavare la risoluzione temporale dalle informazioni dell’header DICOM è non banale. Il campo image time contiene il tempo di acquisizione dell’immagine in \n\nhh.mm.ss, e quindi non è di solito adeguato per fenomeni rapidi. Il campo Trigger Time consente di memorizzare il tempo di acquisizione in millisecondi, a partire dall’inizio dell’acquisizione. Il nome del campo deriva dal fatto che l’acquisizione viene pilotata non dall’operatore ma da un segnale esterno o interno alla macchina. Ad esempio nel caso di una acquisizione sincronizzata con l’ECG cardiaco il trigger time verrà determinato dal segnale ECG acquisito sul paziente.\n\nLa posizione della slice o del volume nel sistema di riferimento della macchina o del paziente sono descritti nell’annex A del documento PS 3.17 (Explanatory Information) dello standard DICOM (riportato anche in Figura 1.11), liberamente accessibile in rete ad esempio dal sito \n\nhttp://​www​.dclunie​.com​/dicom​-status.\n\nFigura 1.11. Sistema di riferimento DICOM (Tratto da DICOM Standard Annex A PS 3.17)\n\nLe coordinate della slice sono fornite nel sistema di riferimento solidale al paziente, dove l’asse z segue la direzione F-H (Piedi-Testa), l’asse y la direzione A-P (Anteriore-Posteriore), l’asse x la direzione R-L (Destra-Sinistra). La posizione del sistema di riferimento paziente rispetto al sistema di riferimento dello scanner (asse z direzione di ingresso del paziente nella macchina, asse y alto-basso, asse x sinistra-destra) dipende dall’orientamento del paziente nella macchina, memorizzato nel campo DICOM Patient Position. Ad esempio il valore FFS (Feet First-Supine) indica che il paziente entra con i piedi in avanti e in posizione supina, quindi gli assi z e x sono invertiti. Il campo image position patient fornisce le coordinate in mm dell’angolo superiore sinistro (l’origine) dell’immagine nel sistema di riferimento del paziente. Il campo image orientation patient fornisce l’orientazione dei due versori che partono dall’origine dell’immagine lungo i due assi del sistema di coordinate solidale all’immagine. Ad esempio il valore 1.0\\0.0\\0.0\\0.0\\1.0\\0.0 indica una immagine assiale.\n\nAlcuni campi DICOM di interesse\n\nGroup Element\n\nTitle\n\nEsempio\n\n[0028-0010]\n\nRows\n\n256\n\n[0028-0011]\n\nColumns\n\n256\n\n[0028-0100]\n\nBits Allocated\n\n16\n\n[0028-0103]\n\nPixel Representation\n\n1\n\n[0028-0030]\n\nPixel Spacing\n\n1.24\\1.24\n\n[0018-0050]\n\nSlice Thickness\n\n8\n\n[0018-0088]\n\nSpacing Between Slices\n\n? Inaffidabile\n\n[0020-0032]\n\nImage Position Patient\n\n-203.1-190.1\\33.9\n\n[0020-0037]\n\nImage Orientation Patient\n\n1.0\\0.0\\0.0\\0.0\\1.0\\0.0\n\n[0018-1060]\n\nTrigger Time\n\n16\n\n[0008-0033]\n\nImage Time\n\n142850\n\n[0018-5100]\n\nPatient Position\n\nFFS","type":"content","url":"/c11-imm-biom#convenzione-per-gli-assi","position":13},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"PACS","lvl2":"Standard DICOM"},"type":"lvl3","url":"/c11-imm-biom#pacs","position":14},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"PACS","lvl2":"Standard DICOM"},"content":"Oltre alle informazioni sull’immagine prima viste lo standard DICOM (Digital Imaging and COmmunications in Medicine, immagini e comunicazione digitali in medicina) definisce i criteri per la comunicazione, la visualizzazione, l’archiviazione e la stampa. In particolare il formato DICOM contiene le informazioni utili all’identificazione del paziente, che possono essere inserite manualmente sulla console di acquisizione da parte del tecnico radiologo o nei sistemi moderni ottenute automaticamente dal sistema HIS (Hospital Information System) attraverso la lettura della cosiddetta work list (la lista degli esami da effettuare nella giornata). Una volta acquisite, le immagini DICOM vengono tipicamente memorizzate in un sistema PACS (Picture archiving and communication system), un sistema hardware e software dedicato all’archiviazione, trasmissione, visualizzazione e stampa delle immagini diagnostiche digitali. Un sistema PACS è normalmente composto da una parte di archiviazione, utilizzata per gestire dati e immagini e una di visualizzazione, che presenta l’immagine diagnostica su speciali monitor ad altissima risoluzione, sui quali è possibile effettuare la diagnosi; i sistemi PACS più evoluti permettono anche l’elaborazione dell’immagine. All’interno del PACS è possibile effettuare delle ricerche attraverso i campi DICOM (ad esempio nome del paziente, data dell’esame, tipo di esame, etc). Un sistema PACS è in grado di mantenere nella propria memoria fisica (tipicamente un array di hard disk) le immagini DICOM per un certo tempo (qualche mese) dipendente dalla quantità di immagini acquisite per giorno e dalla grandezza dell’archivio. In seguito le immagini restano disponibili su supporti di backup come DVD o magnetoottici. I dischi di un sistema PACS devono essere configurati in RAID o altra architettura ridondante in modo che la rottura di un disco non comporti una perdita di dati e possa essere sostituito “a caldo” in caso di problemi senza interrompere il funzionamento del sistema. La comunicazione tra la macchina di acquisizione e il PACS avviene attraverso le procedure definite dallo standard DICOM. Ogni macchina è definita dal proprio indirizzo di rete, dalla porta utilizzata per la comunicazione e dal proprio AETITLE (Application Entity Title). Per la comunicazione la macchina chiamante (client) deve essere dichiarata coi i propri parametri sulla macchina che funge da server. I sistemi PACS sono dispositivi medici, in quanto utilizzati per effettuare diagnosi, a meno che non rientrino nella classe I (funzione di solo archivio). Se implementano altre funzioni (anche la semplice visualizzazione) sono da considerare dispositivi medici a tutti gli effetti e come tali richiedono opportuna certificazione. Un programma che effettui analisi dell’immagine medica includerà tipicamente un client DICOM per interfacciarsi con un PACS e scaricare le immagini da elaborare. I programmi di analisi più evoluti sono in grado di salvare i risultati dell’elaborazione come “secondary DICOM’, cioè file DICOM non prodotti da un dispositivo di acquisizione ma frutto di elaborazioni successive.\n\nLa figura 1.12 mostra la tipica architettura di rete in un centro radiologico. I dispositivi di acquisizione (scanner), ad esempio TAC, MR, PET, etc convogliano le immagini acquisite nel PACS. Dal PACS è possibile stampare lastre radiografiche, produrre CD/DVD per i pazienti, effettuare il backup dei dati, etc.  Un programma di elaborazione di immagini scaricherà i dati dal PACS ed eventualmente salverà i risultati sul PACS stesso alla fine del processo.\n\nFigura 1.12. Architettura di un sistema PACS","type":"content","url":"/c11-imm-biom#pacs","position":15},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl2":"Analisi dell’immagine biomedica"},"type":"lvl2","url":"/c11-imm-biom#analisi-dellimmagine-biomedica","position":16},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl2":"Analisi dell’immagine biomedica"},"content":"Il problema dell’analisi dell’immagine biomedica è parte della cosiddetta computer vision (visione artificiale), una branca dell’intelligenza artificiale che ha origine negli anni 60 del secolo scorso e che si occupa di implementare algoritmi che riproducano le funzioni della visione umana. Un sistema di computer vision è tipicamente strutturato in una serie di operazioni classificate in ordine di complessità come:\n\nImage Acquisition. Include l’acquisizione delle immagini da elaborare attraverso fotocamere, videocamere ,etc. Nell’imaging biomedico corrisponde alla fase di acquisizione delle bioimmagini attraverso i vari dispositivi di acquisizione.\n\nImage pre-processing. Include le operazioni a basso livello sull’immagine, come la riduzione del rumore, il miglioramento del contrasto, il cambio della scala di rappresentazione. Nell’imaging biomedico questa fase include gli algoritmi di interpolazione, compressione e filtraggio.\n\nFeature extraction. Include l’estrazione dalle immagini di caratteristiche geometriche di interesse, come linee, punti, curve ed i cosiddetti blob, cioè aggregati di pixel con caratteristiche comuni.\n\nDetection/Segmentation. Include l’estrazione dall’immagine di oggetti di interesse, ad esempio separando lo sfondo dell’immagine e estraendo dalla scena gli oggetti visibili. Nell’imaging biomedico i punti 3 e 4 della computer vision vengono tipicamente raggruppati negli algoritmi di segmentazione o pattern recognition. Si noti che in questa fase gli oggetti vengono estratti ma non riconosciuti.\n\nHigh-level processing. Questa fase include le operazioni ad alto livello legate al riconoscimento degli oggetti, cioè alla loro classificazione. Nell’imaging biomedico abbiamo la corrispondenza con gli algoritmi di classificazione e registrazione delle bioimmagini.\n\nDecision Making. In questa fase le informazioni ottenute nelle fasi precedenti vengono utilizzate per determinare una azione da intraprendere. Nell’imaging biomedico questo corrisponde ad effettuare una diagnosi con metodi di intelligenza artificiale.\n\nIn questo corso analizzeremo i punti 2-5.","type":"content","url":"/c11-imm-biom#analisi-dellimmagine-biomedica","position":17},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Modello dell’immagine biomedica","lvl2":"Analisi dell’immagine biomedica"},"type":"lvl3","url":"/c11-imm-biom#modello-dellimmagine-biomedica","position":18},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Modello dell’immagine biomedica","lvl2":"Analisi dell’immagine biomedica"},"content":"Nell’ingegneria è diffusa la credenza che sia possibile sviluppare modelli matematici che approssimino il mondo reale con sufficiente accuratezza e il campo dell’imaging biomedico non fa eccezione. Definiamo quindi l’immagine biomedica ideale come un insieme di regioni non sovrapposte ognuna caratterizzata da un certo livello di grigio. Ogni livello di grigio e quindi ogni regione corrisponderà ad un tessuto. Avremo quindi:\\begin{aligned}\nI_0(x,y) &= \\sum_{i=1}^{k} P_i(x,y) ; \\quad P_i(x,y) &=\n\\begin{cases}\ns_i & \\text{se } V(x,y) \\in d_i \\\\\n0   & \\text{altrove}\n\\end{cases}\n\\end{aligned}\n\ned i domini d_{i} sono disgiunti. k è “piccolo”, nel senso che il numero di tessuti costituenti è limitato. In generale tanto più grande è k tanto più il modello sarà simile all’immagine reale, fino a k uguale al numero di pixel dell’immagine quando si ha l’identità completa tra modello e immagine ed il modello diviene totalmente inutile. Per comprendere il senso del modello, si pensi a quale è il fine diagnostico dell’imaging medico. Quello che si vorrebbe è che a ogni tessuto venga associato in modo univoco un singolo valore di segnale, in modo che ogni tessuto sia distinguibile dagli altri senza possibilità di errore. Una immagine di questo tipo che presenta la corrispondenza univoca tessuto → valore di segnale è l’immagine biomedica ideale ai fini diagnostici e coincide con I_{0}. In un certo senso l’elaborazione dell’immagine biomedica si potrebbe definire come l’insieme delle tecniche che consentono di estrarre l’immagine ideale I_{0} dall’immagine reale come acquisita dal dispositivo di imaging. Un esempio di immagine ideale e immagine reale è riportato in Figura 1.13.\n\nFigura 1.13. Sinistra: Immagine reale. Destra: immagine ideale.\n\nIntroduciamo ora nel modello i fattori che corrompono l’immagine ideale:\n\nRumore biologico.\nAlcuni tessuti non sono omogenei ma caratterizzati da un struttura interna. Ad esempio nel cuore il sangue che riempie i due ventricoli potrà in generale essere considerato omogeneo essendo un liquido (non considerando il movimento del sangue stesso) mentre il muscolo cardiaco sarà caratterizzato da una struttura a fibre che produce una disomogeneità nell’immagine. Questo fenomeno viene definito “rumore biologico” n_{B}(x,y), nel senso che le proprietà intrinseche del tessuto divergono dal modello ideale. Evidentemente il concetto di rumore biologico è legato alla risoluzione dell’immagine, che determina la grandezza delle disomogeneità rilevabili. Nell’esempio precedente, il sangue è disomogeneo a livello microscopico (globuli rossi e bianchi, piastrine, etc) ma omogeneo a livello della risoluzione MRI (> 1mm).\n\nEffetto volume parziale (partial volume effect, PVE). Il fatto che il processo di acquisizione sia discreto implica che il segnale viene acquisito in un certo volume di spazio pari alla risoluzione spaziale della metodica. Se due tessuti convivono nello stesso volume elementare, il voxel corrispondente dell’immagine assumerà un valore di livello di grigio intermedio tra i valori caratteristici dei due tessuti (effetto volume parziale, Figura 1.14). Questo effetto viene modellato attraverso la convoluzione con un kernel gaussiano h(x,y), che corrisponde ad una operazione di smoothing dei contorni dell’immagine. Infatti l’effetto volume parziale agisce solo sulle zone di transizione tra i due tessuti.\n\nFigura 1.14. Esempio di effetto di volume parziale (PVE).\n\nAttenuazione. In molti casi l’immagine biomedica sarà affetta da un processo di attenuazione del segnale, che produce una distorsione continua dell’immagine lentamente variabile. Nella MRI ad esempio avremo un effetto indotto dalla disomogeneità nel campo magnetico statico o dalla sensibilità delle bobine, mentre negli ultrasuoni l’effetto sarà legato alla distanza del tessuto dalla sonda. L’effetto di attenuazione verrà descritto da un campo moltiplicativo g(x,y). Il campo moltiplicativo è tipicamente caratterizzato dall’essere “smooth”, cioè dal fatto che varia lentamente senza brusche transizioni.\n\nRumore. Infine l’immagine sarà corrotta da rumore con una certa distribuzione n(x,y), che dipende dal processo fisico utilizzato per l’acquisizione. Ad esempio in MRI il rumore avrà distribuzione di tipo Riciano, negli US avrà distribuzione di Rayleigh, etc. Al contrario di quanto avviene per il rumore Gaussiano, il rumore non è necessariamente di tipo additivo. Un esempio di rumore non additivo verrà introdotto nel seguito.\n\nIn definitiva l’immagine osservata I(x,y) sarà descritta dalla relazione:I(x,y)=[[I_{0}(x,y)+n_{B}(x,y)]*h(x,y)]g(x,y)+n(x,y)\n\nil simbolo + che precede il rumore non va inteso in senso rigorosamente additivo come prima detto.\nLa stima dell’immagine ideale I_{0}(x,y) comporta la stima di tutti gli elementi compresi nella formula, nota I(x,y) che è l’immagine reale. Il problema è naturalmente molto complesso e non prevede una soluzione unica. In generale l’approccio utilizzato è quello di modellare le varie componenti in base a dati noti e trovare una soluzione minimizzando un qualche funzionale.","type":"content","url":"/c11-imm-biom#modello-dellimmagine-biomedica","position":19},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Istogramma","lvl2":"Analisi dell’immagine biomedica"},"type":"lvl3","url":"/c11-imm-biom#istogramma","position":20},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Istogramma","lvl2":"Analisi dell’immagine biomedica"},"content":"È di interesse interpretare quanto detto finora dal punto di vista dell’Istogramma dell’immagine. Un istogramma è una rappresentazione statistica della distribuzione dei livelli di grigio nell’immagine.\nIn ascissa abbiamo i possibili valori di livello di grigio ed in ordinata il numero di pixel che assumono quel determinato valore. Se normalizziamo i valori dell’istogramma per il numero totale di pixel, l’istogramma normalizzato rappresenterà la probabilità di trovare nell’immagine un pixel con quel valore. Spesso l’asse x viene quantizzato in un certo numero di regioni (bins) e lungo l’asse y viene riportato il numero di pixel con valore compreso nel bin per migliorare la qualità visiva dell’istogramma.\nL’istogramma dell’immagine ideale I_{0} sarà costituito da k picchi di altezza pari al numero di pixel appartenenti al dominio k. Si consideri ad esempio l’immagine in Figura 1.15 a 6 pattern e il relativo istogramma, definito da 6 picchi distinti con altezza uguale al numero di pixel appartenenti al pattern.\n\nFigura 1.15. Esempio di istogramma per un’immagine ideale.","type":"content","url":"/c11-imm-biom#istogramma","position":21},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl4":"Effetto volume parziale (PVE)","lvl3":"Istogramma","lvl2":"Analisi dell’immagine biomedica"},"type":"lvl4","url":"/c11-imm-biom#effetto-volume-parziale-pve","position":22},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl4":"Effetto volume parziale (PVE)","lvl3":"Istogramma","lvl2":"Analisi dell’immagine biomedica"},"content":"Se effettuiamo una operazione di smoothing (filtro a media mobile) simulando l’effetto volume parziale, l’istogramma si trasforma come in Figura 1.16. Come si osserva i bordi dell’immagine si sfumano e nell’istogramma compaiono dei nuovi valori dei livelli di grigio tra i picchi, distribuiti in modo uniforme. Infatti la percentuale di tessuti diversi che ricade all’interno della finestra di smoothing è distribuita casualmente secondo una distribuzione uniforme.\n\nFigura 1.16. Esempio di istogramma per un’immagine con PVE.\n\nQuesto è ciò che accade anche nella procedura di acquisizione reale a causa del PVE. Se in un voxel si trovano due tessuti con segnale S_{1} e S_{2}, il segnale risultante del voxel sarà:S = pS_{1}+(1-p)S_{2}\n\ndove p compreso tra 0 e 1 è la percentuale di voxel occupato dal tessuto 1. Evidentemente il valore di S sarà compreso tra S_{1} per (p=1) e S_{2} per (p=0). Nelle immagini biomediche reali p si può considerare distribuito in modo uniforme, in quanto l’oggetto dell’imaging ha forma irregolare e la griglia di acquisizione e la forma dell’oggetto di cui si fa l’imaging non hanno nessuna relazione particolare. Si assumerà quindi una distribuzione di probabilità uniforme tra i valori S_{1} e S_{2}. Si noti che in casi molto particolari, come quello di un fantoccio cubico con le facce parallele agli assi x e y del riferimento di acquisizione questa assunzione può non essere vera.\nQuanto detto si può estendere alla compenetrazione di tre o più tessuti nello stesso voxel.\n\nPossiamo quindi concludere che in generale:\n\nIl PVE introduce nuovi livelli di grigio, compresi tra il livello minimo e il livello massimo dei due tessuti interessati nell’immagine ideale.\n\nI livelli di grigio creati sono distribuiti uniformemente tra i picchi dei tessuti adiacenti.\n\nIl numero di livelli creati dipende dal perimetro dei pattern e non dalla loro area.\n\nIn una immagine reale, a parità di oggetto di cui si fa l’imaging, l’incidenza del PVE dipende dalla risoluzione. Usando pixel più piccoli la percentuale di pixel affetti dal PVE sarà minore. Come detto in precedenza, anche in caso di immagine planare il segnale di un pixel sarà sempre originato da un volume spaziale finito di forma parallelepipodale. Esisterà quindi anche un PVE in direzione perpendicolare al piano di acquisizione, tanto più marcato quanto maggiore è il thickness della fetta.","type":"content","url":"/c11-imm-biom#effetto-volume-parziale-pve","position":23},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl4":"Effetto dell’attenuazione","lvl3":"Istogramma","lvl2":"Analisi dell’immagine biomedica"},"type":"lvl4","url":"/c11-imm-biom#effetto-dellattenuazione","position":24},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl4":"Effetto dell’attenuazione","lvl3":"Istogramma","lvl2":"Analisi dell’immagine biomedica"},"content":"L’attenuazione dell’immagine è modellata da un processo moltiplicativo per cui lo stesso tessuto in parti diverse dell’immagine assume valori diversi. Dal punto di vista dell’istogramma avremo un allargamento e una deformazione dei picchi.  In Figura 1.17 osserviamo una immagine ideale composta di tre tessuti ed il relativo istogramma. L’immagine viene moltiplicata per un campo di attenuazione il cui andamento è mostrato in Figura 1.18, originando una nuova immagine con livelli di grigio distorti. L’istogramma dell’immagine attenuata risulta distorto.\n\nFigura 1.17. Esempio di campo di attenuazione.\n\nFigura 1.18. Istogramma immagine con effetto di attenuazione.","type":"content","url":"/c11-imm-biom#effetto-dellattenuazione","position":25},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl4":"Effetto del rumore","lvl3":"Istogramma","lvl2":"Analisi dell’immagine biomedica"},"type":"lvl4","url":"/c11-imm-biom#effetto-del-rumore","position":26},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl4":"Effetto del rumore","lvl3":"Istogramma","lvl2":"Analisi dell’immagine biomedica"},"content":"Il rumore, se gaussiano a media nulla, crea un allargamento dei picchi dell’istogramma. Infatti si creano dei livelli di grigio che deviano dal valore teorico dei pattern tanto più quanto più è grande la deviazione standard del processo di rumore. Un esempio di effetto di rumore Gaussiano additivo è riportato in Figura 1.19.\n\nFigura 1.19. Istogramma immagine con rumore Gaussiano additivo.\n\nApplicando tutti i fattori di disturbo otteniamo l’immagine reale (Figura 1.20).\nCome si osserva l’istogramma dell’immagine è modificato in modo dipendente dal peso e dalle caratteristiche dei vari fattori di disturbo.\n\nFigura 1.20. Istogramma immagine con pve, attenuazione e rumore.\n\nSe calcoliamo la differenza tra l’immagine reale e l’immagine ideale otteniamo l’immagine e l’istogramma di Figura 1.21.\n\nFigura 1.21. Istogramma dell’immagine differenza.\n\nSi nota come la differenza tra le due immagini sia più pronunciata nelle zone di transizione tra tessuti (che sono quelle di interesse clinico), mentre è minore nelle zone omogenee. L’istogramma della differenza mostra come la maggior parte degli errori siano piccoli (si addensano intorno allo zero), ma esistono anche variazioni molto grandi.","type":"content","url":"/c11-imm-biom#effetto-del-rumore","position":27},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Entropia","lvl2":"Analisi dell’immagine biomedica"},"type":"lvl3","url":"/c11-imm-biom#entropia","position":28},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Entropia","lvl2":"Analisi dell’immagine biomedica"},"content":"I vari processi di distorsione dell’immagine possono essere quantificati introducendo il concetto di Entropia dell’immagine.\nL’entropia di Shannon di una immagine I è una quantità definita dalla teoria dell’informazione ed è data da:H(I) = - \\sum_{g_i \\in G} \\log\\!\\bigl(P(I = g_i)\\bigr)\\, P(I = g_i)\n\nDove g_{i} sono i livelli di grigio dell’immagine I appartenenti all’insieme G dei possibili livelli di grigio, determinato dalla profondità dell’immagine. P(I=g_{i}) è la probabilità che un pixel dell’immagine assuma il valore g_{i}. Essendo P(I=g_{i}) sempre minore o uguale ad uno, l’entropia H assume sempre valori positivi ed assume il valore nullo solo nel caso di P(I=g_{i}) = 1 per un singolo valore di i, quindi nel caso di una immagine uniforme composta da un solo pattern. Secondo la teoria di Shannon, l’entropia esprime la quantità di informazione contenuta nell’immagine. Una immagine composta da rumore avrà massima entropia mentre una immagine composta da un singolo valore avrà entropia minima. In questo senso, stabilita la configurazione dei pattern che costituiscono l’immagine, l’immagine ideale avrà entropia minore rispetto alle immagini ottenute applicando i vari fattori di disturbo.\nLa probabilità P(I=g_{i}) utilizzata per il calcolo dell’entropia non è altro che l’istogramma normalizzato per il numero di pixel dell’immagine, quindi possiamo scrivere:H(I) = -\\frac{1}{N} \\sum_{i=1}^{N} h_I(i)\\, \\log\\!\\left(\\frac{h_I(i)}{N}\\right)\n\ndove h_{I} è l’istogramma di I e N è il numero di pixel di I.\n\nPer cui l‘entropia può essere calcolata facilmente dall’istogramma dell’immagine.In Figura 1.22 sono riportati l’istogramma ed il valore di entropia per i casi di immagine random (composta da rumore casuale, entropia massima), immagine ad un singolo pattern (entropia nulla), immagine a due pattern, immagine a due pattern corrotta da rumore gaussiano, immagine a due pattern corrotta da effetto volume parziale. Come si osserva dagli ultimi tre esempi, l’entropia cresce in dipendenza dai fattori che allontanano l’immagine reale dall’immagine ideale.\n\nFigura 1.22. Valori di entropia per diversi tipi di istogramma.","type":"content","url":"/c11-imm-biom#entropia","position":29},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl2":"Qualità dell’immagine biomedica"},"type":"lvl2","url":"/c11-imm-biom#qualit-dellimmagine-biomedica","position":30},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl2":"Qualità dell’immagine biomedica"},"content":"Il fine di una immagine biomedica è quello di permettere un diagnosi il più possibile accurata da parte del medico. La nozione di qualità dell’immagine biomedica non è facilmente definibile, visto che il modo in cui l’immagine viene utilizzata dall’operatore per definire la diagnosi è fortemente soggettivo e non descrivibile in modo rigoroso. D’altra parte la necessità di definire la qualità di immagine è un punto importante nella progettazione e “quality assessment” dei dispositivi biomedici.\nUn modo comune per rendere maggiormente oggettiva la misura di qualità è l’uso di “phantom”, cioè di fantocci che possono essere costruiti in modo riproducibile e quindi utilizzati per misure ripetute escludendo dalla misura la variabilità dovuta al paziente. Tipicamente le macchine per l’acquisizione di immagini biomediche hanno in dotazione uno o più phantom standardizzati che consentono il quality assessment periodico delle prestazioni del dispositivo. Nel seguito verranno descritti alcuni indici utilizzati comunemente nella misura di qualità dell’immagine biomedica.","type":"content","url":"/c11-imm-biom#qualit-dellimmagine-biomedica","position":31},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Qualitativa o semi-quantitativa","lvl2":"Qualità dell’immagine biomedica"},"type":"lvl3","url":"/c11-imm-biom#qualitativa-o-semi-quantitativa","position":32},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Qualitativa o semi-quantitativa","lvl2":"Qualità dell’immagine biomedica"},"content":"La valutazione viene effettuata da un operatore esperto, che può utilizzare una scala predefinita con un certo numero di livelli per standardizzare la valutazione (analisi semi-quantitativa). La valutazione risulta chiaramente fortemente dipendente dall’operatore. Per minimizzare questo fattore la valutazione può essere fatta da più operatori, mediando i risultati ed eventualmente eliminando i valori estremi.\nUna scala usata correntemente è quella a cinque punti, ad esempio:\n\nnondiagnostic study\n\npoor or suboptimal study\n\nacceptable\n\ngood\n\nexcellent\n\nLa qualità dell’immagine sarà quindi espressa da un numero compreso tra 1 e 5 (anche frazionario se sono coinvolti più osservatori) proporzionale alla qualità dell’immagine.\n\nIn questo caso il processo di valutazione comporterà l’analisi di un certo numero di immagini da parte di operatori esperti con l’attribuzione di un punteggio di qualità. La media dei punteggi (o la mediana volendo escludere i punteggi “estremi”) rappresenterà l’indice di qualità assegnato all’immagine.","type":"content","url":"/c11-imm-biom#qualitativa-o-semi-quantitativa","position":33},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"SNR e CNR","lvl2":"Qualità dell’immagine biomedica"},"type":"lvl3","url":"/c11-imm-biom#snr-e-cnr","position":34},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"SNR e CNR","lvl2":"Qualità dell’immagine biomedica"},"content":"L’SNR e il CNR rappresentano due parametri di qualità largamente utilizzati nella valutazione delle immagini. Facciamo riferimento ad un modello semplificato di immagine biomedica dove l’immagine reale è data dalla somma dell’immagine reale e del rumore.I(x,y)=I_{0}(x,y)+n(x,y)\n\nLa natura del rumore n dipenderà dal processo fisico di acquisizione, per ora supponiamo che il rumore sia gaussiano con media nulla e deviazione standard  \\sigma.In figura osserviamo a sinistra una immagine ideale composta da tre regioni con valore di segnale s1=50, s2=150, s3=200 e a destra la stessa immagine corrotta da rumore con \\sigma=10. L’immagine sintetica di Figura 1.23 imita un phantom usato per la misura di qualità in MRI, costituito da due cilindri concentrici in plastica riempiti con due liquidi diversi. Si tratta quindi di un phantom con due tessuti perfettamente omogenei.\n\nFigura 1.23. Fantoccio: caso ideale e reale e relativi istogrammi.\n\nSi nota come il rumore allarghi la distribuzione del segnale relativa ai vari tessuti, fino a far confondere tra loro pixel appartenenti a tessuti diversi. Maggiore è la deviazione standard del rumore rispetto al valore del segnale, tanto peggiore è la qualità dell’immagine. Si definisce quindi l’SNR (rapporto segnale rumore) come:SNR = \\frac{M_{i}}{\\sigma_{i}}\n\nIl rapporto segnale rumore (SNR) è quindi il rapporto tra il valor medio del segnale in una regione e la deviazione standard (\\sigma, SD) del rumore nella stessa regione. L’SNR dà quindi una misura di quanto il valore del segnale ottenuto dal dispositivo di imaging è corrotto dal rumore. A livello di acquisizione, σ è tipicamente una costante dipendendo dal rumore introdotto dal processo di acquisizione (detettori, amplificatori, etc). Quindi l’SNR dipenderà dalla risoluzione dell’immagine (nel senso che più piccolo è il voxel meno segnale verrà acquisito e quindi M sarà più piccolo diminuendo l’SNR) e dal tempo di acquisizione (maggior tempo di acquisizione incrementa M aumentando l’SNR). L’SNR ottimo è quindi un compromesso tra qualità dell’immagine, tempo di acquisizione e risoluzione.\n\nNell’uso clinico è in realtà più importante il contrasto, cioè la capacità del dispositivo di imaging di distinguere due tessuti diversi. Si introduce quindi il CNR:CNR = \\frac{2|M_{i}-M_{j}|}{\\sigma_{i}+\\sigma_{j}}\n\nIl CNR (rapporto contrasto-rumore) è il rapporto tra la differenza dei valori medi del segnale nelle due regioni e il rumore medio nelle due regioni. Sul CNR valgono le stesse considerazioni fatte per l’SNR.\n\nQuesti indici possono essere valutati in modo manuale tracciando sull’immagine delle regioni di interesse (ROI) e calcolando il valor medio e la deviazione standard del segnale nelle ROI stesse.\nE’ evidente che la misura va effettuata in regioni omogenee dell’immagine e che maggiore è l’area della ROI utilizzata maggiore sarà la precisione della misura. Un limite alle dimensioni della ROI è dato dal PVE, nel senso che i bordi della ROI devono essere lontani dai bordi dei tessuti per evitare di includere regioni dove è presente il PVE. ROI troppo grandi possono dare un valore errato di SNR/CNR se sono presenti fenomeni di attenuazione. Un esempio di tracciamento di ROI per il calcolo del CNR è riportato in Figura 1.24.\n\nFigura 1.24. Esempio di tracciamento ROI per calcolo CNR.\n\nPer ottimizzare la misura si possono far coincidere le ROI con l’intera immagine, acquisendo due immagini dello stesso oggetto con gli stessi parametri di acquisizione. L’immagine ideale non cambia perché l’oggetto è lo stesso, quindi sottraendo le due immagini avremo una immagine differenza dipendente solo dal processo di rumore:\\begin{aligned}\nI_1(x,y) &= I_0(x,y) + n_1(x,y) \\\\\nI_2(x,y) &= I_0(x,y) + n_2(x,y) \\\\\n\\Delta I &= I_2(x,y) - I_1(x,y) = n_2(x,y) - n_1(x,y) = n_d(x,y)\\\\ \n\\end{aligned}\n\nAnche se il rumore è gaussiano, la differenza delle due distribuzioni di rumore n_{d}(x,y) non sarà in generale gaussiana. Una stima superiore della SD dell’immagine differenza è   volte la SD delle due componenti. Misurando quindi la SD sull’immagine differenza e dividendo per  avremo una misura oggettiva del limite superiore del rumore dell’immagine. Notiamo che in questo caso la misura di SD può essere fatta su tutta l’immagine, migliorando la qualità della misura. Un altro vantaggio è che la misura può essere facilmente automatizzata. Questa tecnica può essere utilizzata in sede di valutazione delle apparecchiature quando è possibile acquisire immagini di oggetti inanimati, detti nel gergo phantom o fantocci. Non ha ovviamente significato su immagini acquisite da pazienti dove è impossibile avere due immagini esattamente identiche a causa del movimento del soggetto.\n\nUn altro metodo automatico utilizzabile è quello di estrarre dall’immagine una serie di regioni, ad esempio quadrati di 16x16 pixel. Estratte le regioni, si calcola la SD e si ordinano le regioni per SD crescente. Eliminate le regioni con SD più alto, che sono quelle dove sono presenti più tessuti, il valore di SD rimasto è rappresentativo del rumore.\n\nQuando si effettua una misura su fantoccio è garantita l’omogeneità del tessuto su cui viene effettuata la misura. Quando invece si esaminano immagini di un paziente reale, un problema ulteriore è dato dal fatto che i tessuti di cui si fa l’imaging non sono omogenei come nel modello ideale, ma presentano una struttura (fibre muscolari, etc.). Quindi esiste una variabilità non dovuta al rumore di acquisizione ma alla struttura stessa dell’oggetto (il cosiddetto rumore biologico). Per ovviare a questo inconveniente il rumore può essere stimato sullo sfondo dell’immagine, dove non è presente nessun tessuto. Infatti se vale l’assunzione:I(x,y)=I_{0}(x,y)+n(x,y)\n\nLa SD del rumore è la stessa su tutti i tessuti e quindi anche sul fondo, dove tra l’altro è possibile tracciare una ROI di dimensioni tali da garantire una buona stima dell’SD. In molti casi quando la misura dell’SNR e del CNR viene effettuata su immagini reali si utilizzano quindi le formule:SNR = \\frac{M_{i}}{\\sigma_{BK}}\n\neCNR = \\frac{|M_{i}-M_{j}|}{\\sigma_{BK}}\n\nDove il valor medio è misurato su una ROI nel tessuto di interesse e la SD su una ROI tracciata nel fondo dell’immagine.  Questa tecnica è molto apprezzata dai produttori di dispositivi di acquisizione in quanto consente di ottenere valori di SNR e CNR superiori a quelli reali “visti” dall’operatore come si vedrà nel seguito.","type":"content","url":"/c11-imm-biom#snr-e-cnr","position":35},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl4":"Nota sulla natura del rumore in MRI","lvl3":"SNR e CNR","lvl2":"Qualità dell’immagine biomedica"},"type":"lvl4","url":"/c11-imm-biom#nota-sulla-natura-del-rumore-in-mri","position":36},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl4":"Nota sulla natura del rumore in MRI","lvl3":"SNR e CNR","lvl2":"Qualità dell’immagine biomedica"},"content":"È importante notare come misurare la SD nel fondo dell’immagine abbia senso se e solo se il rumore è di tipo additivo. Questo non è il caso di molte modalità di imaging. Ad esempio, nella MRI le immagini sono ricostruite dal K-spazio, che rappresenta il dominio della frequenza. La parte reale ed immaginaria del K-spazio rappresentano i due canali (fase e quadratura) attraverso i quali viene demodulato il segnale. Il rumore di acquisizione si inserisce al livello dei due canali e viene modellato come gaussiano bianco con SD uguale sui due canali. Per riportare le immagini nel dominio dello spazio, si calcola la trasformata di Fourier inversa del K-spazio ottenendo una immagine complessa c(k,l):c(k,l) = p(k,l) + n_p(k,l) + j \\bigl[ q(k,l) + n_q(k,l) \\bigr]\n\ndove p e q sono la parte reale e immaginaria dell’immagine ideale e np e nq rappresentano il rumore introdotto sui due canali.\nIl modulo di c(k,l), che rappresenta l’immagine MRI comunemente usata, è dato da:z(k,l) = \\lvert c(k,l) \\rvert\n       = \\sqrt{\\bigl(p(k,l) + n_p(k,l)\\bigr)^2\n               + \\bigl(q(k,l) + n_q(k,l)\\bigr)^2}\n\nLa non linearità introdotta dall’operazione di modulo cambia la distribuzione del rumore che diviene di tipo Riciano. Il rumore Riciano non è più additivo, ma moltiplicativo, e quindi il rumore sarà diverso in regioni diverse dell’immagine.\nSe il valore dell’immagine è molto più alto del rumore, si può scrivere trascurando i termini di rumore al quadrato:\\begin{aligned}\nz(k,l) &= m(k,l) + \\frac{p(k,l)\\, n_p(k,l) + q(k,l)\\, n_q(k,l)}{m(k,l)} \\\\\n       &= m(k,l) + n_z(k,l)\n\\end{aligned}\n\ncon\\begin{aligned}\nm(k,l) &= \\sqrt{p(k,l)^2 + q(k,l)^2}, \\\\\nn_z(k,l) &= \\frac{p(k,l)\\, n_p(k,l) + q(k,l)\\, n_q(k,l)}{m(k,l)} .\n\\end{aligned}\n\nRiconducendoci al caso di m immagine di modulo ideale e nz rumore additivo.\nLa SD del rumore sarà:\\sigma_z\n= \\sqrt{\\frac{p(k,l)^2\\,\\sigma_n^2 + q(k,l)^2\\,\\sigma_n^2}{m(k,l)^2}}\n= \\sigma_n\n\nQuindi in regioni di elevata intensità di segnale il rumore si può considerare gaussiano con SD uguale alla SD sui due canali.\nNelle regioni a bassa intensità, come lo sfondo dell’immagine, la distribuzione Riciana è simile alla distribuzione di Rayleigh. Per lo sfondo (segnale nullo) abbiamo:z_B(k,l) = \\sqrt{n_p(k,l)^2 + n_q(k,l)^2}\\sigma_B^2 = \\left( 2 - \\frac{\\pi}{2} \\right) \\sigma_n^2,\n\\qquad\n\\sigma_n = 1.526\\, \\sigma_B .\n\nQuindi nelle immagini MRI, se misuriamo il rumore sullo sfondo otterremo una sottostima del rumore di un fattore 1.526. Questo in realtà è vero per una bobina a singolo canale, per bobine multicanale il fattore di conversione cambia leggerment [1]. Nel caso di immagini MR, la formula corretta da utilizzare sarà quindi:SNR = \\frac{M_{i}}{1.526\\sigma_{BK}}\n\neCNR = \\frac{|M_{i}-M_{j}|}{1.526\\sigma_{BK}}\n\nDove il valor medio è sempre misurato su una ROI nel tessuto di interesse e la SD su una ROI tracciata nel fondo dell’immagine. Questa tecnica è un po’ meno apprezzata dai produttori di dispositivi di acquisizione in quanto ottiene valori di SNR e CNR inferiori rispetto all’assunzione di rumore additivo. In conclusione, nella valutazione del rumore associato ad una immagine biomedica è importante tener conto della fisica dell’acquisizione per effettuare una valutazione corretta. Nel confrontare misure su dispositivi diversi, è importante conoscere che procedura è stata utilizzata per la misura di SNR e CNR.","type":"content","url":"/c11-imm-biom#nota-sulla-natura-del-rumore-in-mri","position":37},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Just Noticeable Difference (JND)","lvl2":"Qualità dell’immagine biomedica"},"type":"lvl3","url":"/c11-imm-biom#just-noticeable-difference-jnd","position":38},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Just Noticeable Difference (JND)","lvl2":"Qualità dell’immagine biomedica"},"content":"L’indice JND indica il valore di incremento relativo del segnale rispetto allo sfondo dopo il quale un oggetto diventa visibile. Si ha quindi JND = (F-B)/B dove B è il segnale dello sfondo e F è il segnale dell’oggetto. E’ stato dimostrato (Weber law) che il valore di JND è largamente indipendente dal valore di B è vale intorno al 2%. Questa assunzione vale nel caso di assenza di rumore, nei casi reali F e B saranno ovviamente delle stime ottenute come media del segnale su una ROI e il valore di JND utile per distinguere i due tessuti sarà più alto.\n\nIn Figura 1.25 è riportato un tipico esperimento per la valutazione del JND, la prima barra ha un JND del 3% (tratto da R Rangayyan, Biomedical Image Analysis, CRC Press 2004). Alcuni fantocci TAC implementano il test JND includendo una serie di inserti con valori JND crescenti. Il test di qualità fatto con tali fantocci prevede il riconoscimento del numero massimo possibile di inserti da parte dell’operatore. I risultati del test JND hanno evidentemente un certo grado di soggettività dipendendo dall’acume visivo dell’operatore, dal monitor usato, dall’illuminazione ambientale, etc.\n\nFigura 1.25. JND.","type":"content","url":"/c11-imm-biom#just-noticeable-difference-jnd","position":39},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Analisi dei profili","lvl2":"Qualità dell’immagine biomedica"},"type":"lvl3","url":"/c11-imm-biom#analisi-dei-profili","position":40},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Analisi dei profili","lvl2":"Qualità dell’immagine biomedica"},"content":"Come introdotto nel modello dell’immagine biomedica, un fattore importante nella valutazione della qualità di immagine è la minimizzazione dell’effetto di smoothing dei bordi dovuto all’effetto volume parziale (partial volume effect, PVE). Per valutare il PVE è possibile estrarre dall’immagine un profilo, che è il grafico dell’intensità di segnale rispetto ad una certa direzione. Per quanto sia possibile estrarre profili anche lungo direzioni oblique, solitamente per evitare problemi dovuti all’interpolazione si preferisce estrarre i profili lungo le coordinate principali (x,y, o z).Consideriamo la Figura 1.26 dove viene riportata un’immagine costituita da uno sfondo di valore 50 con all’interno un quadrato di valore 255. Se tracciamo un profilo lungo x avremo un grafico che presenta due transizioni istantanee. Se applichiamo una operazione di smoothing (che simula il PVE), il profilo apparirà smussato rispetto al caso ideale. Per un PVE ancora maggiore l’effetto di smoothing del profilo si incrementa ulteriormente. E’ quindi ragionevole utilizzare la velocità di transizione del segnale tra due regioni diverse valutata su uno o più profili come indice di qualità dell’immagine. Più rapida la transizione, minore il PVE e migliore la qualità di immagine.\n\nFigura 1.26. Profili per diversi valori di PVE.\n\nIntuitivamente, un parametro di qualità dovrebbe essere correlato all’intervallo spaziale (b-a) in cui avviene la transizione e all’altezza del gradino f(b)-f(a) (Figura 1.27). Un possibile parametro è quindi il valore del gradiente dell’immagine nella direzione x definito come:G_{x}=\\frac{f(b)-f(a)}{b-a}\n\nUn gradiente maggiore corrisponde a contorni meglio definiti e quindi ad una immagine di migliore qualità.\n\nFigura 1.27. Stima del profilo (tratto da R Rangayyan, Biomedical Image Analysis, CRC Press 2004)\n\nÈ stato verificato da Higgins e Jones che il gradiente non è ben correlato con la percezione visiva di un osservatore umano, e quindi può non essere adatto ad una valutazione di qualità delle immagini biomediche. E’ stato quindi proposto un parametro detto acutezza (acutance) definito come:A = \\frac{1}{f(b) - f(a)} \\int_{a}^{b}\n\\left[ \\frac{d}{dx} f(x) \\right]^2 \\, dx\n\nche meglio correla con la percezione visiva della definizione dei contorni. In ogni caso le misure devono essere ripetute in varie direzioni e in varie interfacce tra tessuti per essere significative.\nMentre per una immagine ideale priva di rumore è facile definire a e b come i punti a sinistra e a destra della transizione (ad esempio se i due tessuti hanno valore 50 e 200 b sarà il punto in cui f assume il valore 200 ed a il punto in cui f assume il valore 50), nelle immagini reali affette da rumore sarà necessario definire una soglia per stabilire l’inizio e la fine della transizione.\n\nFortemente connesso al concetto di acutezza è il concetto di Point Spread Function (PSF) riportata in Figura 1.28. La PSF rappresenta l’immagine reale di un punto singolo, ed è quindi in qualche modo la funzione di trasferimento del processo di formazione reale dell’immagine.\n\n*Figura. 1. 28. Point spread function (Tratto da R Rangayyan, Biomedical Image Analysis, CRC Press 2004). *\n\nDa un punto di vista matematico se definiamo un punto ideale nell’immagine ideale come un delta di Dirac, l’immagine reale in uscita dal sistema di imaging sarà la PSF del sistema.\nSe il sistema è lineare e invariante rispetto alla spazio (sistema LSI), l’immagine reale in uscita dal sistema sarà g = PSF * f dove f è l’immagine reale e * rappresenta la convoluzione spaziale.Come tipicamente rappresentato nella figura, la PSF è di solito una funzione gaussiana 2D, il che giustifica l’uso della convoluzione con una gaussiana nel modello dell’immagine biomedica per modellare il PVE. In via di principio è possibile fare l’imaging di un oggetto puntiforme (in realtà per evidenti motivi fisici un oggetto di dimensioni molto piccole) e tracciare i profili della PSF risultante che saranno delle funzioni gaussiane. La larghezza di tali profili (espressa ad esempio come SD della gaussiana) sarà un misura della bontà del sistema di imaging. E’ evidente che questo metodo richiede l’uso di un fantoccio standardizzato o di una procedura di imaging standardizzata per ottenere la PSF.\n\nInfine un profilo tracciato sull’immagine di un fantoccio omogeneo che riempia tutto o buona parte del campo visivo (FOV) del dispositivo può permettere di valutare il campo di attenuazione g introdotto nel modello di immagine biomedica. Nel caso di g unitario (quindi assenza di attenuazione) il profilo dovrebbe risultare perfettamente orizzontale, mentre per g < 1 compare una curvatura del profilo, tanto maggiore quanto maggiore è l’attenuazione.","type":"content","url":"/c11-imm-biom#analisi-dei-profili","position":41},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Artefatti","lvl2":"Qualità dell’immagine biomedica"},"type":"lvl3","url":"/c11-imm-biom#artefatti","position":42},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Artefatti","lvl2":"Qualità dell’immagine biomedica"},"content":"Le immagini biomediche possono essere corrotte da artefatti. Gli artefatti sono strutture riconoscibili, quindi con una coerenza interna, che appaiono sull’immagine anche se non esistono nell’oggetto reale. Il rumore è invece un disturbo completamente casuale, privo di struttura e distribuito uniformemente sull’immagine. La figura mostra due esempi di artefatti in immagini MRI. Gli artefatti sono tipicamente non predicibili e vengono valutati attraverso la probabilità della loro comparsa su lunghe serie di acquisizioni. Alcuni tipi di artefatto possono essere dovuti a motivi fisiologici, si pensi all’effetto del respiro del paziente o alle irregolarità del segnale ECG in acquisizioni cardiache sincronizzate con l’ECG stesso.\n\nFigura 1.29. Esempi di artefatto.","type":"content","url":"/c11-imm-biom#artefatti","position":43},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl2":"Software per l’elaborazione di immagini biomediche"},"type":"lvl2","url":"/c11-imm-biom#software-per-lelaborazione-di-immagini-biomediche","position":44},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl2":"Software per l’elaborazione di immagini biomediche"},"content":"L’elaborazione delle immagini biomediche in ambito clinico deve avvenire, come prima descritto, attraverso software certificato. Esistono numerosi software commerciali, sia prodotti dalle case costruttrici dei dispositivi di acquisizione che da ditte indipendenti dalle stesse. Nel primo caso il software può essere integrato nella macchina di acquisizione (in questo caso la certificazione si opera sull’intera apparecchiatura) o “stand alone”, quindi installato su un computer (nel gergo dei sistemi PACS workstation). Nel secondo caso il software è necessariamente stand alone.\nUn software stand alone residente su di una workstation deve integrarsi con il sistema PACS su cui risiedono le immagini e quindi è tipicamente composto da una interfaccia di base che consente di scaricare o importare immagini in formato DICOM ed inserirle in un archivio interno. Da tale archivio le immagini possono essere estratte per l’elaborazione. I risultati dell’elaborazione vengono salvati sulla workstation. I software clinici sono tipicamente molto costosi (decine di migliaia di euro) e vengono di solito distribuiti con un sistema di licenze annuali che includono l’aggiornamento ed il supporto.\n\nPer scopi di ricerca o di studio sono disponibili vari software stand alone gratuiti (chiaramente non certificati) che possono essere utilizzati per l’archiviazione e l’elaborazione di immagini biomediche.\n\nAlcuni esempi sono:\n\nimageJ (\n\nhttps://​imagej​.net​/ij​/index​.html). E’ un software multipiattaforma (Open Source, Java) sviluppato dall’ NIH (National Institutes of Health, USA). E’ un software essenziale che permette di importare immagini in formato DICOM ed eseguire numerosi tipi di elaborazione. E’ completato da una collezione di plugins (\n\nhttps://​imagej​.nih​.gov​/ij​/plugins/), cioè applicazioni sviluppate da utenti del software, che implementano vari algoritmi di elaborazione. Essendo il software open source, è possibile implementare nuovi plugins o modificare quelli esistenti.\n\nMIPAV (\n\nhttps://​mipav​.cit​.nih​.gov/) E’ un software multipiattaforma (Java) sviluppato dall’ NIH (National Institutes of Health, USA).  E’ un software che permette di importare immagini in formato DICOM ed eseguire numerosi tipi di elaborazione maggiormente evoluto rispetto ad imageJ. Il software non è Open Source ma permette di sviluppare plugins integrabili con il MIPAV.\n\n3D Slicer (\n\nhttps://​www​.slicer​.org/). E’ un software multipiattaforma (C++, Tcl/Tk). E’ un software che permette di importare immagini in formato DICOM ed eseguire numerosi tipi di elaborazione. Rispetto a imageJ e MIPAV permette di elaborare efficacemente dati di tipo volumetrico. Il software è Open Source e permette di sviluppare plugins.\n\nHoros (\n\nhttps://​horosproject​.org/). E’ un software free/open source solo su piattaforma MacOS che implementa la struttura di una vera e propria workstation radiologica, quindi permette di sperimentare un ambiente simile a quello dei prodotti clinici. E’ la versione free di Osirix (\n\nhttps://​www​.osirix​-viewer​.com/) che implementa una versione certificata ad uso clinico.\n\nEsistono molti altri software che consentono la gestione di file DICOM e implementano algoritmi di elaborazione delle bioimmagini. Una lista è disponibile su \n\nhttps://​idoimaging​.com/.\nL’elaborazione delle immagini biomediche può essere naturalmente eseguita sviluppando i propri algoritmi nei vari ambienti di programmazione. In questo corso utilizzeremo l’ambiente MATLAB, un ambiente commerciale che consente di importare immagini DICOM e di utilizzare su queste le varie funzioni di elaborazione disponibili.\nIn alternativa è possibile utilizzare ambienti free come python (\n\nhttps://​www​.python​.org/), o qualsiasi linguaggio di programmazione che possieda librerie per la gestione del formato DICOM. In python è anche disponibile lo strumento ParaView (\n\nhttps://​www​.paraview​.org/), particolarmente utile nella gestione di mesh tridimensionali. In python sono sviluppati i principali ambienti per l’elaborazione di immagini biomediche attraverso reti neurali (CNN) come Keras e Pytorch.","type":"content","url":"/c11-imm-biom#software-per-lelaborazione-di-immagini-biomediche","position":45},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl2":"Bibliografia"},"type":"lvl2","url":"/c11-imm-biom#bibliografia","position":46},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl2":"Bibliografia"},"content":"Constantinides CD, Atalar E, McVeigh ER. Signal-to-noise measurements in magnitude images from NMR phased arrays. Magnetic Resonance in Medicine 1997;38(5):852–857.\n\nR Rangayyan, Biomedical Image Analysis, CRC Press 2004","type":"content","url":"/c11-imm-biom#bibliografia","position":47},{"hierarchy":{"lvl1":"Esempio degli effetti di corruzione dell’immagine - Notebook 1.1"},"type":"lvl1","url":"/n-1-1-effetti-corr-imm-entr","position":0},{"hierarchy":{"lvl1":"Esempio degli effetti di corruzione dell’immagine - Notebook 1.1"},"content":"Import delle librerie necessarie per la simulazione\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage import convolve\nfrom scipy.signal.windows import gaussian\n\n\n\n# Creo il fantoccio sul quale simulare gli effetti di PVE, attenuazione e rumore\ndim = 512\nimage = np.zeros((dim, dim), dtype=np.float32)\n\nimage[:, :] = 50\nimage[49:100, 49:100] = 120\nimage[100:180, 100:450] = 200\nimage[199:500, 199:350] = 90\nimage[229:270, 229:270] = 250\nimage[4:400, 449:500] = 150\n\n\n\nVisualizzazione + istogramma\n\nfig, ax = plt.subplots(1, 2, figsize=(9, 4))\n\nax[0].imshow(image, cmap=\"gray\")\nax[0].set_title(\"Immagine ideale\")\nax[0].axis(\"off\")\n\nax[1].hist(image.ravel(), bins=50)\nax[1].set_title(\"Istogramma\")\n\nplt.show()\n\n\n\nEffetto PVE\n\n# filtro convolutivo gaussiano per simulare l'effetto PVE\nksize = 11\nsigma = 5\n\ng1d = gaussian(ksize, sigma)\nh = np.outer(g1d, g1d)\nh /= h.sum()\n\nimage_pve = convolve(image, h, mode=\"reflect\")\n\n# visualizzazione\nfig, ax = plt.subplots(1, 2, figsize=(9, 4))\n\nax[0].imshow(image_pve, cmap=\"gray\")\nax[0].set_title(\"Immagine con PVE\")\nax[0].axis(\"off\")\n\nax[1].hist(image_pve.ravel(), bins=50)\nax[1].set_title(\"Istogramma\")\n\nplt.show()\n\n\n\nEffetto attenuazione (campo parabolico)\n\n# simulazione dell'effetto di attenuazione\nx, y = np.meshgrid(np.arange(dim), np.arange(dim))\nxc = dim / 2\nyc = dim / 2\n\nsigma_att = dim / 2\nattenuation_field = 1 - ((x - xc)**2) / (2 * sigma_att**2)\nattenuation_field = np.maximum(attenuation_field, 0)\n\nimage_att = image_pve * attenuation_field\n\n# visualizzazione\nfig, ax = plt.subplots(1, 2, figsize=(9, 4))\n\nax[0].imshow(image_att, cmap=\"gray\")\nax[0].set_title(\"Immagine attenuata\")\nax[0].axis(\"off\")\n\nax[1].hist(image_att.ravel(), bins=50)\nax[1].set_title(\"Istogramma\")\n\nplt.show()\n\n\n\n\nRumore additivo Gaussiano\n\n# simulazione del rumore\nnp.random.seed(0)  # riproducibilità (opzionale)\nimage_final = image_att + 5 * np.random.randn(dim, dim)\n\n# visualizzazione\nfig, ax = plt.subplots(1, 2, figsize=(9, 4))\n\nax[0].imshow(image_final, cmap=\"gray\")\nax[0].set_title(\"Immagine finale (rumore)\")\nax[0].axis(\"off\")\n\nax[1].hist(image_final.ravel(), bins=50)\nax[1].set_title(\"Istogramma\")\n\nplt.show()\n\n\n\n\nImmagine differenza\n\ndif = image - image_final\n\nfig, ax = plt.subplots(1, 2, figsize=(9, 4))\n\nax[0].imshow(dif, cmap=\"gray\")\nax[0].set_title(\"Immagine differenza\")\nax[0].axis(\"off\")\n\nax[1].hist(dif.ravel(), bins=50)\nax[1].set_title(\"Istogramma\")\n\nplt.show()\n\n\n","type":"content","url":"/n-1-1-effetti-corr-imm-entr","position":1},{"hierarchy":{"lvl1":"Stima dell’entropia per diversi tipi di istogramma - Notebook 1.2"},"type":"lvl1","url":"/n-1-2-entropia","position":0},{"hierarchy":{"lvl1":"Stima dell’entropia per diversi tipi di istogramma - Notebook 1.2"},"content":"Import delle librerie necessarie alla simulazione\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom skimage.measure import shannon_entropy\nfrom scipy.ndimage import convolve\nfrom scipy.signal.windows import gaussian\n\n\n\nRumore uniformemente distribuito\n\nim = np.random.rand(512, 512)\n\nfig, ax = plt.subplots(1, 2, figsize=(9, 4), constrained_layout=True)\nax[0].imshow(im, cmap=\"gray\", vmin=0, vmax=1)\nax[0].axis(\"off\")\nax[1].hist(im.ravel(), bins=50)\nax[1].set_title(f\"entropia = {shannon_entropy(im):.4f}\")\nplt.show()\n\n\n\n\nImmagine a singolo livello di grigio\n\nim = 255 * np.ones((512, 512), dtype=float)\n\nfig, ax = plt.subplots(1, 2, figsize=(9, 4), constrained_layout=True)\nax[0].imshow(im, cmap=\"gray\", vmin=0, vmax=255)\nax[0].axis(\"off\")\nax[1].hist(im.ravel(), bins=50)\nax[1].set_xlim(240, 260)\nax[1].set_title(f\"entropia = {shannon_entropy(im):.4f}\")\nplt.show()\n\n\n\n\nImmagine con 2 livelli di grigio\n\nim = np.zeros((512, 512), dtype=float)\nim[49:100, 49:100] = 100 \n\nfig, ax = plt.subplots(1, 2, figsize=(9, 4), constrained_layout=True)\nax[0].imshow(im, cmap=\"gray\", vmin=0, vmax=255)\nax[0].axis(\"off\")\nax[1].hist(im.ravel(), bins=50)\nax[1].set_title(f\"entropia = {shannon_entropy(im):.4f}\")\nplt.show()\n\n\n\n\nImmagine con due livelli di grigio e rumore Gaussiano\n\nim = np.zeros((512, 512), dtype=float)\nim[49:100, 49:100] = 100\nim = im + 10 * np.random.randn(512, 512)\n\nfig, ax = plt.subplots(1, 2, figsize=(9, 4), constrained_layout=True)\nax[0].imshow(im, cmap=\"gray\")  \nax[0].axis(\"off\")\nax[1].hist(im.ravel(), bins=80)\nax[1].set_title(f\"entropia = {shannon_entropy(im):.4f}\")\nplt.show()\n\n\n\n\nImmagine con due livelli di grigio, rumore Gaussiano e PVE\n\nim = np.zeros((512, 512), dtype=float)\nim[49:100, 49:100] = 100\n\nksize = 25\nsigma = 11\n\ng = gaussian(ksize, sigma)\nh = np.outer(g, g)\nh = h / h.sum()\n\nim = convolve(im, h, mode=\"reflect\")\n\nfig, ax = plt.subplots(1, 2, figsize=(9, 4), constrained_layout=True)\nax[0].imshow(im, cmap=\"gray\")\nax[0].axis(\"off\")\nax[1].hist(im.ravel(), bins=80)\nax[1].set_title(f\"entropia = {shannon_entropy(im):.4f}\")\nplt.show()\n\n\n","type":"content","url":"/n-1-2-entropia","position":1},{"hierarchy":{"lvl1":"Esempio di rumore riciano in MRI - Notebook 1.3"},"type":"lvl1","url":"/n-1-3-mr-noise-example","position":0},{"hierarchy":{"lvl1":"Esempio di rumore riciano in MRI - Notebook 1.3"},"content":"Import delle librerie necessarie per la simulazione.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nSimuliamo il segnale proveniente dallo sfondo come Gaussiano a media nulla. Simuliamo l’acquisizione su due canali (fase e quadratura) che rappresenteranno la parte reale e la parte immaginaria del segnale acquisito. Visualizziamo quindi il segnale su ogni canale e sulla loro somma quadratica.\n\n\n########## sfondo ############\nsigma = 10\ndim = 256\n\nimageR = np.random.randn(dim,dim)\nimageI = np.random.randn(dim,dim)\nimageM = np.sqrt(np.square(imageR)+np.square(imageI))\n\ncountsR, binsR = np.histogram(imageR, bins = 30)\ncountsI, binsI = np.histogram(imageI, bins = 30)\ncountsM, binsM = np.histogram(imageM, bins = 30)\n\nfig, axes = plt.subplots(1,3)\naxes[0].hist(binsR[:-1], binsR, weights=countsR)\naxes[1].hist(binsI[:-1], binsI, weights=countsI)\naxes[2].hist(binsM[:-1], binsM, weights=countsM)\n\nplt.show()\n\n\n\nOra invece, simuliamo il segnale proveniente da un tessuto con S = 100. Simuliamo l’acquisizione su due canali (fase e quadratura) che rappresenteranno la parte reale e la parte immaginaria del segnale acquisito. Visualizziamo quindi il segnale su ogni canale e sulla loro somma quadratica.\n\n########## immagine ############\nsigma = 10\ndim = 256\n\nimageR = 100 + np.random.randn(dim,dim)\nimageI = 100 + np.random.randn(dim,dim)\nimageM = 100 + np.sqrt(np.square(imageR)+np.square(imageI))\n\ncountsR, binsR = np.histogram(imageR, bins = 30)\ncountsI, binsI = np.histogram(imageI, bins = 30)\ncountsM, binsM = np.histogram(imageM, bins = 30)\n\nfig, axes = plt.subplots(1,3)\naxes[0].hist(binsR[:-1], binsR, weights=countsR)\naxes[1].hist(binsI[:-1], binsI, weights=countsI)\naxes[2].hist(binsM[:-1], binsM, weights=countsM)\n\nplt.show()\n\n\n\nE’ possibile notare dal confronto degli istogrammi che il segnale sullo sfondo non segue una distribuzione Gaussiana, come invece avviene per il segnale proveniente dal tessuto.","type":"content","url":"/n-1-3-mr-noise-example","position":1},{"hierarchy":{"lvl1":"Esercitazione 1: stima dei parametri di qualità di un’immagine biomedica"},"type":"lvl1","url":"/z-1-1-esercitazione","position":0},{"hierarchy":{"lvl1":"Esercitazione 1: stima dei parametri di qualità di un’immagine biomedica"},"content":"","type":"content","url":"/z-1-1-esercitazione","position":1},{"hierarchy":{"lvl1":"Esercitazione 1: stima dei parametri di qualità di un’immagine biomedica","lvl2":"Teoria"},"type":"lvl2","url":"/z-1-1-esercitazione#teoria","position":2},{"hierarchy":{"lvl1":"Esercitazione 1: stima dei parametri di qualità di un’immagine biomedica","lvl2":"Teoria"},"content":"La misura del rumore associato ad una immagine biomedica è un punto fondamentale in molte applicazioni, come l’analisi della qualità di immagine e la realizzazione di filtri adattivi. Per la misura del rumore sono stati sviluppati vari approcci, che possono essere divisi in due categorie:\n\nBasati sul tracciamento di ROI\n\nBasati sull’analisi dell’istogramma\n\nNei metodi basati su ROI, una o più ROI vengono tracciate sui tessuti di interesse, definendo delle zone omogenee ed evitando che la misura sia influenzata dal PVE o dalla presenza di attenuazione. Il rumore sul tessuto viene stimato come la deviazione standard (SD) del segnale sulla ROI. Le ROI possono essere definite manualmente o automaticamente attraverso algoritmi di segmentazione che saranno illustrati nel seguito. La definizione automatica è molto semplificata nel caso di fantocci per la misura della qualità dell’immagine di cui è nota la forma e posizione nello scanner.\n\nNella misura attraverso ROI il punto fondamentale è il compromesso tra grandezza della ROI e necessità di posizionare la ROI in una regione omogenea.\n\nIn Figura 1.30 osserviamo le misure di Media e SD del segnale su ROI di dimensioni crescenti (100 realizzazioni di rumore, ROI quadrate). Si osserva chiaramente come la misura della SD sia più critica rispetto a quella del valor medio e come per la SD siano necessarie ROI con qualche decina di pixel per avere una misura affidabile. Da queste considerazioni derivano gli approcci a sottrazione di immagini e misura del rumore sul fondo utilizzati nella pratica clinica con lo scopo di aumentare il numero di pixel disponibili per la stima della SD.\n\nFigura 1.30. Stima di Media e SD su ROI al variare della dimensione della ROI.\n\nConsideriamo ad esempio il caso in Figura 1.31 di immagini cardiache in LGE (Late Gadolinium Enhancement). Lo scopo di questo tipo di immagini con contrasto è verificare l’estensione dell’infarto miocardio cronico, dove si deposita il contrasto, che appare bianco, mentre il miocardio sano appare con segnale annullato (in realtà avremo sempre un segnale diverso da zero per la natura del rumore MR). Per valutare la validità clinica delle immagini bisogna valutare il CNR tra la zona di LGE (cicatrice da infarto, ROI 1) ed il miocardio sano (ROI 2) (Figura 1.32).\n\nFigura 1.31. Immagini cardiache in LGE.\n\nFigura 1.32. Tracciamento di ROI su immagini cardiache.\n\nCome si vede le regioni in cui tracciare le ROI sono molto piccole, l’area LGE è di 60 mm^2 e l’area miocardio di 140 mm^2, corrispondenti (pixel size = 1.5 mm) a 26 e 60 pixel circa. La misura della SD sarà quindi inaffidabile. E’ molto più affidabile una misura fatta nello sfondo che permette di tracciare una ROI di 3350 mm^2 (1550 pixel circa). In questo caso essendo il miocardio annullato (segnale zero) e quindi con distribuzione di rumore simile al fondo il valore di CNR sarà:CNR=2(M_{LGE}-M_{MIO})/(SD_{BK}+1.526S_{BK})\n\nI metodi basati sull’analisi dell’istogramma cercano invece di caratterizzare il rumore dall’immagine nel suo complesso. In questo caso si utilizza tutta l’informazione contenuta nell’immagine per cui il problema visto nelle ROI non si pone. Consideriamo l’immagine ideale a sei pattern corrotta solo da rumore gaussiano ed il suo istogramma riportati in Figura 1.33.\n\nFigura 1.33. Fantoccio a 6 pattern con rumore Gaussiano e relativo istogramma.\n\nSe tracciamo una ROI all’interno di un pattern la SD del segnale nella ROI sarà sempre la stessa dipendendo dal rumore sommato all’immagine. Se invece tracciamo una ROI a cavallo di due pattern misureremo un valore più alto dovuto alla presenza di due tessuti con segnali diversi all’interno della ROI.\n\nFigura 1.34. Mappa di SD e relativo istogramma.\n\nAutomatizzando la procedura, possiamo far scorrere sull’immagine un kernel (ad esempio 5x5) che rappresenta la ROI e valutare il valore della deviazione standard SD sulla regione di immagine coperta dal kernel. In MATLAB questa operazione è implementata dalla funzione stdfilt. Si otterranno quindi un numero di valori di SD uguale al numero di pixel dell’immagine. Nelle regioni all’interno dei pattern uniformi il valore della SD sarà pari alla deviazione standard del rumore σ, almeno nel caso di rumore gaussiano additivo. Nelle regioni a cavallo di due pattern il valore di SD sarà maggiore di σ. Come al solito la dimensione del kernel rappresenterà un compromesso tra l’esigenza di calcolare correttamente la SD e la necessità di avere la maggior parte delle misure eseguita su regioni omogenee.\nCome si vede dalla figura, l’istogramma della mappa SD avrà un picco in corrispondenza del valore di \\sigma ed una serie di valori più alti corrispondenti alle transizioni distribuiti in modo uniforme.\nSe l’immagine è composta prevalentemente da regioni omogenee (pochi pattern di forma regolare) si può supporre che il contributo delle regioni di transizione sia trascurabile e calcolare \\sigma come media della mappa:\\sigma = mean(M_{SD})\n\nIn realtà è preferibile eliminare gli outliers dovuti ai bordi e computare la mediana della mappa SD invece che la media, cosa che consente di ottenere una stima più accurata. Infatti, al contrario della media, la mediana pesa meno gli alti valori di SD nella parte destra dell’istogramma.\\sigma = median(M_{SD})\n\nSe l’immagine presenta molte transizioni il metodo della mediana può dare risultati non corretti. In questo caso è possibile utilizzare l’istogramma calcolando il valore di σ dal picco principale (tipicamente il primo) dell’istogramma che come visto contiene l’informazione sulle regioni omogenee. Il valor medio di σ misurato sarà dato dalla posizione sull’asse x del massimo valore dell’istogramma h. Cercheremo quindi il massimo dell’istogramma e porremo \\sigma uguale al valore di x corrispondente a tale massimo:\\sigma = x:h(x)=max(h)\n\nIn Figura 1.35 è riportata la stima della SD per i diversi metodi.\n\nFigura 1.35. Valori stimati di SD per diversi metodi.\n\nQuesto approccio è simile ad un altro metodo utilizzato per la stima del rumore nel quale l’immagine viene divisa in N quadrati di lato k non sovrapposti di cui viene calcolata la SD, ed il valore di \\sigma viene stimato come media degli m campioni con valore di SD più basso (al limite un singolo campione). Il metodo basato sull’istogramma è comunque in generale più accurato.\nCome si osserva dal grafico il metodo Mean sovrastima il valore del rumore, mentre gli altri due metodi danno una stima sostanzialmente corretta.\nTutti questi metodi si basano sull’assunzione di rumore gaussiano additivo. Se come avviene tipicamente nelle immagini biomediche questa assunzione non è verificata, bisogna operare sulla base della conoscenza del processo di acquisizione. Consideriamo ad esempio l’immagine MR di un phantom cilindrico, composto da tre cilindri concentrici, due riempiti con acqua e quello intermedio con olio (valore di segnale più alto). Essendo il fantoccio riempito con liquido perfettamente omogeneo il rumore biologico è trascurabile. Le dimensioni del fantoccio sono piccole rispetto al bore della macchina MR (15 cm circa) e quindi si può considerare nulla l’attenuazione. Come si osserva nell’immagine a destra ottenuta con una opportuna finestra di windowing, alcuni pixel dell’immagine non sono stati ricostruiti dal K-spazio ma rappresentano un “riempimento” per ottenere una immagine quadrata (zero-padding). A tali pixel in MR viene assegnato il valore convenzionale 0 e devono essere ignorati nell’elaborazione (Figura 1.36).\n\nFigura 1.36. Zone di zero-padding evidenziate.\n\nCome sappiamo il valore di \\sigma può essere stimato tracciando una ROI su un tessuto omogeneo oppure sul fondo introducendo un opportuno fattore di correzione che è noto trattandosi di una immagine MR.\nProcedendo in questo modo otteniamo i valori di \\sigma per acqua e olio (uguali a meno dell’errore sperimentale) e del fondo che come ci aspettavamo è più basso. Dal valore stimato sul fondo possiamo ottenere la stima corretta applicando il fattore di conversione 1.526 (Figura 1.37).\n\nFigura 1.37. Stima di SD basato su ROI.\n\nSe vogliamo applicare i metodi automatici basati sull’istogramma dobbiamo calcolare la mappa SD ed il relativo istogramma, dove abbiamo eliminato dal computo i pixel a valore nullo (zero padding) (Figura 1.38).\n\nFigura 1.38. Stima della SD automatica.\n\nCome vediamo dal grafico abbiamo un singolo picco, che è però la combinazione dei valori di SD del fondo e di quelli dei due tessuti ad alto SNR. Il calcolo della media della mappa SD sovrastima fortemente \\sigma, in quanto gli alti valori di SD sui bordi alzano la media in modo significativo. Il valore della mediana è abbastanza simile alla SD del rumore di fondo, che rappresenta una parte rilevante dell’immagine. Considerando il massimo dell’istogramma si ha un valore addirittura minore alla SD del fondo. Tali valori dipendono dal rapporto tra numero di pixel del fondo e numero di pixel ad alto SNR e non sono generalizzabili. I valori di SD per i tre metodi sono disponibili nella seguente tabella:\n\nSD mean\n\nSD median\n\nmax hist\n\n22.3108\n\n10.0968\n\n7.8155\n\nIn questo caso sarà necessario identificare in qualche modo le due classi di pixel attraverso un opportuno algoritmo di segmentazione. Banalmente, se consideriamo solo i pixel con livello di grigio superiore a 100 (quindi solo acqua e olio, vedremo in seguito come ottenere tale soglia) abbiamo come istogramma della SD quello di Figura 1.39.\n\nFigura 1.39. Mappa di deviazione standard e relativo istogramma.\n\nCon le seguenti stime per la SD:\n\nSD mean\n\nSD median\n\nmax hist\n\n69.5255\n\n15.7743\n\n12.3800\n\nCome si osserva la stima con i metodi median e max hist migliora in modo significativo. In definitiva per la valutazione automatica del rumore nell’immagine biomedica occorre una scelta oculata dell’algoritmo sulla base delle caratteristiche dell’immagine.","type":"content","url":"/z-1-1-esercitazione#teoria","position":3},{"hierarchy":{"lvl1":"Esercitazione 1: stima dei parametri di qualità di un’immagine biomedica","lvl2":"Esercitazione"},"type":"lvl2","url":"/z-1-1-esercitazione#esercitazione","position":4},{"hierarchy":{"lvl1":"Esercitazione 1: stima dei parametri di qualità di un’immagine biomedica","lvl2":"Esercitazione"},"content":"Lo scopo dell’esercitazione è replicare le misure di qualità dell’immagine che vengono eseguite in un laboratorio MR in modo routinario. L’immagine a disposizione è quella di un fantoccio sferico utilizzato per la valutazione del rapporto segnale rumore e dell’uniformità di segnale (quindi una valutazione dell’eventuale presenza di attenuazione) (Figura 1.40).\nL’immagine è stata acquisita su una macchina MR Signa HDxt General Electric a 3 Tesla, come si può osservare dall’header DICOM. Il valore del campo PatientID = ‘geservice’ indica che le immagini sono state acquisite per un controllo di qualità. L’immagine è una immagine 2D Fast Spin Echo. Il fantoccio è una sfera con diametro 26 cm acquisita in modo da ottenere la massima sezione. Il FOV dovrebbe essere centrato sul centro della sfera, in realtà è abbastanza disassato ed ha il centro intorno a (233 253).\n\nFigura 1.40. Fantoccio.\n\nPer la definizione del protocollo di misura ci riferiamo al Protocollo definito dalla CONSIP per l’esecuzione dei controlli di qualità su scanner MR. La CONSIP è la centrale acquisti per la pubblica amministrazione italiana, e gestisce quindi anche gli acquisti nella sanità pubblica. In particolare, la CONSIP dovrebbe riuscire a migliorare la qualità degli acquisti e ridurre i costi grazie all’aggregazione della domanda sul tutto il territorio nazionale. Nella pratica con cadenza periodica (tipicamente due anni) la CONSIP bandisce una gara per l’acquisizione di apparecchiature mediche e valuta la qualità delle apparecchiature oltre che il costo proposto dai fornitori partecipanti. In base alla qualità rilevata e al costo viene selezionato un fornitore al quale nei due anni successivi si dovrà rivolgere preferibilmente ogni struttura sanitaria pubblica. Il protocollo, che è pubblico, è disponibile come materiale integrativo del corso.\nPer la misura dell’SNR il protocollo è tipicamente strutturato come:\n\nDefinire sull’immagine una ROI (ROI75) posizionata al centro dell’oggetto test di dimensioni pari al 75% dell’oggetto. Determinare il valor medio del segnale nella ROI75.\n\nDefinire una ROI (ROI10) in una zona priva di segnale (fondo) di dimensioni pari al 10% dell’oggetto. Determinare il valor medio del segnale nella ROI10 (valore di baseline).\n\nDeterminare il segnale S come differenza dei valori di segnale tra ROI75 e ROI10.\n\nValutare il rumore N sull’immagine come la deviazione standard del segnale all’interno della ROI75\n\nCalcolare il valore di SNR come SNR=S/N.\n\nIl motivo per cui il protocollo prevede il calcolo della baseline (punto 2) è che il produttore della macchina potrebbe sommare arbitrariamente un valore costante all’immagine facendo salire S e quindi il valore di SNR senza modificare il contrasto. Quindi in realtà la procedura misura il valore di CNR tra il fantoccio ed il fondo.\nDovremo quindi realizzare un programma MATLAB che implementi in modo automatico la procedura del protocollo assicurando che vengano rispettate le indicazioni, in particolare quelle sulle dimensioni delle ROI e che fornisca in uscita il valore di SNR.\n\nIl MATLAB fornisce varie funzioni per il tracciamento di ROI. La più semplice è la funzione getrect che permette di tracciare una ROI rettangolare con il mouse su una immagine. Funzioni più complete sono quelle del toolbox “ROI-Based Processing” come drawfreehand che permette di tracciare una ROI a mano libera, drawcircle (cerchio), drawellipse (ellisse), drawpolygon (poligono generico), drawrectangle (rettangolo), etc. In Python, funzionalità analoghe sono fornite da librerie di visualizzazione e image processing.\nIn particolare, la libreria matplotlib permette di tracciare ROI rettangolari interattive\ntramite widget dedicati, mentre la libreria napari offre strumenti più completi per il\ndisegno e la gestione di ROI interattive. Napari consente il tracciamento di ROI a mano libera, circolari, ellittiche, poligonali e\nrettangolari, in modo analogo alle funzioni drawfreehand, drawcircle, drawellipse,\ndrawpolygon e drawrectangle del toolbox ROI-Based Processing di MATLAB.\n\nIl calcolo dell’uniformità, e quindi della presenza di un campo di attenuazione, viene effettuato con la seguente procedura:\n\nDefinire sull’immagine una ROI (ROI80) posizionata al centro dell’oggetto test di dimensioni pari al 80% dell’oggetto.\n\nDeterminare il valor medio del segnale nella ROI80 (Sm) ed il numero di pixel contenuti nella ROI80 (N)\n\nDeterminare la deviazione media assoluta AAD = \\sum _{i} |S_{i}-S_{m}|/N_{i} dove Si è il valore di segnale dei singoli pixel contenuti nella ROI80\n\nCalcolare UH = 1-ADD/Sm\n\nIn assenza di rumore, se non c’è attenuazione ADD=0 e UH=1 (massima uniformità, nessuna attenuazione). Se c’è attenuazione, ci sarà una variazione di segnale e ADD sarà maggiore di zero abbassando il valore di UH. Per un’immagine reale ci sarà presenza di rumore e ADD sarà comunque diverso da zero in quanto il segnale varierà. Il presupposto della misura è che Sm sarà molto alto, in quanto il fantoccio sarà costruito a questo scopo, per cui in assenza di attenuazione UH sarà molto vicino ad uno.\nAnche in questo caso dovremo realizzare un programma MATLAB automatico che implementi la procedura del protocollo assicurando che vengano rispettate le indicazioni, in particolare quelle sulle dimensioni delle ROI, e che fornisca in uscita il valore di UH.\nInfine, andiamo a valutare la presenza di PVE sull’immagine 1 del fantoccio calcolando l’acutezza della transizione. A questo scopo occorre definire un profilo, cioè un segmento posto sull’immagine che intersechi una transizione, ed estrarre il grafico del valore di segnale sul profilo, come esemplificato in figura per il software ImageJ.\n\nFigura 1.41. Tracciamento di un profilo.\n\nDato il profilo, il valore di acutezza si ottiene utilizzando la formula:A = \\frac{1}{f(b) - f(a)} \\int_{a}^{b}\n\\left[ \\frac{d}{dx} f(x) \\right]^2 \\, dx\n\nDove f è il profilo estratto e a e b sono l’inizio e la fine della transizione. Essendo l’immagine discreta, l’integrale viene approssimato come:A = \\frac{1}{f(b) - f(a)} \\sum_{a}^{b}\n\\left[ \\frac{f(i+1)-f(i)}{d}\\right]^2\n\nDove d è la dimensione del pixel. I punti a e b (inizio e fine della transizione) possono essere definiti manualmente, ma è opportuno automatizzare la procedura per ridurre la variabilità dovuta all’osservatore. Da un punto di vista teorico, all’inizio del profilo il segnale avrà un valore dato dalla baseline (valor medio del fondo) più il rumore con SD sigma. Potremmo definire una soglia conservativa (come baseline + 4\\sigma) al di sopra della quale è estremamente improbabile che il segnale misurato sul profilo sia originato da un pixel del fondo. Analogamente potremmo definire una soglia per la fine della transizione. Nella pratica si utilizza un approccio più semplice, ad esempio, possiamo convenire che a e b siano i punti in cui il segnale raggiunge il 10% ed il 90% del suo valore massimo teorico misurato in precedenza (Sm), rispettivamente. Andranno quindi fissate due soglie 0.1*Sm e 0.9*Sm in modo da trovare a e b.\n\nDovremo realizzare quindi un programma Python che implementi il calcolo dell’acutezza su di un profilo definibile dall’utente o calcolato in modo automatico.\n\nRisultati attesi:\n\nSNR\n\nUH\n\nAcutezza\n\n8\n\n0.90\n\n415","type":"content","url":"/z-1-1-esercitazione#esercitazione","position":5},{"hierarchy":{"lvl1":"Effetto del PVE sul profilo - Notebook 1.4"},"type":"lvl1","url":"/zn-1-4-profile","position":0},{"hierarchy":{"lvl1":"Effetto del PVE sul profilo - Notebook 1.4"},"content":"Import delle librerie necessarie per la simulazione\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.ndimage as ndi\n\n\n\nSimuliamo un oggetto di intensità 255 su sfondo a intensità nulla e aumentiamo il fattore PVE tramite filtro gaussiano per apprezzarne l’effetto sul profilo.\n\nIl profilo viene tracciato orizzontalmente\n\n# immagine iniziale\ndim = 512\nim0 = np.zeros((dim, dim))\nim0[200:400, 200:400] = 255\n\n# kernel\nksize = 9\nh = np.ones((ksize, ksize)) / (ksize**2)\n\n# numero di volte in cui viene applicato il filtraggio, per simulare un PVE sempre maggiore\nn_iters = [0, 1, 5, 20, 50]\n\nfig, ax = plt.subplots(\n    len(n_iters), 2,\n    figsize=(8, 2.5 * len(n_iters)),\n    constrained_layout=True\n)\n\nfor i, n in enumerate(n_iters):\n\n    im = im0.copy()\n    for _ in range(n):\n        im = ndi.convolve(im, h)\n\n    # immagine\n    ax[i, 0].imshow(im, cmap='gray', vmin=0, vmax=255)\n    ax[i, 0].set_title(f\"Iterazioni filtro: {n}\")\n    ax[i, 0].axis(\"off\")\n\n    # profilo\n    ax[i, 1].plot(im[300, :])\n    ax[i, 1].set_ylim(0, 260)\n    ax[i, 1].set_title(\"Profilo riga 300\")\n\nplt.show()\n\n\n","type":"content","url":"/zn-1-4-profile","position":1},{"hierarchy":{"lvl1":"Effetto della dimensione della ROI sulla stima di M e SD - Notebook 1.5"},"type":"lvl1","url":"/zn-1-5-test-m-sd","position":0},{"hierarchy":{"lvl1":"Effetto della dimensione della ROI sulla stima di M e SD - Notebook 1.5"},"content":"Import delle librerie necessarie alla simulazione\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.ndimage as ndi\n\n\n\n\n# simulated SD\nTsd = 10\n\n# simulated mean\nTm = 300\n\n# plot mean/SD dispersion vs ROI dimension\nN = 100\n\nfigure, ax = plt.subplots(1,2, constrained_layout=True)\nax[0].set_xlabel('ROI dim')\nax[0].set_ylabel('Mean (imposed 300)')\nax[0].set_ylim(250,350)\nax[1].set_xlabel('ROI dim')\nax[1].set_ylabel('sd (imposed 10)')\nax[1].set_ylim(0,20)\n\nfor k in range(N):\n    dim = np.array([],int)\n    M   = np.array([])\n    sd  = np.array([])\n    for i in range(1,10):\n        # ROI area\n        new_el = (i)*(i)\n        dim = np.append(dim, new_el) \n\n        # generate image with gaussian noise\n        image = Tm + Tsd*np.random.randn(dim[i-1])\n        M  = np.append(M, np.mean(image, None))\n        sd = np.append(sd, np.std(image, None))\n    ax[0].plot(dim,M)\n    ax[1].plot(dim,sd)\nplt.show()\n\n\n\n","type":"content","url":"/zn-1-5-test-m-sd","position":1},{"hierarchy":{"lvl1":"Esempio calcolo SD su fantoccio e immagine reale - Notebook 1.6"},"type":"lvl1","url":"/zn-1-6-esempio-calcolo-sd","position":0},{"hierarchy":{"lvl1":"Esempio calcolo SD su fantoccio e immagine reale - Notebook 1.6"},"content":"Import delle librerie necessarie per la simulazione.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.ndimage as ndi\nimport pydicom\n\n\n\n","type":"content","url":"/zn-1-6-esempio-calcolo-sd","position":1},{"hierarchy":{"lvl1":"Esempio calcolo SD su fantoccio e immagine reale - Notebook 1.6","lvl2":"Stima SD su immagine simulata."},"type":"lvl2","url":"/zn-1-6-esempio-calcolo-sd#stima-sd-su-immagine-simulata","position":2},{"hierarchy":{"lvl1":"Esempio calcolo SD su fantoccio e immagine reale - Notebook 1.6","lvl2":"Stima SD su immagine simulata."},"content":"\n\n# create ideal image with 6 patterns\ndim = 512\nimage = np.zeros([dim,dim])\nimage[:,:] = 50\nimage[49:100, 49:100] = 120\nimage[100:180, 100:449] = 200\nimage[199:499, 199:349] = 90\nimage[229:269, 229:269] = 250\nimage[4:399, 449:499] = 150\n\n# add gaussian noise\nsigma = 5\nimageN = image + sigma*np.random.randn(dim,dim)\n\n# histogram\ncounts, bins = np.histogram(imageN, bins = 256)\n\nfig, ax = plt.subplots(1,2)\nax[0].imshow(imageN, cmap = 'gray')\nax[1].hist(bins[:-1], bins, weights=counts)\n\n# get map of standard deviation\nimageF = ndi.generic_filter(imageN, np.std, size = 5)\ncountsF, binsF = np.histogram(imageF, bins = 256)\n\nfig, ax = plt.subplots(1,2)\nax[0].imshow(imageF, cmap = 'gray')\nax[1].hist(binsF[:-1], binsF, weights=countsF)\n\n# print estimated values for SD\nsigmaMean = np.mean(imageF, None)\nsigmaMedian = np.median(imageF, None)\nid = np.argmax(countsF)\nsigmaMax = binsF[id]\nplt.show()\n\nprint(\"Assigned sigma: \",sigma)\nprint(\"Sigma Mean: \",sigmaMean)\nprint(\"Sigma Median: \",sigmaMedian)\nprint(\"Sigma Hist: \",sigmaMax)\n\n\n\n\n\n\n\n\n","type":"content","url":"/zn-1-6-esempio-calcolo-sd#stima-sd-su-immagine-simulata","position":3},{"hierarchy":{"lvl1":"Esempio calcolo SD su fantoccio e immagine reale - Notebook 1.6","lvl2":"Stima SD su immagine DICOM."},"type":"lvl2","url":"/zn-1-6-esempio-calcolo-sd#stima-sd-su-immagine-dicom","position":4},{"hierarchy":{"lvl1":"Esempio calcolo SD su fantoccio e immagine reale - Notebook 1.6","lvl2":"Stima SD su immagine DICOM."},"content":"\n\nds = pydicom.dcmread('../data/phantom.dcm')\nprint(\"Patient ID\", ds.PatientID)\n\nI = ds.pixel_array\n\nnz = np.where(I != 0)[0]\n\n# ROI based analysis\ncenterO = (120,80)\ncenterW = (120,120)\ncenterB = (50,50)\nradius = 8\n\nfig, ax = plt.subplots()\nax.imshow(I, cmap = 'gray')\ncircleO = plt.Circle(centerO, radius, color = 'r', fill = False)\nax.add_patch(circleO)\ncircleO = plt.Circle(centerW, radius, color = 'g', fill = False)\nax.add_patch(circleO)\ncircleO = plt.Circle(centerB, radius, color = 'b', fill = False)\nax.add_patch(circleO)\n\nx, y = np.meshgrid(np.arange(ds.Columns), np.arange(ds.Rows))\nmaskO = (x - centerO[0])**2 + (y - centerO[1])**2 <= radius**2\nmaskW = (x - centerW[0])**2 + (y - centerW[1])**2 <= radius**2\nmaskB = (x - centerB[0])**2 + (y - centerB[1])**2 <= radius**2\n\nid = np.where(maskO != 0)\nstdO = np.std(I[id], None)\n\nid = np.where(maskW != 0)\nstdW = np.std(I[id], None)\n\nid = np.where(maskB != 0)\nstdB = np.std(I[id], None)\n\nplt.show()\nprint(\"Stima basata su ROI:\")\nprint(\"Sigma Oil: \",stdO)\nprint(\"Sigma Water: \",stdW)\nprint(\"Sigma Background: \",stdB)\nprint(\"Sigma Background corrected: \",stdB*1.526)\n\n# histogram based analysis\nsdmap = ndi.generic_filter(I, np.std, size = 3)\ncounts, bins = np.histogram(sdmap[nz], bins = 128)\n\nfig, ax = plt.subplots(1,2)\nax[0].imshow(sdmap, cmap = 'gray')\nax[1].hist(bins[:-1], bins, weights=counts)\n\nsigmaMean = np.mean(sdmap[nz], None)\nsigmaMedian = np.median(sdmap[nz], None)\nid = np.argmax(counts)\nsigmaMax = bins[id]\nplt.show()\nprint(\"stima basata sulla mappa di SD:\")\nprint(\"Sigma Mean: \",sigmaMean)\nprint(\"Sigma Median: \",sigmaMedian)\nprint(\"Sigma Hist: \",sigmaMax)\n\n\n# use only high signal part of the data\nnHigh = np.where(I>100)[0]\nsdmap = ndi.generic_filter(I, np.std, size = 9)\ncounts, bins = np.histogram(sdmap[nHigh], bins = 128)\n\nsigmaMean = np.mean(sdmap[nHigh], None)\nsigmaMedian = np.median(sdmap[nHigh], None)\nid = np.argmax(counts)\nsigmaMax = bins[id]\nplt.show()\n\nprint(\"stima basata sulla mappa di SD utilizzando solo le parti ad alto valore di SD:\")\nprint(\"Sigma Mean: \",sigmaMean)\nprint(\"Sigma Median: \",sigmaMedian)\nprint(\"Sigma Hist: \",sigmaMax)\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/zn-1-6-esempio-calcolo-sd#stima-sd-su-immagine-dicom","position":5},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione"},"type":"lvl1","url":"/c21-interpolazione-filtraggio-e-compressione-compl","position":0},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione"},"content":"Come visto in precedenza, il primo passo dopo l’acquisizione dell’immagine biomedica è l’esecuzione di operazioni a basso livello che hanno lo scopo di migliorare l’immagine. Queste operazioni corrispondono al secondo livello nella classificazione degli algoritmi di computer vision.","type":"content","url":"/c21-interpolazione-filtraggio-e-compressione-compl","position":1},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl2":"Interpolazione"},"type":"lvl2","url":"/c21-interpolazione-filtraggio-e-compressione-compl#interpolazione","position":2},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl2":"Interpolazione"},"content":"Una immagine biomedica ha dimensioni definite a livello di acquisizione o del processo di ricostruzione implementato nello scanner. Le dimensioni sono tipicamente legate alla risoluzione spaziale, nel senso che a parità di campo di vista (FOV), modalità con una migliore risoluzione spaziale saranno in grado di acquisire pixel di dimensioni minori e quindi la dimensione dell’immagine sarà maggiore. Le dimensioni tipiche di una immagine radiologica possono andare da 64x64 (immagini SPECT) a 256x256 (MR cardiaca) fino a 512x512 (CT e MRI) o 1024x1024 (CT). La radiografia classica ha risoluzioni più alte, fino a 5000x5000 per la mammografia.Nell’elaborazione delle immagini biomediche, può essere necessario cambiare la dimensione delle immagini, aumentando o diminuendo il numero di pixel componenti (Figura 2.1).\n\nUn tipico esempio è la visualizzazione dell’immagine su di uno schermo per la visualizzazione e refertazione. Tipicamente la dimensione di una immagine radiologica è minore (o comunque diversa) della dimensione del supporto digitale (stampa o schermo) su cui deve essere visualizzata. Un monitor tipico per refertazione radiologica ha risoluzione di 1600x1200 pixel, che è comunque una risoluzione usuale anche nei monitor di uso generale. È necessario quindi interpolare l’immagine in modo da permetterne la visualizzazione in maniera corretta adattando le dimensioni delle immagini a quelle dello schermo (o della porzione di schermo) in cui vanno visualizzate.\nUn altro esempio, come sarà visto nel seguito, è il cambio di risoluzione spaziale nelle immagini 3D per ottenere dei voxel cubici e quindi delle immagini isotrope.\nCome già visto in precedenza, l’immagine biomedica può essere vista come una serie di campioni di un processo fisico acquisiti in uno spazio discreto, la cui risoluzione è uguale alla dimensione del pixel. In pratica l’immagine ci fornisce i campioni in NxM locazioni spaziali poste al centro dei pixel dell’immagine (seguendo la convenzione DICOM).\nConsideriamo l’immagine in Figura 2.1 con FOV 340x340 mm e dimensioni 256x256 e il suo ingrandimento a destra. Il pixel size è 340/256=1.32 mm, e questo è anche l’intervallo di campionamento spaziale.\nOgni pixel corrisponde ad un punto della griglia (le crocette in figura). Se vogliamo aumentare il numero di righe e colonne dell’immagine dobbiamo aumentare la risoluzione della griglia. Supponiamo di raddoppiare la risoluzione interpolando l’immagine a 512x512.\n\nFigura 2.1. L’operazione di interpolazione prevede di stimare il valore dell’intensità del pixel su una nuova grigila.\n\nIn base alla nuova griglia dovremo definire dei nuovi pixel nella posizione corrispondente ai nuovi punti della griglia. A seconda della procedura usata per definire dei nuovi valori avremo vari metodi di interpolazione. Tipicamente il nuovo valore dei pixel verrà dedotto da quello dei pixel vicini, nell’esempio di  Figura 2.1 dobbiamo ricampionare l’immagine sulla griglia rossa utilizzando i valori dei pixel originali (in bianco). Tipicamente per ottimizzare il tempo di elaborazione si utilizzano solo i pixel più “vicini” al pixel da ricomputare.","type":"content","url":"/c21-interpolazione-filtraggio-e-compressione-compl#interpolazione","position":3},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl3":"Nearest neighbor","lvl2":"Interpolazione"},"type":"lvl3","url":"/c21-interpolazione-filtraggio-e-compressione-compl#nearest-neighbor","position":4},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl3":"Nearest neighbor","lvl2":"Interpolazione"},"content":"Un primo approccio (NN, nearest neighbor)  assegna al nuovo pixel il valore del pixel più vicino (Figura 2.2). Il metodo è molto veloce e ha il vantaggio di non introdurre nuovi livelli di grigio, cosa che può essere rilevante da un punto di vista diagnostico. D’altra parte, il metodo NN tende a creare dei gruppi di pixel con lo stesso valore, dando l’impressione visiva di una immagine “pixellata”.\n\nFigura 2.2. Interpolazione NN.","type":"content","url":"/c21-interpolazione-filtraggio-e-compressione-compl#nearest-neighbor","position":5},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl3":"Interpolazione bilineare","lvl2":"Interpolazione"},"type":"lvl3","url":"/c21-interpolazione-filtraggio-e-compressione-compl#interpolazione-bilineare","position":6},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl3":"Interpolazione bilineare","lvl2":"Interpolazione"},"content":"Un metodo più perfezionato è la cosiddetta interpolazione bilineare (Figura 2.3), nella quale il nuovo valore del pixel viene stimato come la media pesata dei valori dei 4 (o 8 in 3D) pixel più vicini. Il valore del pixel nel punto P è la somma dei valori dei pixel in Q_{ij} pesati per l’area normalizzata del rettangolo opposto a Q_{ij}.\n\nFigura 2.3. Interpolazione bilineare.\n\nFormalmente si ha:\\begin{aligned}\nf(x,y) \\approx {} &\n\\frac{f(Q_{11})}{(x_2 - x_1)(y_2 - y_1)} (x_2 - x)(y_2 - y) \\\\\n&+ \\frac{f(Q_{21})}{(x_2 - x_1)(y_2 - y_1)} (x - x_1)(y_2 - y) \\\\\n&+ \\frac{f(Q_{12})}{(x_2 - x_1)(y_2 - y_1)} (x_2 - x)(y - y_1) \\\\\n&+ \\frac{f(Q_{22})}{(x_2 - x_1)(y_2 - y_1)} (x - x_1)(y - y_1).\n\\end{aligned}\n\nL’interpolazione bilineare offre solitamente una qualità visiva migliore dell’interpolazione NN.\nL’interpolazione bilineare può essere vista come una stima del valore atteso del segnale in P modellando la transizione del segnale tra pixel come una retta ed utilizzando solo i due campioni più vicini per stimare i parametri della retta stessa.","type":"content","url":"/c21-interpolazione-filtraggio-e-compressione-compl#interpolazione-bilineare","position":7},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl3":"Interpolazione bicubica e spline","lvl2":"Interpolazione"},"type":"lvl3","url":"/c21-interpolazione-filtraggio-e-compressione-compl#interpolazione-bicubica-e-spline","position":8},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl3":"Interpolazione bicubica e spline","lvl2":"Interpolazione"},"content":"L’idea può essere estesa utilizzando modelli non lineari (ad esempio funzioni polinomiali di grado crescente o funzioni sinusoidali) ed aumentando il numero di campioni utile a stimare dette funzioni. Un esempio è l’interpolazione bicubica (bicubic interpolation) che utilizza polinomi di terzo grado e una griglia di 16 pixel.Esistono poi metodi ancora più perfezionati (spline) in cui l’intera distribuzione dei pixel sull’immagine viene modellata come una superficie ed i valori interpolanti ottenuti di conseguenza.\nNella visualizzazione di immagini mediche, oltre alla qualità visiva percepita dell’interpolazione è importante anche il tempo di elaborazione, in quanto il sistema software deve rispondere in tempo reale ai comandi dell’utente, ad esempio quando viene effettuato uno zoom.","type":"content","url":"/c21-interpolazione-filtraggio-e-compressione-compl#interpolazione-bicubica-e-spline","position":9},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl3":"Reslicing","lvl2":"Interpolazione"},"type":"lvl3","url":"/c21-interpolazione-filtraggio-e-compressione-compl#reslicing","position":10},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl3":"Reslicing","lvl2":"Interpolazione"},"content":"Il processo di interpolazione è di particolare rilevanza nell’elaborazione di immagini 3D, nelle quali tipicamente la distanza inter-fetta (asse z) e maggiore della risoluzione del pixel sulla singola fetta (piano x-y). In questo caso per ottenere un volume isotropo (cioè con risoluzione spaziale uguale su tutti gli assi) è necessario eseguire una operazione di interpolazione 3D che crei delle nuove slice con distanza inter-fetta uguale alla risoluzione del pixel. Tale operazione è detta reslicing (Figura 2.4). L’operazione di interpolazione deve mantenere costante il campo di vista (FOV), cioè la regione di spazio coperta dal volume.\n\nFigura 2.4. Reslicing.\n\nSupponiamo di avere un volume definito da Nz fette con un FOV = [dimX, dimY, dimZ], dove dimX = dx Nx   (pixel spacing x colonne) dimY = dy Ny\t (pixel spacing x righe) dimZ = dz Nz\t (slice spacing x numero slice)\n\nse dz > dx=dy come avviene di solito, dobbiamo creare delle nuove slice con il reslicing in modo da avere la distanza inter-slice dz1 del nuovo volume uguale a dx/dy senza cambiare il FOV. Quindi:\n\ndimZ = dz Nz = dz1 Nz1 = dx Nz1\n\ne quindi\n\nNz1 = dz Nz/dx\n\nIl reslicing viene anche utilizzato per ricostruire piani di vista diversi da quelli acquisiti, ad esempio per ottenere un piano sagittale, coronale o obliquo da una acquisizione assiale come viene fatto usualmente nella TAC.\nSi noti che gli algoritmi standard di interpolazione presenti nei software generici dedicati alla elaborazione di immagini (come Matlab/python) presuppongono che i pixel dell’immagine siano quadrati e i voxel cubici. Nel caso di immagini biomediche questa assunzione può essere errata, specialmente nel caso di immagini 3D. Il processo di interpolazione deve quindi tener conto della posizione reale dei pixel o voxel nello spazio, cosa che si può fare accedendo ai campi DICOM opportuni.\n\nIn MATLAB è possibile utilizzare la funzione interp3 in combinazione con la funzione meshgrid. La funzione interp3 permette anche di scegliere il metodo di interpolazione da utilizzare attraverso opportune opzioni.\nNel caso di immagini isotrope e fattore di interpolazione uguale per tutti gli assi è possibile utilizzare le funzioni imresize e imresize3.\n\nIn Python è possibile utilizzare la funzione interpn della libreria SciPy (Notebook 2.1).\nLa funzione interpn permette anche di scegliere il metodo di interpolazione da utilizzare attraverso opportune opzioni.\nNel caso di immagini isotrope e fattore di interpolazione uguale per tutti gli assi è possibile utilizzare la funzione RegularGridInterpolator.\n\nCome detto in precedenza un’applicazione tipica degli algoritmi di interpolazione è l’estrazione da volumi di dati 3D di una immagine con orientamento diverso da quello di acquisizione (reslicing). Nella visualizzazione triplanar (proiezione ortogonale) dal volume dei dati vengono estratti tre piani tra loro perpendicolari attraverso una operazione di interpolazione. L’origine dei tre piani può essere definita in modo interattivo dall’operatore, che così può esplorare il volume dei dati. La visualizzazione triplanar è tipicamente disponibile su tutte le workstation per l’analisi di bioimmagini. Un esempio è riporato in Figura 2.5.\n\nFigura 2.5. Triplanar view di un fantoccio ottenuta con imagej.\n\nIn Python la visualizzazione triplanare di dati volumetrici (piani assiale, sagittale e coronale)\npuò essere effettuata tramite il visualizzatore multidimensionale napari, che consente\nl’esplorazione interattiva delle slice lungo i tre assi principali.","type":"content","url":"/c21-interpolazione-filtraggio-e-compressione-compl#reslicing","position":11},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl2":"Filtraggio dell’immagine biomedica"},"type":"lvl2","url":"/c21-interpolazione-filtraggio-e-compressione-compl#filtraggio-dellimmagine-biomedica","position":12},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl2":"Filtraggio dell’immagine biomedica"},"content":"Gli algoritmi di filtraggio consentono l’elaborazione dell’immagine biomedica in modo da modificarne le caratteristiche. Non è banale definire in modo univoco cosa si intende per operazione di filtraggio dal punto di vista radiologico. Seguendo l’approccio della computer vision, possiamo definire filtraggio una operazione che cambia il contenuto informativo dell’immagine senza estrarre informazioni topologiche (in questo differente dal processo di segmentazione). Quindi un filtro potrà evidenziare i contorni dell’immagine, mentre un processo di segmentazione convertirà i contorni evidenziati in una entità geometrica, come una curva o una maschera.\nIl filtraggio ha essenzialmente due possibili scopi:\n\nMigliorare la visualizzazione dell’immagine per ottimizzare l’analisi visiva della stessa.\n\nPredisporre l’immagine per ottimizzare una elaborazione successiva, tipicamente una segmentazione.\n\nPer quanto riguarda il secondo punto, l’operazione di filtraggio avviene tipicamente in modo trasparente rispetto all’utente, che osserva solo il risultato dell’operazione successiva che rappresenta il fine del processo di elaborazione. Il primo caso è invece critico rispetto al processo diagnostico, in quanto un errore nell’algoritmo di filtraggio può causare la perdita di informazione utile e fini clinici e quindi indurre un errore diagnostico. L’introduzione di algoritmi di filtraggio in fase diagnostica è quindi effettuato con estrema cautela.\n\nLe operazioni di filtraggio sull’immagine si possono classificare in tre categorie principali:\n\nPuntuali: Un’operazione puntuale opera solo sul singolo pixel dell’immagine, trasformandolo in base ad una funzione g(v) dove v è il valore dell’intensità del pixel. Esempio di operazioni puntuali sono il negativo dell’immagine e in generale le lookup table compresa l’operazione di windowing e l’applicazione della gamma function. In una operazione puntuale il valore del pixel sull’immagine filtrata dipende solo dal valore dello stesso pixel nell’immagine originale.\n\nLocali: una operazione locale elabora un pixel in base al valore del pixel stesso e dei pixel circostanti. Esempio di operazione locale è la convoluzione spaziale che vedremo in dettaglio nel seguito.\n\nGlobali: Le operazioni globali operano sull’immagine nel suo complesso. Esempio di operazioni globali sono le operazioni sull’istogramma, come l’equalizzazione.","type":"content","url":"/c21-interpolazione-filtraggio-e-compressione-compl#filtraggio-dellimmagine-biomedica","position":13},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl3":"Operazioni puntuali","lvl2":"Filtraggio dell’immagine biomedica"},"type":"lvl3","url":"/c21-interpolazione-filtraggio-e-compressione-compl#operazioni-puntuali","position":14},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl3":"Operazioni puntuali","lvl2":"Filtraggio dell’immagine biomedica"},"content":"Le operazioni puntuali sono equivalenti ad una trasformazione dei livelli di grigio. In pratica un livello di grigio v dell’immagine viene trasformato in un nuovo livello u secondo una trasformazione u=t(v). t può essere utilmente codificata in una lookup-table, cioè una tabella con una sola colonna ed un numero di righe uguale alla profondità dell’immagine. Il vantaggio della lookup-table è che il filtraggio può essere eseguito in modo molto efficiente da un punto di vista computazionale. Una volta definita la tabella come un array T di N elementi dove N sono i possibili valori che può assumere v, il valore di u sarà semplicemente u=T(v), e quindi il nuovo valore di u verrà computato senza nessuna operazione algebrica ma attraverso un semplice accesso in memoria. Le color map per la visualizzazione a falsi colori sono un esempio di una lookup-table a tre colonne dove la tripletta (R,G,B) è data da (R,G,B) = LT(:,v) dove v è il livello di grigio.\n\nEsempi di trasformazioni puntuali sono (si consideri una immagine a 256 livelli):\n\nl’inversione dei livelli di grigio u = (255-v)\n\nil Windowing\n\nle operazioni di tipo gamma u=v^{\\gamma}\n\nla binarizzazione. Viene decisa una soglia T e l’immagine viene ridotta ad una immagine bimodale (u = A se v>T, u = B se v <T)\n\nLa Figura 2.6 riassume alcune operazioni di filtraggio puntuale. Come si osserva, una operazione di filtraggio puntuale può essere sempre espressa come una curva (non necessariamente continua) in un piano bidimensionale con due assi di dimensioni pari alla profondità dell’immagine originale (in questo caso 255, 8 bit) e dell’immagine filtrata (in questo caso normalizzata a 1).\n\nFigura 2.6. Esempi di filtri locali puntuali.","type":"content","url":"/c21-interpolazione-filtraggio-e-compressione-compl#operazioni-puntuali","position":15},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl4":"Windowing","lvl3":"Operazioni puntuali","lvl2":"Filtraggio dell’immagine biomedica"},"type":"lvl4","url":"/c21-interpolazione-filtraggio-e-compressione-compl#windowing","position":16},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl4":"Windowing","lvl3":"Operazioni puntuali","lvl2":"Filtraggio dell’immagine biomedica"},"content":"Esaminiamo in dettaglio l’operazione di Windowing che è di cruciale importanza nella visualizzazione delle immagini biomediche. Una immagine biomedica è in generale codificata su 16 bit, cioè ogni pixel dell’immagine può assumere 2^{16}= 65536 valori, corrispondenti ad altrettanti livelli di grigio. Di fatto il numero di livelli che vengono utilizzati è più basso, ad esempio nella risonanza magnetica solitamente qualche migliaio. Ad ogni modo, gli strumenti di visualizzazione utilizzati nella pratica clinica, come schermi video tradizionali o LCD, sono in grado di visualizzare solo 256 livelli di grigio. D’altra parte, risoluzioni superiori sarebbero inutili in quanto l’occhio umano è in grado di distinguere un numero limitato di livelli di grigio contemporaneamente, dell’ordine di qualche decina. Poiché l’occhio è invece capace di separare un numero molto maggiore di colori diversi, la rappresentazione delle immagini in falsi colori, cioè facendo corrispondere ad ogni livello di grigio un determinato colore attraverso una mappa prestabilita può essere utilizzata per caratterizzare immagini mediche. Questa tecnica trova però applicazione solo in medicina nucleare e nella creazione di mappe parametriche.\nTutti gli strumenti di visualizzazione devono quindi affrontare il problema di rappresentare un numero di livelli di grigio superiore a quello che lo strumento di visualizzazione può sostenere. Le immagini visualizzate o stampate sono quindi fortemente caratterizzate dal modo in cui questo problema è risolto, ed essendo la soluzione del problema tutt’altro che univoca, bisogna tener conto del fatto che ogni immagine stampata o visualizzata su schermo può rappresentare solo parzialmente l’informazione globale contenuta nei dati effettivamente acquisiti dallo scanner.\n\nIl processo mediante il quale i livelli di grigio dell’immagine reale vengono rappresentate sullo schermo o stampate viene solitamente detto windowing. I pixel che giacciono in un certo intervallo di valori (una finestra o window), che può essere impostato dall’utente, vengono rappresentati utilizzando tutti i livelli disponibili (tipicamente 256). I valori al di sotto della finestra selezionata vengono tutti posti a zero, e verranno quindi rappresentati in nero. A tutti i valori superiori verrà invece assegnato il valore massimo visualizzabile (tipicamente 255) e quindi essi appariranno bianchi. La Figura 2.7 mostra un esempio delle differenze di visualizzazione ottenibili attraverso diverse scelte della finestra da utilizzare. Nell’immagine superiore i livelli di grigio vengono mappati in modo proporzionale. Quindi i livelli visualizzati (da 0 a 255, asse y) vengono ottenuti normalizzando i valori originali rispetto al valor massimo dell’immagine originale. Nell’immagine in basso è stata effettuata una opportuna operazione di windowing per visualizzare al meglio la zona di interesse. Si può notare come alcune zone abbiano perso notevolmente di risoluzione a causa dello schiacciamento dei livelli di grigio relativi.\n\nFigura 2.7. Effetto dell’operazione di windowing sulla visualizzazione di una immagine.\n\nL’operazione di Windowing è implementata in tutte le stazioni radiografiche e viene tipicamente pilotata dal movimento del mouse. Ad esempio il movimento orizzontale del mouse può pilotare la larghezza della finestra e il movimento verticale la posizione del centro della finestra sull’asse x. Comunque il metodo con cui viene implementato il filtraggio è in realtà molto raffinato e mira a permettere una regolazione del windowing veloce ma allo stesso tempo precisa. La transizione dal livello 0 al livello massimo della finestra non è necessariamente lineare, ma può assumere varie forme. Il formato DICOM contiene alcuni parametri che pilotano l’operazione di windowing come riportato in Tabella 2.1.\n\nTabella 2.1. Alcuni parametri DICOM che pilotano l’operazione di windowing.\n\nGroup Element\n\nTitle\n\nEsempio\n\n[0028-0106]\n\nSmallest Image Pixel Value\n\n0\n\n[0028-0107]\n\nLargest Image Pixel Value\n\n4177\n\n[0028-0150]\n\nWindow Centre\n\n163\n\n[0028-0151]\n\nWindow Width\n\n327\n\nI primi due parametri, “Smallest Image Pixel Value” e “Largest Image Pixel Value” permettono di leggere il massimo e minimo valore dell’immagine e di creare una funzione di windowing lineare, evitando di comprendere nell’operazione di windowing valori al di fuori del campo di definizione dell’immagine, anche se compresi nella massima profondità teorica. I parametri successivi “Window Centre” e “Window Width” definiscono una finestra di Windowing da utilizzare nella visualizzazione dell’immagine. Questa finestra viene tipicamente salvata nel DICOM al momento dell’acquisizione in base ad una ottimizzazione fatta dal produttore. Il programma di visualizzazione leggerà dal DICOM la finestra e produrrà una visualizzazione iniziale dell’immagine che l’utente potrà poi modificare.\nCome illustrato in Figura 2.8, l’operazione di Windowing per quanto semplice in via di principio è estremamente importante nella pratica clinica, e necessita quindi di una accurata implementazione.\nVale la pena di notare che le lastre radiografiche tradizionali o le immagini digitali in formato standard (jpeg, tiff) allegate ad un referto elettronico rappresentano il prodotto di una precisa operazione di Windowing eseguita dal medico refertante, con l’obiettivo di fornire la migliore informazione iconografica al paziente ed al medico inviante. Esse non hanno però valore legale, in quanto l’operazione di Windowing può cancellare alcune componenti delle immagini se eseguita in modo inappropriato. L’unica fonte certa sono quindi le immagini digitali in formato DICOM, che vengono quindi immagazzinate e sempre più spesso consegnate al paziente su supporto digitale (tipicamente un DVD contenente un visualizzatore DICOM).\n\nFigura 2.8 (tratto da R Rangayyan, Biomedical Image Analysis, CRC Press 2004); esempio di Windowing a fini diagnostici su immagini CT.","type":"content","url":"/c21-interpolazione-filtraggio-e-compressione-compl#windowing","position":17},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl3":"Operazioni locali e filtraggio spaziale","lvl2":"Filtraggio dell’immagine biomedica"},"type":"lvl3","url":"/c21-interpolazione-filtraggio-e-compressione-compl#operazioni-locali-e-filtraggio-spaziale","position":18},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl3":"Operazioni locali e filtraggio spaziale","lvl2":"Filtraggio dell’immagine biomedica"},"content":"L’operazione di base nel filtraggio locale delle immagini è l’operazione di convoluzione spaziale detta anche a finestra mobile. Definiamo una matrice KxK, detta anche kernel o nucleo del filtro. Di solito il kernel è quadrato e di dimensioni dispari. Siano w_{ij} gli elementi del kernel e f_{mn} gli elementi dell’immagine di dimensioni MxN.  Allora il punto g_{mn} dell’immagine filtrata sarà dato da:g_{mn} =\n\\sum_{i=-\\frac{k-1}{2}}^{\\frac{k+1}{2}}\n\\sum_{j=-\\frac{k-1}{2}}^{\\frac{k+1}{2}}\nf_{m+i,\\,n+j}\\, w_{i,j}\n\nIn pratica l’operazione di convoluzione spaziale consiste nel far scorrere il kernel sull’immagine e sostituire volta per volta il pixel dell’immagine corrispondente al pixel centrale del kernel con un valore che è la somma dei pixel dell’immagine coperti dal kernel pesati per gli elementi del kernel stesso. Solitamente il kernel è normalizzato, e quindi la somma degli elementi del kernel è unitaria in modo da non mutare sostanzialmente la dinamica dell’immagine dopo il filtraggio. Questo punto è importante nell’imaging medico in quanto il valore del segnale può avere un significato diagnostico (come nella TAC). Inoltre essendo le immagini codificate come interi a 16 bit mutare la dinamica può causare problemi di overflow numerico.\nIl risultato di un’operazione di convoluzione spaziale è un’immagine di dimensione pari alla somma dell’immagine più la dimensione del kernel. Solitamente si considera solo la parte centrale dell’immagine risultante per ottenere una immagine delle stesse dimensioni dell’immagine originale.\n\nL’operazione di filtraggio spaziale è implementata in MATLAB tramite la funzione conv2 e, nel caso tridimensionale, tramite la funzione conv. L’opzione mode='same' consente di mantenere la dimensione dell’immagine filtrata. In Python, le operazioni equivalenti sono fornite dalla libreria SciPy mediante le funzioni\nscipy.signal.convolve2d e scipy.signal.convolve.\nIn alternativa, è possibile utilizzare la funzione scipy.ndimage.convolve, che presenta\ncaratteristiche analoghe a imfilter di MATLAB.\nI principali filtri possono essere definiti in modo automatico tramite funzioni dedicate,\ncome scipy.ndimage.gaussian_filter, oppure mediante kernel definiti manualmente.\n\nCambiando il kernel, si possono ottenere i vari tipi di filtraggio. Il primo esempio è il filtro di smoothing, che è in grado di ridurre il rumore. Il filtro di smoothing KxK è definito come:W_{ij}=\\frac{1}{K^{2}} \\quad \\forall \\quad i,j\n\nUn esempio 3x3 è\\frac{1}{9}*\\begin{bmatrix}\n1 & 1 & 1 \\\\\n1 & 1 & 1 \\\\\n1 & 1 & 1 \n\\end{bmatrix}\n\nNotiamo che per semplicità si preferisce esprimere il filtro con valori interi, spesso sottintendendo l’operazione di normalizzazione che comunque viene sempre eseguita.\nIl filtro effettua una operazione di media mobile, sostituendo ad un pixel dell’immagine il valore della media dei pixel in un intorno. Il filtro riduce efficacemente il rumore nelle regioni omogenee, mentre introduce un “ammorbidimento” dei contorni nelle regioni di confine tra i pattern, cosa non desiderabile nelle immagini biomediche. I due effetti sono tanto più rilevanti quanto più è grande il kernel.\nUna variante del filtro a media mobile è il filtro pillbox di Figura 2.9 (opzione disk della funzione\nfspecial in MATLAB), che realizza una media mobile limitata a una regione circolare.\nIn Python, un filtro equivalente può essere ottenuto tramite un kernel circolare normalizzato,\noppure utilizzando la funzione scipy.ndimage.uniform_filter in combinazione con una maschera\ncircolare.\n\nFigura 2.9. Filtro “pillbox”.\n\nUn altro esempio di filtri dedicati alla riduzione del rumore sono i filtri di tipo gaussiano, dove il kernel è definito come una gaussiana bidimensionale. Un esempio di filtro gaussiano 3x3 il seguente:\\frac{1}{16}*\\begin{bmatrix}\n1 & 2 & 1 \\\\\n2 & 4 & 2 \\\\\n1 & 2 & 1 \n\\end{bmatrix}\n\nI filtri gaussiani riducono il rumore riducendo l’effetto di sfuocaσmento dell’immagine. Esempi di filtri gaussiani con diversa deviazione standard sono:\\sigma = 0.391, \\quad dim=3 \\times 3 \\\\\n\\begin{bmatrix}\n1 & 4 & 1 \\\\\n4 & 12 & 4 \\\\\n1 & 4 & 1 \n\\end{bmatrix}\\sigma = 0.625, \\quad dim=5 \\times 5 \\\\\n\\begin{bmatrix}\n 1 & 2 & 3 & 2 & 1 \\\\\n 2 & 7 & 11 & 7 & 2 \\\\\n 3 & 11 & 17 & 11 & 3 \\\\\n 2 & 7 & 11 & 7 & 2 \\\\\n 1 & 2 & 3  & 2 & 1 \\\\\n\\end{bmatrix}\\sigma = 1, \\quad dim = 9 \\times 9 \\\\\n\\begin{bmatrix}\n 0 &0 &1 & 1 & 1 & 1 &1 &0 &0 \\\\\n 0 &1 &2 & 3 & 3 & 3 &2 &1 &0 \\\\\n 1 &2 &3 & 6 & 7 & 6 &3 &2 &1 \\\\\n 1 &3 &6 & 9 &11 & 9 &6 &3 &1 \\\\\n 1 &3 &7 &11 &12 &11 &7 &3 &1 \\\\\n 1 &3 &6 & 9 &11 & 9 &6 &3 &1 \\\\\n 1 &2 &3 & 6 & 7 & 6 &3 &2 &1 \\\\\n 0 &1 &2 & 3 & 3 & 3 &2 &1 &0 \\\\\n 0 &0 &1 & 1 & 1 & 1 &1 &0 &0\n \\end{bmatrix}\n\nNel filtro gaussiano, la somma del valore del peso centrale è maggiore degli altri e i pesi sono proporzionali alla distanza dal centro secondo l’equazione della gaussiana.\nUn filtro gaussiano è definito quindi da due fattori. Il primo è la grandezza del kernel, il secondo è il valore della deviazione standard \\sigma che definisce il valore degli elementi del kernel stesso. Si noti che per \\sigma molto più grande delle dimensioni del kernel il filtro gaussiano collassa in un filtro a media mobile, per \\sigma molto piccolo si ottiene un kernel diverso da zero solo nel punto centrale che lascia l’immagine inalterata.\n\nCome visto in precedenza, l’effetto dei filtri di smoothing (media mobile o gaussiano) è simile al partial volume effect (PVE) che avviene a livello di acquisizione. Tali filtri possono quindi essere utilizzati per simulare il PVE nel modello dell’immagine biomedica.\n\nI filtri a convoluzione spaziale possono essere applicati efficacemente anche per evidenziare i contorni in una immagine. Il gradiente di un’immagine f(x,y) nel punto (x,y) è il vettore:\\nabla f = \n\\begin{bmatrix}\nG_{x} \\\\\nG_{y} \n\\end{bmatrix} =\n\\begin{bmatrix}\n\\frac{\\partial f}{\\partial x} \\\\\n\\frac{\\partial f}{\\partial y} \n\\end{bmatrix}\n\nE’ noto che il vettore gradiente punta nella direzione di massima velocità di variazione di f nei punti (x,y). Pertanto nel problema della rivelazione dei bordi è importante l’ampiezza di questo vettore data da:\\nabla f =\n\\begin{bmatrix}\nG_{x}^{2} & G_{y}^{2} \n\\end{bmatrix}^{1/2}\n\nAnche la direzione del gradiente \\alpha(x,y) è una quantità importante:\\alpha(x,y) = tan^{-1} \n\\begin{bmatrix}\n\\frac{G_{x}}{G_{y}} \n\\end{bmatrix}\n\ndove l’angolo è misurato rispetto all’asse x. Notare che il calcolo del gradiente di un’immagine è basato sul calcolo delle derivate \\frac{\\partial f}{\\partial x} e \\frac{\\partial f}{\\partial y} per ogni pixel dell’immagine. Quindi il calcolo del gradiente di un’immagine deve essere fatto in due passi utilizzando due kernel: uno per la direzione x ed uno per la direzione y (immagini 2D) o tre passi (immagini 3D). E’ possibile anche definire un gradiente temporale in caso di immagini 2D+T o 3D+T.\nIn calcolo del gradiente implica il calcolo del valore della derivata da campioni discreti vista la natura discreta delle immagini. La derivata può essere approssimata in diversi modi. Per una maschera 3x3 il modo più semplice è il gradiente di Sobel:\\frac{\\partial f}{\\partial x} = \n\\begin{bmatrix}\n-1 & 0 & 1 \\\\\n-2 & 0 & 2 \\\\\n-1 & 0 & 1 \n\\end{bmatrix}\\frac{\\partial f}{\\partial y} = \n\\begin{bmatrix}\n1 & 2 & 1 \\\\\n0 & 0 & 0 \\\\\n-1 & -2 & -1 \n\\end{bmatrix}\n\nL’operazione di derivata basata sull’operatore di Sobel è data da:G_y = (w_{31} + 2w_{32} + w_{33}) - (w_{11} + 2w_{12} + w_{13})G_x = (w_{13} + 2w_{23} + w_{33}) - (w_{11} + 2w_{12} + w_{13})\n\nNotiamo che la formulazione precedente dei kernel è valida se \\Delta x=\\Delta y, cioè se la risoluzione spaziale sull’asse x è uguale a quella sull’asse y (pixel quadrato). Questa ipotesi è di solito verificata per immagini 2D, mentre se espandiamo il filtro in 3D non è in generale vera e si deve esplicitare nel kernel il valore della dimensione del voxel o interpolare il volume prima del filtraggio.\n\nUn’altra implementazione del filtro derivativo è il filtro di Prewitt.\\frac{\\partial f}{\\partial x} = \n\\begin{bmatrix}\n-1 & 0 & 1 \\\\\n-1 & 0 & 1 \\\\\n-1 & 0 & 1 \n\\end{bmatrix}\\frac{\\partial f}{\\partial y} = \n\\begin{bmatrix}\n1 & 1 & 1 \\\\\n0 & 0 & 0 \\\\\n-1 & -1 & -1 \n\\end{bmatrix}\n\nAnalogamente si può definire il Laplaciano che implementa la derivata seconda dell’immagine:\\nabla^2 f(x,y) = \\frac{\\partial^2 f}{\\partial x^2} +\n\\frac{\\partial^2 f}{\\partial y^2}\n\nche corrisponde al kernel discreto:\\nabla^2 =\n\\begin{bmatrix}\n0 & -1 & 0 \\\\\n-1 & 4 & -1 \\\\\n0 & -1 & 0\n\\end{bmatrix}\n\nL’immagine di gradiente ha la caratteristica di avere un valore elevato sui contorni dell’immagine e valore nullo nelle regioni ad intensità costante. Per eliminare il problema di pixel negativi che possono essere introdotti dal filtro possiamo normalizzare l’immagine o sommare un valore costante. Uno dei possibili usi dell’immagine di gradiente/laplaciano è di fungere da guida nel filtraggio con un filtro di smoothing, in pratica il filtraggio viene effettuato solo nelle regioni nelle quali il valore dell’immagine di gradiente è basso come vedremo in seguito.\nSe sommiamo all’immagine il laplaciano abbiamo un filtro che produce una maggiore definizione dei contorni (sharpening operator):\\nabla =\n\\begin{bmatrix}\n0 & -1 & 0 \\\\\n-1 & 5 & -1 \\\\\n0 & -1 & 0\n\\end{bmatrix}\n\nL’efficacia di questo filtro è dovuta alla capacità del sistema occhio-cervello di concentrarsi sui bordi eliminando le zone a intensità costante.\nFiltri laplaciani più complessi sono i filtri “a sombrero”.\n\nFigura 2.10. Esempi di filtri a sombrero.\n\nIn MATLAB i filtri derivativi possono essere ottenuti tramite la funzione fspecial.\nIl gradiente dell’immagine può inoltre essere calcolato direttamente mediante la funzione\nimgradient, che consente di utilizzare diversi metodi per il calcolo delle derivate. In Python non esiste una funzione unica equivalente a imgradient; tuttavia, funzionalità\nanaloghe sono fornite dalle librerie SciPy e scikit-image. In particolare, gli operatori\ndi gradiente più comuni sono disponibili in scipy.ndimage e skimage.filters, tra cui:\n\nSobel (scipy.ndimage.sobel)\n\nPrewitt (scipy.ndimage.prewitt)\n\nRoberts (skimage.filters.roberts)\n\nDifferenze centrali, calcolabili tramite derivate discrete o convoluzioni\n\nDifferenze in avanti, ottenibili mediante operatori di differenza finita\n\nIl modulo e la direzione del gradiente possono essere calcolati a partire dalle componenti\nlungo le direzioni orizzontali e verticali, rispettivamente come|\\nabla f| = \\sqrt{G_x^2 + G_y^2}, \\qquad\n\\theta = \\arctan\\left(\\frac{G_y}{G_x}\\right).","type":"content","url":"/c21-interpolazione-filtraggio-e-compressione-compl#operazioni-locali-e-filtraggio-spaziale","position":19},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl4":"Filtraggio adattivo","lvl3":"Operazioni locali e filtraggio spaziale","lvl2":"Filtraggio dell’immagine biomedica"},"type":"lvl4","url":"/c21-interpolazione-filtraggio-e-compressione-compl#filtraggio-adattivo","position":20},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl4":"Filtraggio adattivo","lvl3":"Operazioni locali e filtraggio spaziale","lvl2":"Filtraggio dell’immagine biomedica"},"content":"I filtri visti finora agiscono nello stesso modo su tutta l’immagine. Una classe di filtri più complessa è quella dei filtri anisotropici, che funzionano in modo diverso in regioni diverse dell’immagine. Questi filtri vengono detti anche adattivi in quanto adattano il loro comportamento sulla base del contenuto dell’immagine. Tipicamente un filtro anisotropico utilizza una informazione di gradiente per localizzare i bordi dell’immagine che devono essere preservati nell’operazione di filtraggio. Il filtraggio viene modulato in base al valore locale del gradiente.Un esempio semplice di un filtro adattivo è un filtro di smoothing che effettua lo smoothing solo dove il gradiente locale dell’immagine è inferiore ad un certo valore di soglia, mentre dove il valore è superiore lascia l’immagine inalterata.\nNella pratica per realizzare un semplice filtro adattivo si computa il gradiente dell’immagine I_{OR} attraverso un filtraggio locale come descritto in precedenza. Si ottiene così una immagine G di dimensioni uguale all’originale che contiene il gradiente dell’immagine stessa. Stabilita una soglia T_{g} si applica a G un filtro binario puntuale con soglia T_{g}, ottenendo una immagine mask_{HG}, che vale uno se il gradiente è maggiore di T_{g} e zero altrove. Una immagine binaria di questo tipo è spesso detta maschera (mask). Applicando un filtro puntuale di inversione a mask_{HG} si ottiene la sua inversa mask_{LG}, che rappresenta la maschera delle regioni dell’immagine con gradiente inferiore o uguale a T_{g}.\nA questo punto si applica un filtraggio locale a media mobile (smoothing) o gaussiano all’immagine I_{OR} ottenendo l’immagine filtrata I_{MM}, dove lo smoothing è applicato su tutta l’immagine. L’immagine filtrata in modo adattivo I_{FA} si ottiene dalla formula:I_{FA} = I_{OR} * mask_{HG} + I_{MM} * mask_{LG}\n\nInfatti essendo le due maschere mutuamente esclusive (la loro somma da uno su tutti i pixel) l’immagine risultante è uguale all’immagine originale nelle zone ad alto gradiente ed alla immagine filtrata nelle zone a basso gradiente.Le prestazioni di un filtro adattivo così realizzato dipendono ovviamente dal valore di T_{g}. Se T_{g} è troppo alto alcuni contorni verranno sfumati, se T_{g} è troppo basso regioni uniformi dell’immagine non verranno filtrate. Per individuare il valore ottimo di T_{g} è utile esprimere tale soglia in funzione delle statistiche del rumore associato all’immagine. Facciamo riferimento ad un modello semplificato di immagine biomedica dove l’immagine reale è data dalla somma dell’immagine reale e di rumore gaussiano con media nulla e deviazione standard \\sigma.I(x,y)=I_{0}(x,y)+n(x,y)\n\nSu un pattern omogeneo dell’immagine (dove vogliamo che venga applicato il filtro) il segnale sarà distribuito il modo gaussiano con media Sp (valore del segnale sul pattern) e deviazione standard \\sigma. Il valore del gradiente sul pattern intuitivamente dipenderà da \\sigma e dal tipo di filtro utilizzato per calcolare le due componenti del gradiente stesso. Nel caso semplificato in cui si computi la derivata come differenza tra il valore di due pixel adiacenti (kernel [1 0 -1]) il valore di Gx sara Ga(Sp,\\sigma)-Gb(Sp, \\sigma) dove Ga e Gb sono due realizzazioni indipendenti di un processo gaussiano con  media Sp e deviazione standard \\sigma. Sapendo che il 95\\% dei valori generati da un processo gaussiano è compreso nella finestra [-1.96\\sigma,1.96\\sigma] approssimabile a [-2\\sigma,2\\sigma] nel caso peggiore |Gx| potrà assumere il valore 4\\sigma e così per Gy, dando un gradiente complessivo massimo G = |Gx| + |Gy| = 8\\sigma. Si tratta evidentemente di un caso limite molto improbabile, specialmente nel caso che il kernel usato per computare il gradiente sia di dimensioni non ridottissime, ma che giustifica l’idea di imporre:T_{g} = k \\sigma\n\nEliminando quindi la dipendenza di T_{g} dal rumore. Il valore di \\sigma può essere calcolato in modo manuale o automatico utilizzando le tecniche viste nel calcolo del SNR e CNR. Il valore di k consente di regolare il funzionamento del filtro, il valore di k varia tipicamente tra 1 e 4.\nUn filtro adattivo così definito ha lo svantaggio principale di transire in modo brusco dalla zona in cui viene effettuato lo smoothing alla zona in cui l’immagine originale viene preservata, a causa dell’utilizzo di un filtro a gradino. L’altro svantaggio è di richiedere la definizione di k per stabilire la soglia.\n\nUn’alternativa più evoluta è rappresentata dal filtro di Wiener.\nIn MATLAB tale filtro è implementato tramite la funzione wiener2. In Python, un filtro equivalente è disponibile nella libreria SciPy\nmediante la funzione scipy.signal.wiener.\nIl metodo si basa sul calcolo delle statistiche locali dell’immagine:\nl’immagine viene inizialmente filtrata con un filtro a media mobile,\nottenendo I_{MM}, che rappresenta la media locale per ciascun pixel.\nSuccessivamente viene calcolata la mappa della varianza locale\nI_{VAR}, stimata sullo stesso kernel utilizzato per il filtro a media mobile.\n\nL’immagine filtrata risulta quindi:I_W = I_{MM} +\n\\frac{I_{VAR} - \\sigma^2}{I_{VAR}}\n\\left( I_{OR} - I_{MM} \\right)\n\nSe la varianza dell’immagine è uguale a quella del rumore (pattern uniforme) il secondo termine si annulla e l’immagine risultante è uguale a quella filtrata con il filtro a media mobile. Se I_{VAR} è molto grande rispetto al rumore dell’immagine l’immagine filtrata risulta uguale all’immagine originale. Nei due casi estremi il filtro di Wiener si comporta quindi come il filtro adattivo visto prima. Il vantaggio del filtro di Wiener è che gestisce in modo graduale la transizione tra le zone omogenee e le zone ad alto gradiente dove sono presenti i contorni. Inoltre non è richiesto di definire il valore di k ma solo di valutare il rumore sull’immagine \\sigma.\n\nUn possibile approccio per la stima automatica di \\sigma consiste nel far scorrere\nsull’immagine un kernel (ad esempio 5 \\times 5) e nel calcolare il valore della\ndeviazione standard SD sulla regione di immagine coperta dal kernel.\nIn MATLAB tale operazione è implementata tramite la funzione stdfilt. In Python, una funzionalità equivalente può essere ottenuta utilizzando la libreria\nSciPy, mediante la funzione scipy.ndimage.generic_filter, oppure calcolando la\ndeviazione standard locale a partire da media e varianza locali.\n\nL’operazione produce una mappa di valori di SD avente la stessa dimensione\ndell’immagine di partenza. Nelle regioni corrispondenti a pattern uniformi,\nil valore di SD approssima \\sigma nel caso di rumore gaussiano additivo.\nAl contrario, nelle regioni in prossimità dei bordi tra pattern differenti,\nil valore di SD risulta maggiore di \\sigma. Un esempio è riportato in Figura 2.11.\n\nFigura 2.11. Sinistra: immagine originale. Destra: mappa di deviazione standard.\n\nIn Figura 2.11. osserviamo una immagine formata da pattern omogenei con rumore gaussiano a media nulla e \\sigma=10 e la corrispondente mappa della SD. Come si nota dalla figura il valore di SD sulla mappa è massimo i corrispondenza dei bordi ed assume valori bassi e variabili nelle regioni omogenee. Questo è confermato dall’istogramma della mappa SD riportato in Figura 2.12. Che presenta un picco evidente in corrispondenza della SD misurata nelle zone omogenee con il massimo in corrispondenza della σ del rumore gaussiano imposto. Dalla mappa SD è possibile quindi valutare il valore di \\sigma utile per la configurazione del filtro adattivo. L’approccio più semplice trascura la SD delle regioni di transizione e stima σ come la media delle SD sulla mappa (nel caso in figura si ottiene \\sigma=11.4). Volendo eliminare gli outliers dovuti ai bordi è preferibile computare la mediana della mappa SD  (nel caso in Figura 2.12 si ottiene \\sigma=9.2). Un approccio più accurato estrae il primo picco dell’istogramma e ne computa la posizione del massimo.\nSe il rumore non è gaussiano si dovrà tenere conto di ciò nel calcolo di \\sigma. Ad esempio nelle immagini MR ci attendiamo che l’istogramma della SD map presenti due picchi, uno in corrispondenza dello sfondo ed un altro spostato a destra di un fattore 1.526 in corrispondenza alle regioni omogenee ad alto SNR. In questo caso il valore di σ andrà valutato direttamente sul secondo picco o sul primo introducendo l’opportuno fattore correttivo.\n\nFigura 2.12. Istogramma della mappa di deviazione standard.","type":"content","url":"/c21-interpolazione-filtraggio-e-compressione-compl#filtraggio-adattivo","position":21},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl4":"Adaptive Template Filtering","lvl3":"Operazioni locali e filtraggio spaziale","lvl2":"Filtraggio dell’immagine biomedica"},"type":"lvl4","url":"/c21-interpolazione-filtraggio-e-compressione-compl#adaptive-template-filtering","position":22},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl4":"Adaptive Template Filtering","lvl3":"Operazioni locali e filtraggio spaziale","lvl2":"Filtraggio dell’immagine biomedica"},"content":"Per “Adaptive Template Filtering” si indica un filtraggio basato sulla scelta adattiva di un determinato template (cioè un kernel di filtraggio) in base alle caratteristiche dell’immagine sottostante. Una implementazione possibile è quella proposta da C. B. Ahn.  L’idea di base è quella di avere una collezione di possibili template e di scegliere per ogni locazione dell’immagine un particolare template ottimizzato. L’obiettivo dell’algoritmo è quello di migliorare l’SNR evitando allo stesso tempo la perdita di definizione dei contorni. Il numero dei possibili template dipende dalla grandezza del template stesso ed è dato da:NT = \\sum_{k=2}^{N} C_N^{k}\n   = \\sum_{k=2}^{N} \\frac{n!}{k!(n-k)!}\n\nper un template 3 \\times 3 abbiamo chiaramente  N=9 e un numero totale di NT=255 configurazioni possibili. La Figura 2.13 mostra la collezione dei template 3x3. Essendo m il numero di 1 nel template, si ha un template per m=9, 8 template per m=8, 28 template per m=7, etc.\n\nFigura 2.13. Esempio di banco di filtri per filtraggio adattivo.\n\nLa scelta del template ottimo viene effettuata attraverso il calcolo di un opportuno indice. In particolare, viene calcolata la deviazione standard (SD) del valore dei pixel sul template, data da:\\sigma(k,l) =\n\\sqrt{\n\\frac{1}{m-1}\n\\sum_{i,j \\in T}\n\\left[ x(i,j) - \\bar{x}(k,l) \\right]^2\n}\\bar{x}(k,l) =\n\\frac{1}{m}\n\\sum_{i,j \\in T} x(i,j)\n\ndove x(i,j) sono i valori dei pixel nel template, (k,l) sono le coordinate del pixel da computare (e quindi le coordinate del centro del template), T_{j} è il template corrente e N_{j} è la dimensione m del template T_{j} . Per ogni pixel dell’immagine da filtrare, la SD viene calcolata per ogni possibile template. I template vengono divisi in due classi in base alla SD: I template con SD minore di una certa soglia (plane templates) e quelli con SD superiore alla soglia (edge templates).\nSe vengono riconosciuti uno o più plane template, viene scelto quello con dimensione maggiore. Se non ci sono plane template, viene selezionato  l’edge template con SD minima.\nIn realtà per diminuire i tempi di calcolo si procede esaminando i template in ordine di dimensione e fermandosi appena viene trovato un plane template. Nella sostanza l’algoritmo individua per ogni pixel la distribuzione spaziale dei pixel circostanti “simili” al pixel corrente, ed effettua l’operazione di smoothing tenendo conto solo di detti pixel. Il punto fondamentale dell’algoritmo è la scelta della soglia sulla SD. Similmente a quanto visto in precedenza la soglia è definita come:\\tau = a\\sigma_{n}\n\ndove è la SD del rumore sull’immagine e a è un fattore di scala, che assume un valore tra 1.2 e 1.6. La stima di \\sigma_{n} può essere effettuata come visto in precedenza per il filtro adattivo.","type":"content","url":"/c21-interpolazione-filtraggio-e-compressione-compl#adaptive-template-filtering","position":23},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl3":"Operatori globali ed equalizzazione dell’istogramma","lvl2":"Filtraggio dell’immagine biomedica"},"type":"lvl3","url":"/c21-interpolazione-filtraggio-e-compressione-compl#operatori-globali-ed-equalizzazione-dellistogramma","position":24},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl3":"Operatori globali ed equalizzazione dell’istogramma","lvl2":"Filtraggio dell’immagine biomedica"},"content":"Un esempio di filtraggio globale dell’immagine è rappresentato della procedura di equalizzazione dell’istogramma, che ha lo scopo di aumentare il contrasto percepito dell’immagine. Nella procedura di equalizzazione l’istogramma dell’immagine viene modificato in modo da divenire costante. Quindi se consideriamo l’immagine f con istogramma h(f), il filtro produrrà una immagine g con istogramma h(g) tale che h(g) = C per tutti i valori di livello di grigio di g. Si ha chiaramente C = N/p  dove N è il numero di pixel dell’immagine e p la profondità dell’immagine stessa.\nInoltre, deve essere conservato l’ordinamento dei livelli di grigio, per cui se f1< f2 deve risultare g1<g2, quindi la trasformazione deve essere monotona.\nIn pratica il filtro deve trovare “l’inversa” dell’istogramma di f, in modo che la funzione monotona T che caratterizza il filtro sia tale che:T(h(f)) = h^{-1}(f)h(f)  = C\n\nTale funzione è la distribuzione cumulativa (CDF) dell’istogramma da equalizzare, definita come:g_k = \\frac{p-1}{N} \\sum_{j=0}^{k} f_k,\n\\qquad k = 0,1,\\ldots,p-1\n\nConsideriamo il fantoccio TAC in Figura 2.14 ed il relativo istogramma (in blu). Per chiarezza è stata eliminata la prima riga dell’istogramma che risulta molto alta contenendo la zona di zero padding.\n\nFigura 2.14. Sinistra: fantoccio TAC. Destra: Istogramma (blu) e relativa CDF (rosso).\n\nLa CFD relativa all’istogramma è visualizzata in rosso. Come si osserva la CDF è “piatta” nelle regioni dove l’istogramma ha valori bassi mentre cresce ripidamente dove l’istogramma ha valori alti. Il filtro usa la CDF come “hash table” per definire la trasformazione implementata. Quindi ad esempio, i valori di livello di grigio dell’immagine tra 2000 e 3000 verranno sostituiti da valori molto simili dati dal valore della CDF che varia lentamente tra i due picchi a 2000 e 3000, “appiattendo” l’istogramma.\nL’immagine filtrata risulta come in Figura 2.15.\n\nFigura 2.15. Esempio di equalizzazione dell’istogramma.\n\nSi osserva come l’equalizzazione dell’istogramma “allarghi” i picchi e introduca una quantizzazione\ndei livelli di grigio, concentrandoli nelle regioni in cui la CDF risulta pressoché costante.\n\nIn Python l’equalizzazione dell’istogramma può essere implementata tramite la libreria\nscikit-image usando la funzione skimage.exposure.equalize_hist.\nA differenza di histeq di MATLAB, l’operazione può essere applicata anche a immagini a\nprofondità maggiore (ad esempio 16 bit), a patto di gestire correttamente la normalizzazione\ndell’intervallo di intensità. L’esempio è implementato nel Notebook 2.3.","type":"content","url":"/c21-interpolazione-filtraggio-e-compressione-compl#operatori-globali-ed-equalizzazione-dellistogramma","position":25},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl2":"Compressione dell’immagine biomedica"},"type":"lvl2","url":"/c21-interpolazione-filtraggio-e-compressione-compl#compressione-dellimmagine-biomedica","position":26},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl2":"Compressione dell’immagine biomedica"},"content":"Accenniamo infine al problema della compressione delle immagini biomediche. La compressione ha lo scopo di ridurre il numero di bit necessari per l’immagazzinamento dell’immagine biomedica. Il vantaggio è quello di ridurre la dimensione dei supporti informatici necessari all’immagazzinamento a breve e lungo termine e di migliorare la velocità di trasmissione delle immagini stesse nelle reti di comunicazione. Quest’ultima applicazione è quella oggi più rilevante. Lo svantaggio della compressione è che per usufruire dell’immagine compressa è necessario un algoritmo di decompressione che ripristini l’immagine originaria. In altri termini il software con cui si utilizza l’immagine deve implementare le caratteristiche DICOM di compressione, altrimenti l’immagine sarà inutilizzabile. La compressione viene quindi solitamente utilizzata o quando strettamente necessaria (telemedicina) o quando tutte le operazioni sull’immagine sono sotto il controllo di una singola struttura (ad esempio backup di un PACS). L’argomento verrà trattato più in dettaglio nel seguito quando si descriveranno i sistemi PACS.L’efficacia di una operazione di compressione si misura attraverso il rapporto  di  compressione che è  il rapporto tra le dimensioni dell’immagine dopo la compressione e le dimensioni originali. Il rapporto di compressione si può indicare in termini percentuali (25% o 0.25 indica che la dimensione dell’immagine compressa è il 25% di quella originale) o come X:1, dove X indica quante volte l’immagine originale è più grande di quella compressa (ad esempio un rapporto di compressione 4:1 indica che l’immagine compressa è 4 volte più piccola e corrisponde ad un rapporto percentuale del 25%).\nUna distinzione fondamentale è quella tra compressione  senza  perdite (lossless), che è reversibile nel senso che dall’immagine compressa è possibile ricostruire l’immagine originale, e compressione con perdite nella quale una parte dell’informazione associata all’immagine viene persa nel processo di conversione. La compressione con perdite permette rapporti di compressione molto maggiori, di almeno un ordine di grandezza. Un esempio di compressione senza perdite sono gli archivi digitali (zip, rar, etc). L’esempio tipico di compressione con perdite è il formato mp3 per i file audio e MPEG per i file video.In ogni caso perché sia possibile una compressione deve esistere una ridondanza, cioè il numero di bit che codificano l’immagine deve essere maggiore del numero di bit che descrivono il contenuto informativo dell’immagine stessa. Questo accade pressoché sempre nell’imaging biomedico, si pensi ad esempio al segnale di fondo che non apporta nessun contenuto informativo. Inoltre nelle immagini biomediche pixel vicini hanno spesso valori simili e questa caratteristica può utilmente essere usata in fase di compressione. Questo è vero anche in senso temporale nelle immagini dinamiche, dove si possono utilizzare algoritmi come l’MPEG.\nEsistono innumerevoli algoritmi di compressione con perdite e senza perdite. Tra i classici ricordiamo l’RLE (Run Lenght Encoding) che sfrutta le ripetizioni di byte uguali, l’LZW (Lempel Ziv Welch) che impiega le ripetizioni di stringhe di byte uguali, l’algoritmo di Huffman che utilizza codici di rappresentazione più brevi per i pixel che appaiono più frequentemente.\nTra gli algoritmi di compressione con perdite ricordiamo la DCT (Discrete Cosine Transform) e la compressione Wavelets che sfruttano la scomposizione dell’immagine in sub immagini con un diverso grado di dettaglio. Le immagini con grado di dettaglio molto elevato (corrispondenti a variazioni tra pixel molto vicine) vengono considerate rumore ed eliminate.\nNella compressione con perdite si definisce l’errore di compressione come l’errore quadratico medio misurato tra l’immagine originale I e l’immagine dopo la compressione/decompressione D:\\overline{e^2} =\n\\frac{1}{N^2}\n\\sum_{i=1}^{N}\n\\sum_{j=1}^{N}\n\\left( I(i,j) - D(i,j) \\right)^2\n\nsi definisce anche il rapporto segnale/errore di compressione (Figura 2.16) come:\\mathrm{PSNR} =\n10 \\log_{10}\n\\left(\n\\frac{\n\\displaystyle\n\\sum_{i=1}^{N}\n\\sum_{j=1}^{N}\nI^2(i,j)\n}{\n\\overline{e^2}\n}\n\\right)\n\nDi solito esisterà un rapporto inverso tra errore percentuale di compressione (PSNR) e rapporto di compressione (CR).\n\nFigura 2.16. Rapporto segnale/errore di compressione.\n\nNaturalmente nell’imaging biomedico è fondamentale anche il giudizio “visivo” dell’operatore. Il formato DICOM supporta JPEG, JPEG Lossless, JPEG 2000, and Run-length encoding (RLE). Il Lossless JPEG (compressione JPEG senza perdite) viene usato tipicamente in immagini angiografiche che sono di dimensioni elevate e sostanzialmente bimodali. Ottiene rapporti di compressione di qualche unità (fino a 4:1). Il formato JPEG 2000 supporta la compressione con perdite e senza perdite con tecnica DCT o Wavelet. Inoltre permette di scegliere livelli di compressione diversi in diverse regioni dell’immagine. È usato soprattutto in applicazioni di telemedicina.\nNell’ambito dell’imaging biomedico è importante notare come la compressione con perdite comporti la perdita di una parte del contenuto informativo, e va quindi utilizzata con estrema cautela.","type":"content","url":"/c21-interpolazione-filtraggio-e-compressione-compl#compressione-dellimmagine-biomedica","position":27},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl2":"Super-Resolution"},"type":"lvl2","url":"/c21-interpolazione-filtraggio-e-compressione-compl#super-resolution","position":28},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl2":"Super-Resolution"},"content":"Gli algoritmi di interpolazione descritti in precedenza possono ridurre la dimensione del voxel ma non possono ovviamente migliorare la risoluzione “fisica” dell’immagine che è limitata dall’effetto volume parziale. Modelliamo il caso di un nodulo di piccole dimensioni nel fegato e studiamo le conseguenze del PVE sulla possibilità di riconoscere il modulo in un sistema di imaging. Consideriamo una sfera di raggio r (ad esempio 1mm), formata da un materiale che produca un segnale S1 in un certo sistema di imaging, immersa in un altro materiale con associato un segnale S2, ed eseguiamo una scansione con voxel cubico di dimensione 2r \\times 2r \\times 2r. Il valore di CNR tra sfera e tessuto contenente come visto in precedenza sarà:CNR =\n\\frac{|M_1 - M_2|}{\\sigma}\n=\n\\frac{|p S_1 + (1-p) S_2 - S_2|}{\\sigma}\n=\n\\frac{p\\,|S_1 - S_2|}{\\sigma}\n\nDove M_1 e M_2 sono i segnali misurati. Infatti M_2 sarà uguale a S_2 mentre M_1 sarà dato dal contributo dei due tessuti nel voxel in dipendenza da p. La relazione ci dice che il CNR misurato si ottiene moltiplicando il valore di CNR ideale per il fattore p di riempimento del voxel. Nel caso migliore della sfera perfettamente inscritta nel cerchio  p=\\pi/6 mentre nel caso peggiore di sfera inclusa in modo simmetrico in otto voxel p=\\pi/48. Quindi il CNR rispetto al valore teorico si riduce di circa il 50% nel caso migliore fino a diventare circa il 6% nel caso peggiore. Se la soglia di CNR che assicura la visibilità della sfera è intermedia tra i due casi, non essendo prevedibile a livello di acquisizione il valore di p, la sfera risulterà visibile o meno in modo casuale in dipendenza dalla posizione della sfera stessa nel sistema di imaging. Consideriamo l’esempio semplificativo di Figura 2.17.\n\nFigura 2.17. Esempio di diversi valori di riempimento di un pixel per una cerchio di raggio r.\n\nA sinistra osserviamo una immagine ad alta risoluzione (HR) 2048x2048 di un fantoccio circolare. Il cerchio ha valore di segnale S_1=1000 mentre il fondo ha valore di segnale S=100. Il raggio del cerchio r è 128. Se interpoliamo a bassa risoluzione (LR) con passo 256 otteniamo una immagine 8x8, con dimensioni del pixel dell’immagine LR uguale alle dimensioni del cerchio. Se il pixel LR corrisponde esattamente al cerchio il cerchio si riduce ad un unico pixel di valore\n\\frac{\\pi r^{2}}{4\\pi r^{2}}S_{1} + \\frac{(4-\\pi)r^{2}}{4\\pi r^{2}}S_{2} \\approx 806 (immagine centrale). Se il cerchio è equi-diviso in quattro pixel LR avremo quattro pixel LR con valore diverso dal fondo pari a  valore \\frac{\\pi r^{2}}{16\\pi r^{2}}S_{1} + \\frac{(4-\\pi/4)r^{2}}{4\\pi r^{2}}S_{2} \\approx 276  (immagine a destra).\nFacendo variare lo “sfasamento” tra voxel LR e cerchio, si ottengono tutti i possibili valori di contrasto in dipendenza dallo sfasamento come riportato in Figura 2.18.\n\nFigura 2.18. Andamento del contrasto al variare dello sfasamento.\n\nTornando al caso reale, un nodulo con dimensioni dell’ordine della dimensione del voxel potrà essere visibile o meno sulla base della posizione del paziente rispetto al sistema di imaging in modo del tutto imprevedibile. La “certezza” di osservare l’oggetto si ha solo quando le sue dimensioni sono tali da saturare con certezza almeno un intero voxel. In teoria, se ripetiamo l’acquisizione un numero grande di volte variando la posizione dell’oggetto e/o la posizione della griglia di acquisizione, prima o poi otterremo il caso ottimale di sfera inscritta nel voxel e quindi visibile almeno in un set di immagini. Questa idea è alla base del concetto di super-imaging.\n\nL’idea alla base delle tecniche di Super-Resolution è quella di combinare immagini diverse della stessa struttura anatomica per ottenere una migliore risoluzione spaziale. Consideriamo  l’esempio di Figura 2.19, dove viene riportata una tipica acquisizione cardiaca MRI, nella quale vengono ottenute immagini cardiache lungo gli assi principali del cuore (asse corto, due camere, tre camere, quattro camere).\n\nFigura 2.19. Immagini multipiano del cuore in MR (4C, 2C, 3C, SA).\n\nOgni tipo di immagine è acquisita su di un piano diverso e presenta una risoluzione spaziale fortemente anisotropica (tipicamente 1.7x.17x8 mm). Nelle tecniche di super-resolution si cerca di combinare i voxel anisotropi in modo da migliorare la risoluzione spaziale sull’asse perpendicolare al piano di acquisizione (tecnicamente in MRI la slice-select direction). In Figura 2.20 sono riportati due esempi per tre acquisizioni su piani perpendicolari e per tre acquisizioni “interallacciate” in cui la distanza inter-slice è una frazione del thickness.\n\nFigura 2.20. Concetto di super-resolution (E van Reeth et al, Concepts in Magnetic Resonance part A 2012;40A:306-325).\n\nLo stesso concetto si può applicare ad una serie temporale di immagini, con la fondamentale differenza che si ha una deformazione della struttura dell’organo che va compensata con algoritmi di registrazione come sarà descritto nel seguito.\nIn generale il problema della super-resolution si può descrivere come (k=1,...,N):Y_k=D_k T_k h_k X+n_k\n\nY_k sono le N immagini a bassa risoluzione che abbiamo disponibili dal sistema di imaging (le immagini reali nel modello di immagine biomedica introdotta nel primo capitolo). D_k rappresenta in processo di “downsampling” che riduce il numero di pixel dell’immagine X. T_k rappresenta la trasformazione geometrica che descrive la posizione della griglia di acquisizione più l’eventuale deformazione indotta dal processo di acquisizione, ad esempio per i movimenti del paziente tra una acquisizione e l’altra. hk rappresenta l’effetto volume parziale che può essere modellato come la convoluzione con un kernel gaussiano in 2D o 3D. Nel caso di slice spacing nullo h_k è un filtro gaussiano 3D con kernel anisotropico con dimensione legata alla risoluzione spaziale lungo i tre assi. X è l’immagine “ideale” ad alta risoluzione. n_k è il rumore indotto dal sistema di imaging per il quale possono essere fatte le considerazione viste in precedenza per il modello dell’immagine biomedica.\nLo scopo di un algoritmo di super-resolution è stimare X note le immagini a bassa risoluzione Y_k.\nSe la risoluzione spaziale delle immagini Y_k è la stessa, h_1=h_2=...=h_N = h e D_1=D_2=...=D_N=D. Tipicamente anche il rumore avrà la stessa distribuzione in tutte le acquisizioni, quindi avremo:Y_k= D T_k h X+n\n\nIn generale il problema di stimare X da  Y_k è un tipico problema inverso, che è tipicamente “mal posto” e quindi non ha una soluzione univoca. Il modo più semplice di risolvere il problema è attraverso un algoritmo di “iterated back-projection”, in cui viene fatta una stima di X e si cerca di minimizzare in modo iterativo la differenza tra gli Y_k prodotti dalla stima di X e gli Y_k misurati (M Irani et al CVGIP 1991, doi: 10.1016/1049-9652(91)90045-L).\nPreliminarmente alla soluzione del problema inverso, è necessario stimare il disallineamento tra le immagini Y_k. In alcuni casi il disallineamento può essere noto, ad esempio dalle informazioni DICOM. Altrimenti il disallineamento deve essere stimato attraverso un algoritmo di registrazione di immagini che verrà introdotto nei capitoli successivi.\nL’idea di base dell’algoritmo è quella di partire da una stima dell’immagine HR X0 (ottenuta ad esempio dall’interpolazione di una immagine LR). A questo punto da X0 si stimano le immagini LR sulla base della conoscenza di D_k, che come prima detto si considera nota, e di h che è un filtro opportuno che tiene conto del PVE. Si ottiene così la stima di Y_k al passo 1 come:\\hat{Y}_k^{(1)}=D T_k h X_0\n\nQuesto passaggio simula il processo di acquisizione delle immagini LR, quindi h e D devono essere definiti in modo congruente al rapporto tra le risoluzioni spaziali delle immagini HR e LR.\nA questo punto l’algoritmo calcola la differenza tra l’immagine LR stimata e quella reale:G_k=Y_k-\\hat{Y}_k^{(1)}\n\nIl gradiente G utilizzato per ottenere la nuova stima di X è:G = \\sum_{k=1}^{N} D T_k^{-1} G_k\n\nDove T_k^{-1} rappresenta la trasformazione geometrica inversa che riallinea le mappe G. Nota G, si calcola il nuovo valore di X come:\\hat{X}_1 = \\hat{X}_0 + \\lambda G*H\n\nDove \\lambda determina la velocità della convergenza (un valore alto rischia di non assicurare la convergenza, un valore basso aumenta il numero di iterazioni necessarie ad ottenere una soluzione). H rappresenta il kernel di un filtro che ottimizza la convergenza, e che può essere scelto in modo arbitrario. Per le immagini biomediche standard la scelta ottima è di solito H = h.\nIl processo prosegue in modo iterativo\\hat{X}^{(n+1)} = \\hat{X}^{(n)} + \\lambda G^{(n)}*H\n\nFino a quando la differenza percentuale tra i valori di X stimati a due passi successivi non scende sotto una certa soglia. Si dimostra che l’algoritmo minimizza l’errore quadratico medio tra le Y_k misurate e quelle stimate:\\varepsilon^{(n)} =\n\\sqrt{\n\\sum_{k=1}^{N}\n\\left( Y_k - \\hat{Y}_k^{(n)} \\right)^2\n}\n\nEssendo il problema mal posto, la soluzione non è unica e quindi può dipendere dalla scelta delle condizioni iniziali \\hat{X}^{(0)}. Tipicamente \\hat{X}^{(0)} viene ottenuta interpolando una immagine a bassa risoluzione Y_k riportandola alle dimensioni dell’immagine ad alta risoluzione. Per risolvere il problema si possono inserire delle condizioni di regolarizzazione, che introducono delle ipotesi a priori sul tipo di soluzione che si desidera raggiungere. L’ipotesi tipica è che X sia “smooth”, cioè che non siano possibili transizioni troppo brusche tra i livelli di grigio. In questo caso viene minimizzata la metrica:\\varepsilon =\n\\sum_{k=1}^{N}\n\\left( Y_k - \\hat{Y}_k^{(n)} \\right)^2\n+ \\gamma \\lVert C \\hat{X} \\rVert^2\n\nDove C rappresenta un filtraggio di tipo passa-alto, quindi ad esempio il gradiente dell’immagine, e γ è una costante che rappresenta il peso della regolarizzazione nel processo di minimizzazione della metrica. Chiaramente la presenza del filtro C penalizza le soluzioni con bruschi cambiamenti di segnale ottenendo la regolarizzazione.\n\nSono stati proposti molti altri algoritmi per la soluzione del problema inverso in super-resolution, quali il  Deterministic Regularized Approach,  lo Statistical Regularized Approach e altri. Per maggiori informazioni si può fare riferimento alle review di Van Reeth et al (doi: 10.1002/cmra.21249) e H Greenspan et al (doi:10.1093/comjnl/bxm075).","type":"content","url":"/c21-interpolazione-filtraggio-e-compressione-compl#super-resolution","position":29},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl2":"Bibliografia"},"type":"lvl2","url":"/c21-interpolazione-filtraggio-e-compressione-compl#bibliografia","position":30},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl2":"Bibliografia"},"content":"R Rangayyan, Biomedical Image Analysis, CRC Press 2004\n\nM Irani et al CVGIP 1991, doi: 10.1016/1049-9652(91)90045-L\n\nVan Reeth et al (doi: 10.1002/cmra.21249) e H Greenspan et al (doi:10.1093/comjnl/bxm075)","type":"content","url":"/c21-interpolazione-filtraggio-e-compressione-compl#bibliografia","position":31},{"hierarchy":{"lvl1":"Esempi di interpolazione - Notebook 2.1"},"type":"lvl1","url":"/n-2-1-esempi-interpolazione","position":0},{"hierarchy":{"lvl1":"Esempi di interpolazione - Notebook 2.1"},"content":"Import delle librerie necessarie per la simulazione\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.ndimage as ndi\nfrom scipy.interpolate import interpn\nfrom scipy.spatial.distance import euclidean\nfrom scipy.ndimage import zoom\nimport pydicom\nimport os\nimport imageio\nimport napari\n\n\n\n","type":"content","url":"/n-2-1-esempi-interpolazione","position":1},{"hierarchy":{"lvl1":"Esempi di interpolazione - Notebook 2.1","lvl2":"Lettura file DICOM"},"type":"lvl2","url":"/n-2-1-esempi-interpolazione#lettura-file-dicom","position":2},{"hierarchy":{"lvl1":"Esempi di interpolazione - Notebook 2.1","lvl2":"Lettura file DICOM"},"content":"Import dei dati DICOM da cartella specifica e visualizzazione 3D del dataset tramite la libreria napari.\n\n# set directory\ndirectory = '../data/Phantom_CT_PET/2-CT 2.5mm-5.464/'\n\n# get all files in directory\nfiles = sorted(os.listdir(directory))\nprint(files)\n\n# get info from dicom\nds = pydicom.dcmread(directory + files[0])\nprint(\"Patient ID:\", ds.PatientID)\n\n# get size along 3 axis\nxSize = ds.Rows\nySize = ds.Columns\nzSize = len(files)\nprint(xSize, ySize, zSize)\n\n# get resolution\ndx = ds.PixelSpacing[0]\ndy = ds.PixelSpacing[1]\nloc1 = ds.ImagePositionPatient\nds2  = pydicom.dcmread(directory + files[1])\nloc2 = ds2.ImagePositionPatient\ndz = euclidean(loc1, loc2)\n\n# import volume\nvol = imageio.volread(directory, 'DICOM')\nFOV = [dx*xSize,dy*ySize,dz*zSize]\nprint(\"FOV:\", FOV)\n\n# check  size\nprint(\"dimensioni del dato:\", vol.shape)\n\n#use napari\nviewer = napari.view_image(vol, name = \"my volume\", colormap = \"gray\")\n\n#run the viewer\nnapari.run()\n\n\n\n\n\n","type":"content","url":"/n-2-1-esempi-interpolazione#lettura-file-dicom","position":3},{"hierarchy":{"lvl1":"Esempi di interpolazione - Notebook 2.1","lvl2":"Interpolazione tramite interpn"},"type":"lvl2","url":"/n-2-1-esempi-interpolazione#interpolazione-tramite-interpn","position":4},{"hierarchy":{"lvl1":"Esempi di interpolazione - Notebook 2.1","lvl2":"Interpolazione tramite interpn"},"content":"Esempio di interpolazione tramite l’utilizzo della funzione interpn e visualizzazione 3D tramite la libreria napari.\n\n# original coordinates\nx = np.arange(0,xSize*dx,dx)\ny = np.arange(0,ySize*dy,dy)\nz = np.arange(0,zSize*dz,dz)\n\n# new coordinates\nnew_x = np.arange(0,xSize*dx,dx)\nnew_y = np.arange(0,ySize*dy,dy)\nnew_z = np.arange(0,(zSize-1)*dz,dx)\n\n# mesh of new coordinates\nnew_coords = np.meshgrid(new_z,new_x, new_y, indexing = 'ij');\nvolumeInt = interpn((z,x,y), vol, np.stack(new_coords,axis=-1))\n\n# check new size\nprint(\"dimensioni dopo interpolazione:\", volumeInt.shape)\n\n# use napari\nviewer = napari.view_image(volumeInt, name = \"my volume interpolated\", colormap = \"gray\")\n\n# run the viewer\nnapari.run()\n\n\n\n\n\n","type":"content","url":"/n-2-1-esempi-interpolazione#interpolazione-tramite-interpn","position":5},{"hierarchy":{"lvl1":"Esempi di interpolazione - Notebook 2.1","lvl2":"Interpolazione tramite zoom"},"type":"lvl2","url":"/n-2-1-esempi-interpolazione#interpolazione-tramite-zoom","position":6},{"hierarchy":{"lvl1":"Esempi di interpolazione - Notebook 2.1","lvl2":"Interpolazione tramite zoom"},"content":"Interpolazione nel piano tramite l’utilizzo della funzione zoom\n\n# plane interpolation\nimg = vol[33,:,:]\n\n# original image\nfig, ax = plt.subplots()\nax.imshow(img, cmap = \"gray\")\nax.set_title(\"Original\")\nplt.show()\n\n# set scaling factor\nscaling_factors = (2, 2)\n\n\n\n","type":"content","url":"/n-2-1-esempi-interpolazione#interpolazione-tramite-zoom","position":7},{"hierarchy":{"lvl1":"Esempi di interpolazione - Notebook 2.1","lvl2":"Interpolazione NN"},"type":"lvl2","url":"/n-2-1-esempi-interpolazione#interpolazione-nn","position":8},{"hierarchy":{"lvl1":"Esempi di interpolazione - Notebook 2.1","lvl2":"Interpolazione NN"},"content":"\n\n# nearest neighbor\nresized_image = ndi.zoom(img, zoom = scaling_factors, order = 0)\nfig, ax = plt.subplots()\nax.imshow(img, cmap = \"gray\")\nax.set_title(\"NN\")\nplt.show()\n\n\n\n","type":"content","url":"/n-2-1-esempi-interpolazione#interpolazione-nn","position":9},{"hierarchy":{"lvl1":"Esempi di interpolazione - Notebook 2.1","lvl2":"Interpolazione bilineare"},"type":"lvl2","url":"/n-2-1-esempi-interpolazione#interpolazione-bilineare","position":10},{"hierarchy":{"lvl1":"Esempi di interpolazione - Notebook 2.1","lvl2":"Interpolazione bilineare"},"content":"\n\n# bilinear\nresized_image = ndi.zoom(img, zoom = scaling_factors, order = 1)\nfig, ax = plt.subplots()\nax.imshow(img, cmap = \"gray\")\nax.set_title(\"Bilinear\")\nplt.show()\n\n\n\n\n","type":"content","url":"/n-2-1-esempi-interpolazione#interpolazione-bilineare","position":11},{"hierarchy":{"lvl1":"Esempi di interpolazione - Notebook 2.1","lvl2":"Interpolazione bicubica"},"type":"lvl2","url":"/n-2-1-esempi-interpolazione#interpolazione-bicubica","position":12},{"hierarchy":{"lvl1":"Esempi di interpolazione - Notebook 2.1","lvl2":"Interpolazione bicubica"},"content":"\n\n# bicubic\nresized_image = ndi.zoom(img, zoom = scaling_factors, order = 3)\nfig, ax = plt.subplots()\nax.imshow(img, cmap = \"gray\")\nax.set_title(\"Bicubic\")\nplt.show()\n\n\n","type":"content","url":"/n-2-1-esempi-interpolazione#interpolazione-bicubica","position":13},{"hierarchy":{"lvl1":"Esempi di filtraggio - Notebook 2.2"},"type":"lvl1","url":"/n-2-2-esempi-filtraggio","position":0},{"hierarchy":{"lvl1":"Esempi di filtraggio - Notebook 2.2"},"content":"Import delle librerie necessarie per la simulazione\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.ndimage as ndi\nimport pydicom\nimport os\nimport imageio\nfrom scipy.spatial.distance import euclidean\n\n\n\n","type":"content","url":"/n-2-2-esempi-filtraggio","position":1},{"hierarchy":{"lvl1":"Esempi di filtraggio - Notebook 2.2","lvl2":"Lettura file DICOM"},"type":"lvl2","url":"/n-2-2-esempi-filtraggio#lettura-file-dicom","position":2},{"hierarchy":{"lvl1":"Esempi di filtraggio - Notebook 2.2","lvl2":"Lettura file DICOM"},"content":"Esempio di import di uno o più file DICOM e visualizzazione di una fetta del fantoccio\n\n# set directory\ndirectory = '../data/Phantom_CT_PET/2-CT 2.5mm-5.464/'\n\n# get all files in directory\nfiles = sorted(os.listdir(directory))\n\n# get info from dicom\nds = pydicom.dcmread(directory + files[0])\nprint(\"Patient ID:\", ds.PatientID)\n\n# get size along 3 axis\nxSize = ds.Rows\nySize = ds.Columns\nzSize = len(files)\nprint(xSize, ySize, zSize)\n\n# get resolution\ndx = ds.PixelSpacing[0]\ndy = ds.PixelSpacing[1]\nloc1 = ds.ImagePositionPatient\nds2  = pydicom.dcmread(directory + files[1])\nloc2 = ds2.ImagePositionPatient\ndz = euclidean(loc1, loc2)\nprint(\"dimesioni del dato:\", dx, dy, dz)\n\n# import volume\nvol = imageio.volread(directory, 'DICOM')\nFOV = [dx*xSize,dy*ySize,dz*zSize]\nprint(\"FOV:\", FOV)\nprint(vol.shape)\n\n# get plane\nimg = vol[9,:,:]\nprint(img.shape)\n\n# original image\nfig, ax = plt.subplots()\nax.imshow(img, cmap = 'gray')\nax.set_title(\"immagine originale\")\nplt.show()\n\n\n\n\n\n","type":"content","url":"/n-2-2-esempi-filtraggio#lettura-file-dicom","position":3},{"hierarchy":{"lvl1":"Esempi di filtraggio - Notebook 2.2","lvl2":"Filtro a media mobile"},"type":"lvl2","url":"/n-2-2-esempi-filtraggio#filtro-a-media-mobile","position":4},{"hierarchy":{"lvl1":"Esempi di filtraggio - Notebook 2.2","lvl2":"Filtro a media mobile"},"content":"è possibile cambiare la dimensione del filtro per apprezzarne l’effetto sull’immagine filtrata.\n\n# moving average filter\nweights = [[1/9, 1/9, 1/9],\n           [1/9, 1/9, 1/9],\n           [1/9, 1/9, 1/9]]\n\nimg_ma = ndi.convolve(img, weights)\n\nfig, ax = plt.subplots(1,2)\nax[0].imshow(img_ma, cmap = 'gray')\nax[0].set_title(\"immagine filtrata con media mobile\")\nax[1].imshow(img-img_ma, cmap = 'gray')\nax[1].set_title(\"immagine differenza\")\nplt.show()\n\n\n\n\n\n","type":"content","url":"/n-2-2-esempi-filtraggio#filtro-a-media-mobile","position":5},{"hierarchy":{"lvl1":"Esempi di filtraggio - Notebook 2.2","lvl2":"Filtro Gaussiano"},"type":"lvl2","url":"/n-2-2-esempi-filtraggio#filtro-gaussiano","position":6},{"hierarchy":{"lvl1":"Esempi di filtraggio - Notebook 2.2","lvl2":"Filtro Gaussiano"},"content":"è possibile variare la dimensione del filtro e la \\sigma per apprezzarne l’effetto sull’immagine filtrata.\n\n# gaussian average filter\nimg_gaussian = ndi.gaussian_filter(img, sigma = 3)\n\nfig, ax = plt.subplots(1,2)\nax[0].imshow(img_gaussian, cmap = 'gray')\nax[0].set_title(\"immagine filtrata con media gaussiana\")\nax[1].imshow(img-img_gaussian, cmap = 'gray')\nax[1].set_title(\"immagine differenza\")\nplt.show()\n\n\n\n\n\n","type":"content","url":"/n-2-2-esempi-filtraggio#filtro-gaussiano","position":7},{"hierarchy":{"lvl1":"Esempi di filtraggio - Notebook 2.2","lvl2":"Filtro mediano"},"type":"lvl2","url":"/n-2-2-esempi-filtraggio#filtro-mediano","position":8},{"hierarchy":{"lvl1":"Esempi di filtraggio - Notebook 2.2","lvl2":"Filtro mediano"},"content":"notare il mantenimento dei bordi rispetto al filtro a media mobile o a quello Gaussiano.\n\n# median filter\nimg_median = ndi.median_filter(img, size = 3)\n\nfig, ax = plt.subplots(1,2)\nax[0].imshow(img_median, cmap = 'gray')\nax[0].set_title(\"immagine filtrata con mediana\")\nax[1].imshow(img-img_median, cmap = 'gray')\nax[1].set_title(\"immagine differenza\")\nplt.show()\n\n\n\n\n\n","type":"content","url":"/n-2-2-esempi-filtraggio#filtro-mediano","position":9},{"hierarchy":{"lvl1":"Esempi di filtraggio - Notebook 2.2","lvl2":"Filtro di Sobel"},"type":"lvl2","url":"/n-2-2-esempi-filtraggio#filtro-di-sobel","position":10},{"hierarchy":{"lvl1":"Esempi di filtraggio - Notebook 2.2","lvl2":"Filtro di Sobel"},"content":"esempio di filtro derivativo per enfatizzare i contorni.\n\n# sobel filter\nimg_sobel_0 = ndi.sobel(img, axis = 0)\n\nfig, ax = plt.subplots(1,2)\nax[0].imshow(img_sobel_0, cmap = 'gray')\nax[0].set_title(\"immagine filtrata con Sobel lungo l'asse 0\")\nax[1].imshow(img-img_sobel_0, cmap = 'gray')\nax[1].set_title(\"immagine differenza\")\nplt.show()\n\nimg_sobel_1 = ndi.sobel(img, axis = 1)\n\nfig, ax = plt.subplots(1,2)\nax[0].imshow(img_sobel_1, cmap = 'gray')\nax[0].set_title(\"immagine filtrata con Sobel lungo l'asse 1\")\nax[1].imshow(img-img_sobel_1, cmap = 'gray')\nax[1].set_title(\"immagine differenza\")\nplt.show()\n\nimg_sobel = np.sqrt(img_sobel_0**2 + img_sobel_1**2)\n\nfig, ax = plt.subplots(1,2)\nax[0].imshow(img_sobel, cmap = 'gray')\nax[0].set_title(\"immagine filtrata lungo entrambi gli assi\")\nax[1].imshow(img-img_sobel, cmap = 'gray')\nax[1].set_title(\"immagine differenza\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/n-2-2-esempi-filtraggio#filtro-di-sobel","position":11},{"hierarchy":{"lvl1":"Esempi di filtraggio - Notebook 2.2","lvl2":"Filtro laplaciano"},"type":"lvl2","url":"/n-2-2-esempi-filtraggio#filtro-laplaciano","position":12},{"hierarchy":{"lvl1":"Esempi di filtraggio - Notebook 2.2","lvl2":"Filtro laplaciano"},"content":"altro tipo di filtro derivativo. Notare le differenze con il filtro di Sobel.\n\n# laplace filter\nimg_laplace = ndi.laplace(img)\nfig, ax = plt.subplots(1,2)\nax[0].imshow(img_laplace, cmap = 'gray')\nax[0].set_title(\"immagine filtrata con Laplace\")\nax[1].imshow(img-img_laplace, cmap = 'gray')\nax[1].set_title(\"immagine differenza\")\nplt.show()\n\n\n\n","type":"content","url":"/n-2-2-esempi-filtraggio#filtro-laplaciano","position":13},{"hierarchy":{"lvl1":"Equalizzazione dell’istogramma - Notebook 2.3"},"type":"lvl1","url":"/n-2-3-hist-equalization","position":0},{"hierarchy":{"lvl1":"Equalizzazione dell’istogramma - Notebook 2.3"},"content":"Import delle librerie necessarie per la simulazione\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport imageio\n\n\n\n","type":"content","url":"/n-2-3-hist-equalization","position":1},{"hierarchy":{"lvl1":"Equalizzazione dell’istogramma - Notebook 2.3","lvl2":"Import dati DICOM"},"type":"lvl2","url":"/n-2-3-hist-equalization#import-dati-dicom","position":2},{"hierarchy":{"lvl1":"Equalizzazione dell’istogramma - Notebook 2.3","lvl2":"Import dati DICOM"},"content":"\n\nI = imageio.v2.imread(\"../data/phantom_15.dcm\")\n\n\n\n\n","type":"content","url":"/n-2-3-hist-equalization#import-dati-dicom","position":3},{"hierarchy":{"lvl1":"Equalizzazione dell’istogramma - Notebook 2.3","lvl2":"Equalizzazione dell’istogramma"},"type":"lvl2","url":"/n-2-3-hist-equalization#equalizzazione-dellistogramma","position":4},{"hierarchy":{"lvl1":"Equalizzazione dell’istogramma - Notebook 2.3","lvl2":"Equalizzazione dell’istogramma"},"content":"\n\n# (opzionale) se I può avere negativi, porta il minimo a 0\nI = I - I.min()\n\n# livelli (assumo immagine discreta intera)\np = int(I.max()) + 1\nN = I.size\nprint(\"N =\", N)\n\n# --- escludo gli zeri prima di equalizzare ---\nmask = I > 0\nNnz = int(mask.sum())\nprint(\"N (non-zero) =\", Nnz)\n\n# histogram SOLO sui pixel > 0\ncounts = np.bincount(I[mask].astype(np.int64), minlength=p)\ncounts[0] = 0\n\n# CDF normalizzata\nCDF = np.cumsum(counts)\nCDF = CDF / CDF[-1]  # ora è in [0,1]\n\n# visualizza (PDF e CDF)\nfig, ax = plt.subplots(1, 2, figsize=(10, 4))\nax[0].imshow(I, cmap=\"gray\")\nax[0].set_title(\"Immagine originale (shifted)\")\n\nax[1].plot(counts / counts.sum(), label=\"PDF (I>0)\")\nax[1].plot(CDF, label=\"CDF (I>0)\")\nax[1].set_title(\"Istogramma e CDF (esclusi gli zeri)\")\nax[1].legend()\nplt.show()\n\n# LUT per equalizzazione\nLUT = np.floor((p - 1) * CDF).astype(np.uint16)\n\n# applico solo dove I>0\nIeq = np.zeros_like(I)\nIeq[mask] = LUT[I[mask].astype(np.int64)]\n\n# histogram equalizzata \ncounts_eq = np.bincount(Ieq[mask].astype(np.int64), minlength=p)\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 4))\nax[0].imshow(Ieq, cmap=\"gray\")\nax[0].set_title(\"Immagine equalizzata (solo I>0)\")\n\nax[1].plot(counts_eq / counts_eq.sum())\nax[1].set_title(\"Istogramma equalizzato (solo I>0)\")\nplt.show()\n\n\n\n\n\n\n\n\n","type":"content","url":"/n-2-3-hist-equalization#equalizzazione-dellistogramma","position":5},{"hierarchy":{"lvl1":"Esercitazione 2: stima dell’effetto di diversi tipi di filtraggio su alcune misure di qualità dell’immagine biomedica"},"type":"lvl1","url":"/z-2-1-esercitazione","position":0},{"hierarchy":{"lvl1":"Esercitazione 2: stima dell’effetto di diversi tipi di filtraggio su alcune misure di qualità dell’immagine biomedica"},"content":"Scopo dell’esercitazione è realizzare un programma che consenta di valutare vari algoritmi di filtraggio 3D su di un fantoccio MR (Figura 2.21) in termini di SNR e conservazione delle transizioni.\n\nFigura 2.21. Fantoccio MR.\n\nIl fantoccio che utilizziamo è un fantoccio MR costituito da tre cilindri concentrici disassati. L’intercapedine tra il secondo ed il terzo cilindro è riempita con un liquido ad alto contrasto, le altre due con acqua. E’ stata acquisita una serie di fette assiali rispetto al fantoccio contenute nella directory volume3D.\n\nPoiché vogliamo applicare un filtraggio 3D, il primo passo è quello di verificare se il volume di dati è isotropo leggendo gli opportuni campi DICOM e nel caso non lo sia operare una interpolazione per ottenere un volume isotropo. La procedura di interpolazione dovrà conservare il FOV del volume originale. In MATLAB è possibile utilizzare le funzioni interp3 in combinazione con meshgrid,\noppure la funzione imresize3 con le opportune opzioni di interpolazione.\nIl volume ottenuto può essere visualizzato tramite volumeViewer\n(disponibile a partire da MATLAB R2017a). In Python, l’interpolazione tridimensionale può essere effettuata utilizzando le funzioni\nRegularGridInterpolator o interpn della libreria SciPy, oppure, nel caso di volumi\nisotropi con fattore di scala uniforme, tramite funzioni di ridimensionamento come\nscipy.ndimage.zoom o skimage.transform.resize.\n\nIl volume risultante può essere visualizzato mediante viewer tridimensionali dedicati,\ncome napari, che consente l’esplorazione interattiva di dati volumetrici.\n\nUna volta ottenuto il volume interpolato, vogliamo sperimentare vari tipi di filtraggi in 3D e valutare l’SNR e la conservazione delle transizioni sul cilindro a massimo segnale. Per semplicità calcoleremo l’SNR e l’acutezza su una singola fetta in 2D (quella centrale) attraverso la definizione di una opportuna ROI sul cilindro centrale (acqua) e di un profilo verticale posto al centro dell’immagine.\n\nGli algoritmi di filtraggio da sperimentare includono:\n\nUn filtro a media mobile con kernel 7x7x7\n\nUn filtro gaussiano con kernel 7x7x7 e valore di sigma ottimizzato per massimizzare l’SNR e conservare le transizioni.\n\nUn filtro adattivo di Wiener con kernel 7x7x7\n\nI primi due filtri sono di tipo convolutivo e, in MATLAB, possono essere implementati tramite\nle funzioni fspecial3 (MATLAB ≥ R2018b) e imfilter, oppure mediante la funzione conv3\ndefinendo opportuni kernel tridimensionali. Per il filtro gaussiano è inoltre disponibile\nla funzione imgaussfilt3 (MATLAB ≥ R2018b), che esegue direttamente il filtraggio senza la\nnecessità di utilizzare fspecial. In Python, filtraggi convolutivi tridimensionali possono essere implementati utilizzando la funzione scipy.ndimage.convolve o scipy.signal.convolve, definendo esplicitamente\ni kernel 3D. Nel caso del filtro gaussiano, è disponibile una funzione dedicata, scipy.ndimage.gaussian_filter, che applica direttamente il filtraggio gaussiano al volume senza la necessità di definire manualmente il kernel.\n\nIl filtro di Wiener bidimensionale è implementato in MATLAB dalla funzione wiener2.\nTuttavia, non è disponibile una versione tridimensionale della funzione, che deve quindi\nessere implementata esplicitamente a partire dalla formulazione teorica del filtro di Wiener. In Python, una funzione dedicata per il filtro di Wiener 3D non è disponibile come routine\nad alto livello; pertanto, il filtraggio deve essere realizzato calcolando esplicitamente\nle statistiche locali del volume.\n\nLa formulazione del filtro di Wiener è data da:I_W = I_{\\mathrm{MM}} +\n\\frac{\\lvert I_{\\mathrm{VAR}} - \\sigma^2 \\rvert}{I_{\\mathrm{VAR}}}\n\\left( I_{\\mathrm{OR}} - I_{\\mathrm{MM}} \\right)\n\ndove I_{\\mathrm{OR}} rappresenta il volume originale tridimensionale,\nI_{\\mathrm{MM}} è il volume filtrato tramite media mobile (calcolato al passo precedente), I_{\\mathrm{VAR}} è la mappa della varianza locale, e \\sigma è la deviazione standard\ndel rumore associato all’immagine. In Python, l’implementazione del filtro di Wiener 3D può essere ottenuta combinando funzioni di convoluzione tridimensionale della libreria SciPy (ad esempio\nscipy.ndimage.convolve o scipy.ndimage.uniform_filter) per il calcolo della media e\ndella varianza locali, seguite dall’applicazione diretta della formula del filtro.\n\nI_{\\mathrm{VAR}} è la mappa della varianza sull’immagine.\nIn teoria, essendo I_{\\mathrm{VAR}}>\\sigma^2, il fattore di modulazione del filtro di Wiener \\frac{I_{\\mathrm{VAR}}-\\sigma^2}{I_{\\mathrm{VAR}}} dovrebbe variarare tra 0 e 1. In pratica tale valore diverge per valori di I_{\\mathrm{VAR}} nulli (regioni di zero padding) o per valori di I_{\\mathrm{VAR}} molto piccoli. Tali regioni vanno quindi escluse dal filtraggio se presenti oppure bisogna porre \\frac{I_{\\mathrm{VAR}}-\\sigma^2}{I_{\\mathrm{VAR}}}=1 dove assume valori non definiti (isnan, isinf) o maggiori di uno. Inoltre, sul fondo dell’immagine MR I_{\\mathrm{VAR}} sarà minore di \\sigma^2 per le proprietà del rumore riciano, per cui il fattore di modulazione del filtro \\frac{I_{\\mathrm{VAR}}-\\sigma^2}{I_{\\mathrm{VAR}}}  non si annullerà riducendo il filtraggio, cosa comunque irrilevante visto che interessa filtrare le regioni dove è presente un segnale.\n\nPer il calcolo della mappa di varianza locale I_{\\mathrm{VAR}}, in MATLAB è possibile\nutilizzare la funzione stdfilt, tenendo conto che il parametro nhood deve essere definito\nopportunamente per ottenere un filtraggio tridimensionale (funzionalità non documentata);\nin caso contrario, il filtraggio viene effettuato in modalità bidimensionale, slice-by-slice.\n\nIn Python, il calcolo della varianza locale tridimensionale può essere implementato\nesplicitamente utilizzando la libreria SciPy, ad esempio tramite la funzione\nscipy.ndimage.uniform_filter. In particolare, la varianza locale può essere ottenuta\na partire dalla relazione\\mathrm{Var}(X) = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2,\n\ncalcolando media e media dei quadrati su una finestra tridimensionale.\n\nPer il calcolo di \\sigma vogliamo implementare un metodo automatico che non richieda il tracciamento di ROI o una conoscenza a priori della geometria del fantoccio, quindi applicheremo un metodo automatico basato sull’analisi dell’istogramma di I_{\\mathrm{VAR}}.\nPer quanto riguarda la valutazione dell’acutezza, un profilo estratto dal centro dell’immagine del fantoccio non filtrato apparirà come in Figura 2.22, infatti il segnale in corrispondenza della parete dei cilindri (plexiglass) in MR è nullo a meno del rumore. Per semplicità valuteremo il valore dell’acutezza alla transizione tra fondo e acqua (aria->cilindro esterno).\n\nFigura 2.22. Esempio di profilo.\n\nIn conclusione, dato il volume MR fornito il programma da realizzare deve eseguire i tre tipi di filtraggio e fornire i valori di SNR e acutezza calcolati nella fetta centrale del fantoccio per il volume originale ed il volume filtrato con i filtri a media mobile, gaussiano e di Wiener. Il filtro Gaussiano va ottimizzato per migliorarne le prestazioni. Per visualizzare l’effetto dei vari algoritmi, è utile visualizzare la differenza tra l’immagine filtrata e quella originale.\n\nIMMAGINE\n\nSNR CLINDRO CENTRALE\n\nACUTEZZA TRANSIZIONE FONDO - CILINDRO ESTERNO\n\nORIGINALE\n\n\n\n\n\nFILTRATA MEDIA MOBILE\n\n\n\n\n\nFILTRATA FILTRO GAUSSIANO\n\n\n\n\n\nFILTRATA FILTRO DI WIENER\n\n\n\n","type":"content","url":"/z-2-1-esercitazione","position":1},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica"},"type":"lvl1","url":"/c31-segmentazione","position":0},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica"},"content":"Seguendo la classificazione della Computer Vision, la segmentazione dell’immagine biomedica è una operazione di medio livello, intermedia tra le operazioni a basso livello (interpolazione e filtraggio) e quelle ad alto livello (classificazione e registrazione). La segmentazione, o pattern recognition, realizza l’estrazione dall’immagine di regioni di interesse. Se consideriamo l’immagine biomedica in figura, la segmentazione riconosce i sei pattern fondamentali (le quattro ossa, i tessuti molli, lo sfondo) assegnandogli una label (i numeri in questo caso). Il riconoscimento dei pattern (quindi l’associazione label/nome anatomico è invece tipica di operazioni al livello superiore.\n\nFigura 3.1. Esempio di immagine biomedica. Atlante anatomico disponibile alla pagina \n\nhttp://​www​.info​-radiologie​.ch\n\nCome introdotto nel capitolo 1, l’immagine biomedica può essere modellata come una immagine “ideale”, formata da una serie di pattern omogenei, corrotta da vari processi quali l’effetto volume parziale, l’attenuazione, il rumore.I(x,y)=[[I_{0}(x,y)+n_{B}(x,y)]*h(x,y)]g(x,y)+n(x,y)\n\nlo scopo della segmentazione è estrarre dall’immagine reale I(x,y), che è quella a noi nota, l’immagine ideale I_{0}(x,y) sulla base della conoscenza del processo di formazione dell’immagine e identificare i pattern che compongono I_{0}(x,y) associando ogni pattern una etichetta o label.\nDall’osservazione dell’equazione che descrive il modello generale di immagine biomedica è facile capire come il problema sia fortemente indeterminato, in quanto i fattori che corrompono l’immagine reale sono solo parzialmente noti.\nE’ importante notare come i pattern in cui vogliamo raggruppare i pixel/voxel dell’immagine non siano “oggettivi”, ma dipendano dal quesito clinico a cui vogliamo rispondere ed in base al quale viene acquisita l’immagine. Infine, non è detto che sia di interesse estrarre tutti i pattern costituenti l’immagine, anzi in generale il processo di segmentazione sarà mirato ad estrarre un numero limitato di strutture di interesse. Possiamo quindi definire il processo di segmentazione di una immagine biomedica come:\n\nDefiniamo segmentazione di una immagine biomedica un processo che consente di associare ad un certo numero di strutture anatomiche che ci interessa individuare una lista univoca di pixel/voxel afferenti a dette strutture.\n\nPer segmentazione si può intendere quindi una operazione che associ ad alcuni pixel/voxel dell’immagine una etichetta (label) che individui a quale oggetto appartiene quel determinato pixel/voxel. I pixel/voxel rimanenti possono essere assegnati ad una label “indeterminata” che raggruppa pixel/voxel afferenti a strutture anatomiche diverse ma che non ci interessa riconoscere.\nIn questo approccio il risultato della segmentazione sono delle maschere (mask), cioè delle immagini binarie di dimensioni uguali a quella su cui viene applicata la segmentazione, che assumono valore non nullo se un pixel appartiene ad un certo oggetto e zero altrove. Per quanto detto prima un processo di segmentazione produrrà K+1 maschere, dove K è il numero di oggetti segmentati. Un metodo alternativo è creare una immagine dove ogni pixel assume un valore V(k) con k=1…K+1. Nella segmentazione di immagini 3D le maschere diverranno anch’esse degli array 3D.\n\nAlternativamente, i K oggetti segmentati possono essere rappresentati da K contorni, dove ogni contorno è una lista ordinata e chiusa di punti definiti sullo spazio dell’immagine. Il numero di punti del contorno definirà la precisione con cui il contorno è definito. In 3D, il contorno diviene una nuvola di punti in uno spazio 3D che definisce una superficie chiusa. Essendo l’ordinamento di una nuvola di punti in 3D non banalmente definibile, la superficie viene tipicamente definita attraverso un particolare ordinamento dato da un processo di triangolarizzazione che produce una mesh, come quella utilizzata negli algoritmi di visualizzazione 3D.\n\nLe rappresentazioni a maschera e a contorni sono sostanzialmente equivalenti (Figura 3.2), nel senso che è possibile passare dall’una all’altra in modo semplice. Avendo una maschera, per ottenere il contorno corrispondente si può ad esempio calcolare la distance transform che sarà introdotta nel seguito e considerare solo i livelli di griglio pari ad uno. Oppure si può calcolare il gradiente che sarà diverso da zero solo sui bordi della maschera ed adottare l’algoritmo di Canny. Per passare da un contorno ad una maschera si può utilizzare il “fence algorithm” o “Even-odd rule”. In uno spazio discreto il contorno è definito come un poligono a N lati che approssima una curva. Per ogni pixel dell’immagine si definisce una semiretta che parte dal punto ed esce fuori dall’immagine stessa, la direzione della semiretta è irrilevante.\n\nFigura 3.2. Equivalenza tra maschera e contorni\n\nSe la semiretta interseca il contorno un numero dispari di volte il punto è all’interno del contorno, se la semiretta interseca il contorno un numero pari di volte (0 si considera pari) il punto è all’esterno.\nL’implementazione è abbastanza semplice, se si considera una semiretta lungo l’asse x orientata verso lo 0:\n\nMETTERE QUI CODICE O RIMANDARE A NOTEBOOK\n\ntest if the pixel (x,y) is inside the contour\nfunction flag = pointInsidePoly(x,y,poly)\n\nN = length(poly);    % number of contour points\nj=N;\nflag=0 ; % number of crossings\nfor i=1:N\nif ( ((poly(i,1) > y) ~= (poly(j,1) > y)) & (x < ((poly(j,2) - poly(i,2))*     (y-poly(i,1))/(poly(j,1) - poly(i,1)) + poly(i,2) )) )\nflag = not(flag);\nend\nj=i;\nend\n\n((poly(i,1) > y) ~= (poly(j,1) > y)) controlla che la coordinata y sia tra la minima e massima coordinata y del segmento.  x < ((poly(j,2) - poly(i,2))*     (y-poly(i,1))/(poly(j,1) - poly(i,1)) + poly(i,2) )) controlla che coordinata x sia a destra del segmento. Usando la funzione MATLAB pointInsidePoly :fence algorithm\n\nmask = zeros(dim);\nfor x=1:dim       % explore all pixels\nfor y=1:dim\n% test if the pixel (x,y) is inside the contour\nc = pointInsidePoly(x,y,pos);\nmask(x,y) = c;\nend\nend\n\nIn Matlab la funzione  poly2mask implementa un algoritmo simile.\n\nIl numero di algoritmi di segmentazione sviluppati in letteratura ed utilizzati nella pratica clinica è virtualmente infinito. Nel seguito verrà introdotta una classificazione generale e verranno forniti alcuni esempi.","type":"content","url":"/c31-segmentazione","position":1},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Machine learning"},"type":"lvl2","url":"/c31-segmentazione#machine-learning","position":2},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Machine learning"},"content":"Dal punto di vista del “machine learning”, un algoritmo di segmentazione è un classificatore che assegna una classe a ciascun pixel dell’immagine. Gli algoritmi di machine learning si possono suddividere in unsupervised e supervised.\n\nMETTERE FIGURA QUI\n\nSi parlerà di unsupervised learning se nel processo non viene utilizzato un set di dati validato, per cui l’algoritmo è basato su di un modello che deve includere una descrizione “accurata” dei dati da classificare. La base di conoscenza è quindi racchiusa nel modello, per cui l’approccio unsupervised è anche detto model-driven.\nInvece, nel supervised learning la base di conoscenza è data da un insieme di dati labellati, ad esempio coppie di immagini con la relativa segmentazione. Il modello qui è molto più “generale” rispetto la caso unsupervised ed “impara” la sua configurazione ottimale dall’esempio fornito attraverso i dati labellati. L’approccio supervised è quindi anche detto data-driven.","type":"content","url":"/c31-segmentazione#machine-learning","position":3},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Procedure di ottimizzazione","lvl2":"Machine learning"},"type":"lvl3","url":"/c31-segmentazione#procedure-di-ottimizzazione","position":4},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Procedure di ottimizzazione","lvl2":"Machine learning"},"content":"Prima di introdurre in modo formale il concetto di segmentazione di immagini, è opportuno definire il concetto di ottimizzazione, cioè della ricerca della combinazione di parametri che massimizza (o minimizza) una certa funzione. Utilizziamo come esempio il cosiddetto problema TSP o Problema del Commesso Viaggiatore (Travelling Salesman Problem, da cui la sigla TSP). La definizione del problema è abbastanza semplice: un commesso viaggiatore deve visitare un certo numero di clienti prima di tornare a casa. Conosce la posizione dei clienti e il tempo necessario a spostarsi dall’uno all’altro. Vuole ovviamente visitare tutti i clienti una sola volta nel tempo più breve possibile. In termini più formali, il problema consiste nel costruire un grafo i cui nodi rappresentano i clienti, mentre gli archi rappresentano i percorsi fra i nodi, e di trovare su di esso un ciclo che tocchi tutti i nodi una ed una sola volta e abbia la durata complessiva minima. Il problema, semplice da descrivere, è però complesso da risolvere. Il numero delle sue soluzioni, infatti, cresce molto rapidamente con il numero dei nodi. Consideriamo l’esempio in figura:\n\nMETTERE FIGURA QUI\n\nNel grafo per il problema TSP il nodo 1 rappresenta il punto di partenza, i nodi 2:5 i clienti da visitare. I numeri sugli archi che connettono i nodi la lunghezza del percorso. Il tutto può essere illustrato anche in forma tabellare:\n\nMETTERE TABELLA QUI\n\nNotiamo che la tabella in questo caso è simmetrica, cioè la distanza tra due nodi non dipende dall’ordine. Questo può non essere vero in alcune applicazioni. Nel caso illustrato in figura le soluzioni possibili sono 4! = 24, infatti il primo nodo (1) e l’ultimo nodo (1) sono fissi ed abbiamo quindi tutte le possibili combinazioni di 4 elementi. Nel dettaglio i percorsi possibili sono: 1-2-3-4-5-1 (di durata 36), 1-2-3-5-4-1 (di durata 38), 1-2-4-3-5-1 (di durata 42), 1-2-4-5-3-1 (di durata 40), 1-2-5-3-4-1 (di durata 40), 1-2-5-4-3-1 (di durata 44), 1-3-2-4-5-1 (di durata 48), 1-3-2-5-4-1 (di durata 46), 1-3-4-2-5-1 (di durata 48), 1-3-4-5-2-1 (di durata 44), 1-3-5-2-4-1 (di durata 44), 1-3-5-4-2-1 (di durata 40), 1-4-3-2-5-1 (di durata 38), 1-4-3-5-2-1 (di durata 40), 1-4-2-3-5-1 (di durata 44), 1-4-2-5-3-1 (di durata 44), 1-4-5-3-2-1 (di durata 38), 1-4-5-2-3-1 (di durata 46), 1-5-3-4-2-1 (di durata 42), 1-5-3-2-4-1 (di durata 44), 1-5-4-3-2-1 (di durata 36), 1-5-4-2-3-1 (di durata 48), 1-5-2-3-4-1 (di durata 38), 1-5-2-4-3-1 (di durata 48).\nLe soluzioni migliori quindi sono 1-2-3-4-5-1 e 1-5-4-3-2-1, entrambe di durata 36, com’era ovvio,\ndato che si è considerato un grafo simmetrico.\nIn questo caso abbiamo risolto il problema calcolando tutte le possibili soluzioni e scegliendo la soluzione ottima. Questo approccio è detto a ricerca esaustiva, o brute force. Se il dominio di input è finito, tali algoritmi trovano sempre la soluzione corretta.\nIn generale, per n nodi il numero di soluzioni possibili sarà (n-1)!, che cresce molto rapidamente con n. Per comprendere la difficoltà di gestione di un problema di tale complessità, consideriamo un calcolatore capace di compiere 4 109 operazioni al secondo (circa 4 Gflops, dell’ordine della potenza di calcolo di un PC a 4 GHz di uso comune). Ammettiamo di riuscire a computare la lunghezza di un percorso con n operazioni (in realtà ce ne vorranno certamente di più, bisognerebbe contare anche gli accessi in memoria in lettura o scrittura). Allora il computo di tutte le soluzioni richiederà un numero di operazioni pari a:\n\nNO = n*(n-1)! = n!\n\nPer n=20 abbiamo N! = 2.4 1018 operazioni.\nIl tempo necessario con il calcolatore ipotizzato prima sarà (consideriamo 3 107 sec in un anno):\n\nT = 2.4 1018/ 4 109 = 6 108 sec ~ 20 Anni\n\nNon avendo tanta pazienza usiamo il calcolatore più veloce esistente nel 2016 (Sunway TaihuLight, China, 93000 Tera Flops) composto da 10 milioni di processori (\n\nwww.top500.org). Per curiosità la potenza necessaria al funzionamento del calcolatore è 15 MW pari a quella erogata da un piccola centrale elettrica. Con questo calcolatore sarebbero necessari:\n\nT = 2.4 1018/ 93 1015 = 26 sec\n\nPurtroppo già per n=25 la soluzione cinese non funziona\n\nT = 1.55 1025/93 1015 = 1.6 108  sec ~ 5 Anni\n\nIl grafico successivo mostra in scala logaritmica il tempo di elaborazione stimato in anni per un computer a 93000 Teraflops. Come si osserva, problemi di tipo TSP con complessità superiore a 25 sono di fatto incomputabili.\n\nMETTERE FIGURA QUI\n\nL’esempio dimostra come al crescere della dimensione del problema non sia possibile risolvere lo stesso in modo esaustivo. Il problema TSP e un esempio della famiglia  di problemi NP-completi, cioè problemi di complessità che cresce in modo non lineare con la dimensione del problema. Tali problemi non possono essere risolti in modo esaustivo quando le dimensioni dei dati di input crescono sopra una certa dimensione. Anche se esistono approcci che consentono in alcuni casi di risolvere in modo esatto un particolare problema NP-completo, in generale quello che si può fare è trovare degli algoritmi che ottengano delle soluzioni approssimate con una complessità accettabile. Tali algoritmi non potranno comunque fornire mai una soluzione sicuramente ottima, proprio perché le soluzioni possibili non sono note e quindi è impossibile determinare se una soluzione sia la migliore o meno.\nL’esempio più semplice di soluzione non esaustiva è la ricerca casuale o random. Invece di fare una ricerca esaustiva in modo sistematico utilizzando sempre lo stesso ordine di ricerca, possiamo scegliere un percorso di ricerca variabile in modo random volta per volta. Se viene esplorato tutto il dominino di input, tali algoritmi sono un caso particolare di un algoritmo esaustivo. Se viene esplorata solo una parte del dominio di input, gli algoritmi random hanno una certa probabilità di trovare la soluzione ottima pari al rapporto tra numero di percorsi esplorati e numero totale di percorsi. L’algoritmo di ricerca casuale non ha utilità pratica, ma serve come confronto per gli altri algoritmi di ottimizzazione, nel senso che qualsiasi algoritmo di ottimizzazione ragionevole deve funzionare meglio della ricerca casuale.\nUn esempio di soluzione approssimata del problema TSP si può ottenere con un approccio di tipo Greedy. Un esempio di programmazione Greedy sono gli algoritmi del tipo best-first-search, dove ci si muove in un grafo che descrive un problema scegliendo via via i nodi che sembrano migliori per risolvere il problema. Riconsideriamo il problema TSP visto in precedenza:\nPartendo dal nodo 1, il nodo alla minore distanza è il nodo 2 (distanza 8). Ci muoviamo quindi nel nodo 2. Dal nodo 2 il nodo più vicino è il nodo 3 (distanza 3). Dal nodo 3 andremo nel 4 (distanza 6) e dal nodo 4 dovremo andare nel 5 (distanza 9) e poi nell’1 (distanza 10). Il percorso (1-2-3-4-5-1) ha lunghezza totale 8+3+6+9+10=36, che come si era visto è la distanza minima. Con un algoritmo semplice abbiamo quindi trovato la soluzione ottima. In particolare il numero di passi dell’algoritmo Greedy è (N-1)+(N-2)+(N-3)+…..+1, infatti al primo passo devo fare N-1 confronti, al secondo N-2, etc. L’ordine di grandezza del numero di passi è N2 molto minore del numero di passi dell’algoritmo esaustivo. In generale non è detto che in questo modo si trovi la soluzione migliore, consideriamo ad esempio il grafo:\n\nMETTERE FIGURA QUI\n\nL’algoritmo best-first-search ritrova il percorso ottimo visto prima 1-2-3-4-5-1=36, ma esiste un percorso migliore 1-5-2-3-4-1=35 che non viene “visto”. Questo conferma quanto si era detto a proposito del problema TSP, cioè che esistono algoritmi di limitata complessità computazionale che\ndanno una soluzione approssimata, ma non necessariamente la migliore.\nLa soluzione prodotta dall’algoritmo best-first-search dipende dal nodo iniziale che si sceglie per la partenza dell’algoritmo. La figura mostra i risultati di un algoritmo best-first-search applicato al problema TSP Berlin52 variando il nodo di partenza dell’algoritmo.\n\nMETTERE FIGURA QUI\n\nSi osserva come la soluzione dipenda dal nodo, la soluzione migliore è 8182. Per confronto la migliore soluzione nota del problema Berlin52 è 7542\nPer concludere, dato un certo problema NP-completo da risolvere per via algoritmica, esisterà una soluzione esaustiva che risolve il problema in maniere ottima, ma sarà applicabile solo per dimensioni del problema molto piccole e non interessanti da un punto di vista pratico. Per risolvere il problema sarà necessario utilizzare un algoritmo di ottimizzazione più veloce dell’algoritmo esaustivo. Tale algoritmo non potrà in generale assicurare la soluzione ottima, ma solo una soluzione “ragionevolmente” buona. In generale la soluzione trovata da un algoritmo non esaustivo dipenderà dalle condizioni iniziali, e quindi varierà in funzione delle condizioni iniziali stesse.\n\nIn generale un processo di ottimizzazione sarà definito da tre caratteristiche:\n\nSpazio di ricerca (Search-space). Lo spazio di ricerca è l’insieme dei valori che possono assumere le possibili soluzioni. Ad esempio, nel problema TSP tutte le possibili liste dei nodi senza ripetizioni.\n\nMetrica: La quantità da massimizzare o minimizzare, nel caso del TSP la lunghezza di un percorso.\n\nIl processo di ottimizzazione: L’algoritmo utilizzato per trovare all’interno del search-space la soluzione che massimizza o minimizza la metrica. Nel caso del TSP il metodo greedy.\n\nGli algoritmi di ottimizzazione possono essere classificati come:\n\nOttimizzatori locali. Un ottimizzatore locale trova una soluzione partendo da un punto del search-space (condizioni iniziali). La soluzione trovata dipenderà quindi dalle condizioni iniziali. Nel problema TSP, l’algoritmo greedy applicato ad un singolo nodo iniziale è un ottimizzatore locale e la soluzione dipende dal nodo selezionato.\n\nOttimizzatori globali. Un ottimizzatore globale trova la soluzione migliore all’interno del search-space indipendentemente dai parametri di ingresso. Come detto in precedenza, l’unico vero ottimizzatore globale è la ricerca esaustiva. Nel caso del TSP un algoritmo greedy iterato su tutti i nodi si può considerare una approssimazione di un ottimizzatore globale, in quanto il risultato non dipende dalle condizioni iniziali.","type":"content","url":"/c31-segmentazione#procedure-di-ottimizzazione","position":5},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Segmentazione a Soglia"},"type":"lvl2","url":"/c31-segmentazione#segmentazione-a-soglia","position":6},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Segmentazione a Soglia"},"content":"Nella segmentazione a soglia ogni pixel viene associato ad un tipo di tessuto mediante l’intensità di segnale. La segmentazione a soglia corrisponde quindi a scegliere una o più soglie nell’istogramma dell’immagine e a classificare i pixel mediante tale informazione. Consideriamo il fantoccio MR in figura:\n\nMETTERE FIGURA QUI\n\nIl fantoccio è costituito da 6 pattern (in realtà c’è anche una parte esterna indistinguibile dallo sfondo data dallo zero padding) come in tabella:Pattern\tTessuto\tSoglie\n\n1\tSfondo\tAria (sfondo)\t<90\n2\tCilindro acqua esterno\tAcqua\t>90  <500\n3\tParete cilindro ext\tVetro (sfondo)\t<90\n4\tCilindro olio\tOlio\t>500\n5\tParete cilindro int\tVetro (sfondo)\t<90\n6\tCilindro acqua interno\tAcqua\t>90  <500\n\nI sei pattern sono associati a quattro diversi “tessuti” (aria, vetro, olio, acqua). L’aria e il vetro in MR sono due oggetti del tutto equivalenti, in quanto non forniscono alcun segnale MR. Sono quindi impossibili da distinguere con una segmentazione a soglia e li possiamo raggruppare in una classe “sfondo” (background), ottenendo quindi tre classi. Analogamente le due regioni contenenti acqua hanno lo stesso segnale. Consideriamo l’istogramma dell’immagine. Appaiono i tre picchi che descrivono la distribuzione del segnale, con ampiezza proporzionale al numero di pixel. Per dividere le classi occorrono due soglie (il numero di soglie è N-1 dove N è il numero di classi). Fissiamo le soglie a T1=90 (sfondo-acqua) e T2=500 (acqua-olio). Estraiamo dall’immagine i pixel con valori s tali che s<T1, T1≤s<T2, s≥T2 e poniamo ad un valore diverso da 0 tali pixel e a zero gli altri. Otteniamo quindi tre maschere (mask) che descrivono la distribuzione delle tre classi sull’immagine.\n\nMETTERE FIGURA QUI\n\nLe tre maschere rappresentano (colore bianco) la distribuzione dei pixel appartenenti alle classi sfondo, acqua e olio rispettivamente.\nNotiamo che alcuni pixel sono stati erroneamente assegnati alla maschera “sfondo” anche se sono acqua. Questo è dovuto alla imperfetta separazione dei picchi sfondo-acqua nell’istogramma.Dall’esempio individuiamo i limiti fondamentali della segmentazione a soglia:\n\nLa segmentazione a soglia non è in grado di distinguere oggetti topologicamente diversi ma ai quali è associato lo stesso livello di segnale. Nel nostro caso i pattern 1-3-5 e 2-6 vengono rappresentati in una sola maschera. Questo rappresenta un problema in molte applicazioni cliniche (es: ventricolo destro e sinistro nel cuore, grasso viscerale e grasso subcutaneo, etc).\n\nPossono apparire pixel spuri dovuti alle variazioni di segnale indotte dal rumore\n\nLe soglie vengono definite manualmente tramite esame visivo dell’istogramma.\n\nIl punto 1. è un limite intrinseco della segmentazione a soglia. Come si vedrà nel seguito può essere superato facendo seguire alla segmentazione a soglia l’applicazione di algoritmi di tipo topologico applicati alle maschere prodotte dalla segmentazione a soglia, come il labeling.\nIl punto 2. implica che la qualità della segmentazione ottenibile con un algoritmo a soglia dipende sostanzialmente dal CNR tra i due tessuti da separare. Anche in questo caso gli errori di segmentazione possono essere corretti, almeno in parte, applicando algoritmi di filtraggio al risultato della segmentazione.\nPer quanto riguarda il punto 3. esistono vari approcci automatici che consentono di trovare la soglia “ottima” che divide i pixel in classi attraverso soglie opportune. Tali algoritmi adottano un approccio “unsupervised learning”.","type":"content","url":"/c31-segmentazione#segmentazione-a-soglia","position":7},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Algoritmo di Otsu (adattato da Wikipedia Italia)","lvl2":"Segmentazione a Soglia"},"type":"lvl3","url":"/c31-segmentazione#algoritmo-di-otsu-adattato-da-wikipedia-italia","position":8},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Algoritmo di Otsu (adattato da Wikipedia Italia)","lvl2":"Segmentazione a Soglia"},"content":"L’algoritmo di Otsu nella sua versione originale presume che nell’immagine da segmentare siano presenti due sole classi e quindi calcola la soglia ottima per separare queste due classi minimizzando la varianza intra classe. L’algoritmo può essere esteso a più classi (multi Otsu method). L’algoritmo di Otsu standard minimizza la quantità (varianza intra classe):\n\nMETTERE FORMULA QUI\n\ndove ω è la probabilità di una classe separata dall’altra dalla soglia t e σ è la SD della classe. Quindi l’algoritmo cerca di trovare la soglia che separa l’immagine in due sottoclassi che siano il più possibile omogenee al loro interno. Intuitivamente, immaginando un istogramma a due picchi il metodo di Otsu consiste nel trovare la soglia t intermedia tra i due picchi che li divida in modo ottimale.\nSi dimostra che minimizzare la varianza intra-classe è equivalente a massimizzare la varianza inter-classe:\n\nMETTERE FORMULA QUI\n\ndove  μ è il valor medio di una classe. Il metodo consiste nel provare in modo esaustivo tutti i possibili t (che sono in numero uguale alla profondità dell’immagine) e prendere il t che minimizza la varianza inter classe. Da un punto di vista algoritmico:\n\nSi calcola l’istogramma h dell’immagine. La probabilità ω si ottiene normalizzando l’istogramma per il numero di pixel dell’immagine (da un punto di vista pratico la normalizzazione non è necessaria).\n\nSi calcola il valore di σ2b(t) per ogni t calcolando METERE FORMULE QUI\n\ne  ω2(t)  e μ2(t) in modo analogo operando le somme da t+1 in avanti.\n\nSi sceglie il t che massimizza σ2b(t)\n\nIn questo modo la soglia t viene definita in modo automatico. In MATLAB il metodo di Otsu è implementato nella funzione otsuthresh quando si utilizza l’istogramma estratto dall’immagine o  graythresh quando si opera direttamente sull’immagine stessa.\nIl metodo di Otsu opera secondo un processo di ottimizzazione basato sulla ricerca esaustiva su tutti i possibili valori del parametro t da ottimizzare, ed è quindi non particolarmente efficiente da un punto di vista computazionale. Tuttavia, essendo il calcolo della quantità σ2b(t) da ottimizzare molto rapido il calcolo di t può essere effettuato in tempi ragionevoli. Notiamo che il tempo di calcolo è fortemente dipendente dal valore di profondità dell’immagine che determina il numero di prove da effettuare e la lunghezza delle somme da calcolare. Si noti infine che possono esistere più valori di t per cui σ2b(t) è massima, si pensi al caso di un istogramma con due picchi ben separati. In questo caso l’algoritmo ritornerà il valor medio tra i valori di t che massimizzano σ2b(t).","type":"content","url":"/c31-segmentazione#algoritmo-di-otsu-adattato-da-wikipedia-italia","position":9},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Clustering"},"type":"lvl2","url":"/c31-segmentazione#clustering","position":10},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Clustering"},"content":"Un altro metodo per la scelta dei valori di soglia corretti che minimizzino l’errore nella segmentazione è l’uso di algoritmi di clustering.  Il Clustering è una tecnica di analisi dei dati volta alla selezione e raggruppamento di elementi omogenei in un insieme di dati.\nL’approccio è del tipo unsupervised, nel senso che l’algoritmo di clustering riesce a dividere i dati in una serie di insiemi avendo come unico input il numero di insiemi da trovare. Gli algoritmi di Clustering si possono applicare a dati di diversa natura, e trovano applicazione in molteplici discipline come la bioinformatica, il marketing, la genetica, etc.OBS 1\tOBS 2\tOBS 3\t\t\tOBS K\n\nE 1\t\t\t\t\t\t\nE 2\t\t\t\t\t\t\nE 3\n\nE N\n\nIn generale i dati di ingresso di un algoritmo di clustering sono composti da una tabella con N righe, che corrispondono agli elementi da analizzare, e da K colonne che corrispondono alle osservazioni disponibili sugli elementi stessi. Ad esempio le righe della tabella potrebbero rappresentare dei pazienti e le colonne dei dati clinici sui pazienti (analisi del sangue, valori di pressione, etc). Lo scopo del clustering è raggruppare gli elementi simili tra loro in gruppi (cluster) sulla base delle osservazioni.\nIn figura è esemplificato un problema classico di clustering: abbiamo un insieme di dati (caratterizzati da una coppia di valori) intuitivamente raggruppabili in quattro classi e vogliamo ottenere la soluzione a destra in cui i dati sono opportunamente raggruppati. In questo caso l’osservazione intuitiva che i dati si raggruppano in quattro cluster corrisponde all’utilizzo come distanza tra i dati della distanza geometrica sul piano. Punti tra loro “vicini” secondo la distanza scelta sono assegnati allo stesso insieme.\n\nMETTERE FIGURA QUI\n\nNel campo dell’analisi delle immagini biomediche, le righe della tabella rappresentano le locazioni spaziali su cui vengono acquisite le immagini (quindi i pixel/voxel dell’immagine), mentre le colonne rappresentano i valori di segnale acquisiti in corrispondenza delle locazioni spaziali. In definitiva, l’input dell’algoritmo di clustering sarà una matrice NxM, dove N è il numero di locazioni spaziali considerate (numero di pixel o voxel) e M il numero di acquisizioni disponibili.\nQuindi per una immagine 2D che contiene Nx x Ny pixel, la matrice di clustering corrispondente sarà un matrice a Nx x Ny righe ed una singola colonna. Per una immagine 3D la matrice di clustering corrispondente sarà una matrice a Nx x Ny xNz righe ed una singola colonna. Una immagine a colori RGB corrisponderà ad una matrice a tre colonne (K=3), in quanto per ogni pixel sono disponibili tre misure corrispondenti alla relativa tripletta RGB.\n\nMETTERE FIGURA QUI\n\nUna immagine 2D +T corrisponderà ad una matrice di clustering con N = Nx x Ny e K = nT, in quanto per ogni pixel sono disponibili nT osservazioni ad intervallo di tempo diversi. Analogamente una immagine 3D + nT corrisponderà a una matrice di clustering con N = Nx x Ny x Nz e K = nT. E’ importante notare che la corrispondenza ha senso se gli nT frame temporali sono allineati, cioè se i pixel nella serie temporale corrispondono alle stesse locazioni spaziali. Ad esempio una serie temporale in cui il paziente è fermo e viene iniettato un contrasto che cambia i livelli di grigio può essere interpretata in questo modo, mentre una serie temporale che segue il battito cardiaco no. Altri esempi in MRI sono immagini T1, T2, PD pesate dello stesso distretto anatomico. Oppure potremmo avere immagini multiecho acquisite a tempi di eco diversi.\n\nMETTERE FIGURA QUI\n\nGli algoritmi di clustering usati nella segmentazione di immagini si possono classificare in tre classi:\n\nClustering esclusivo (k-means)\n\nClustering non esclusivo (Fuzzy C-mean)\n\nClustering probabilistico\n\nNel clustering esclusivo, un dato deve appartenere ad uno ed un solo cluster. L’implementazione più nota di questo approccio è l’algoritmo K-MEANS.","type":"content","url":"/c31-segmentazione#clustering","position":11},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Algoritmo K-means","lvl2":"Clustering"},"type":"lvl3","url":"/c31-segmentazione#algoritmo-k-means","position":12},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Algoritmo K-means","lvl2":"Clustering"},"content":"L’algoritmo K-means (MacQueen, 1967) è l’algoritmo base ed uno dei primi ad essere stato realizzato per la classificazione di dati. Dato un insieme Y = (y1,…yn) di dati da classificare in C gruppi (cluster) e una distanza || || definita su Y, possiamo definire una funzione obiettivo da minimizzare. Questa funzione è:\n\nMETTERE FORMULA QUI\n\ndove è la distanza tra un dato yj e un centroide vk. Per ogni dato vengono contate solo le distanze rispetto al centroide più vicino. Questa funzione obiettivo è un indicatore della distanza dei dati y dai centri dei rispettivi cluster. La distanza utilizzata può essere una qualsiasi metrica. Il problema di trovare l’assegnazione dei dati ai cluster che realizzi il minimo di J è NP-completo con complessità computazionale O(ndC+1 log n) dove d è la dimensionalità dei dati. Il problema per n non piccolissimo non è quindi risolubile in modo esaustivo e si utilizzano algoritmi in grado di trovare un minimo locale di J.\nLa procedura più nota è abbastanza semplice e richiede la definizione di un numero di cluster (C) in cui dividere i dati. Si definiscono C centroidi, uno per ogni cluster da trovare, in modo casuale ma in modo che siano abbastanza lontani tra loro, in modo da migliorare la convergenza dell’algoritmo. A questo punto per ogni dato calcoliamo la distanza dai centroidi ed associamo il dato al centroide più vicino. Calcoliamo ora C nuovi centroidi, come centro di massa dei cluster ottenuti al passo precedente. Possiamo ora ricomputare la distanza di tutti i dati dai nuovi centroidi e creare così dei nuovi cluster. Iterando il procedimento, arriveremo ad un punto in cui i centroidi si stabilizzano senza cambiare più di posizione. Saremo quindi in una situazione di convergenza dell’algoritmo che ci dà il clustering desiderato.\n\nLa procedura per trovare il valore minimo di J è la seguente:\n\nAssegnare un valore iniziale ai C centroidi vk\n\nIterare i due passi seguenti fino a quando i valori di vk si modificano:\n\nAssegnare ogni dato y ad uno ed un solo cluster sulla base della distanza di y dal centroide del cluster.\n\nAggiornare i valori dei centroidi vk come media dei dati y appartenenti al centroide k\n\nLa procedura descritta precedentemente minimizza la funzione obiettivo, ma non è ovviamente un procedimento esaustivo. È possibile quindi provare che l’algoritmo k-means converge sempre, ma non è detto che la configurazione di convergenza sia quella che trova un minimo assoluto della funzione obiettivo. L’algoritmo k-means è quindi un ottimizzatore locale. Poiché lo stato di convergenza dell’algoritmo dipende dalla definizione iniziale dei centroidi, è possibile ripetere il procedimento più volte assegnando in modo casuale il valore iniziale dei centroidi e tra le varie configurazioni stabili trovare quella dove la funzione obiettivo è minima.\nSi noti che dal punto di vista dell’algoritmo di clustering il numero di dimensioni che caratterizza i dati y e quindi i centroidi v è irrilevante. In generale y sarà un vettore a n dimensioni e v avrà n dimensioni come y. La distanza verrà calcolata nello spazio n-dimensionale dei dati y.In MATLAB l’algoritmo K-MEANS è implementato nella funzione kmeans (Statistics Toolbox).\n\nL’algoritmo presenta alcune difficoltà:\n\nÈ necessario un algoritmo per inizializzare i centroidi. Un possibile metodo è far coincidere i C centroidi iniziali con C campioni scelti a caso dai dati. Oppure è possibile definire i valori iniziali dei centroidi sulla base della conoscenza a priori dei dati (opzione start di kmeans)\n\nIl risultato finale dipende dai centroidi iniziali. Questo può essere sfruttato facendo girare più volte l’algoritmo in dipendenza da condizioni iniziali diverse e prendendo come risultato ottimo quello che minimizza J. (opzione nreplicates di kmeans).\n\nPuò accadere che il un cluster si svuoti, per cui non può essere aggiornato. In questo caso l’algoritmo si blocca (opzione EmptyAction).\n\nIl risultato dipende dalla metrica (opzione Distance). A volte è opportuno normalizzare le variabili rispetto alla deviazione standard (variabili standardizzate).\n\nIl risultato dipende dal numero C di cluster che è predefinito.\n\nRiguardo l’ultimo punto, in generale non esiste un metodo per la stima del numero ottimo di cluster. Si utilizza un modello (ad esempio nella segmentazione di immagini possiamo conoscere a priori quanti tessuti vogliamo segmentare), oppure si fanno diversi tentativi con C diversi e si cerca di stimare la soluzione migliore utilizzando il valore di J.\nUna variante del k-means è il metodo “Iterative Self-Organizing Data Analysis Technique” (ISODATA), che è sostanzialmente un algoritmo k-means in cui il numero di cluster non è un parametro di input non modificabile ma uno dei risultati dell’algoritmo che viene computato insieme ai cluster. Nell’approccio ISODATA due cluster possono essere combinati tra loro se la loro separazione è minore di una certa soglia mentre un singolo cluster può essere diviso in due se è troppo “disperso” nello spazio di ricerca.\nQuando applicato alla segmentazione di immagini, la metrica utilizzata è tipicamente la differenza assoluta o la media quadratica delle differenze dei livelli di grigio. Nel caso di immagini singole le due metriche sono chiaramente coincidenti. Nel caso di sequenze temporali, si possono usare metriche di tipo correlativo che descrivono ad esempio la correlazione temporale tra la variazione del livello di segnale dei pixel nel tempo.\nNel caso dell’immagine del fantoccio introdotta in precedenza, l’analisi visiva dell’istogramma ci suggerisce l’uso di C=3 cluster. Il problema è chiaramente monodimensionale (singola immagine). L’algoritmo kmeans con tre cluster ci fornisce tre centroidi (12.4, 174, 921) che rappresentano i valori tipici dei tre tessuti e le maschere dei tre tessuti sull’immagine:\n\nMETTERE FIGURA QUI\n\nQuesta rappresentazione è alternativa rispetto alle tre maschere distinte che avevamo introdotto in precedenza ma ha l’identico significato. Il risultato è molto simile a quello ottenuto con la valutazione manuale delle soglie, con la differenza che qui le soglie stesse vengono trovate automaticamente. Questo tipo di maschere possono essere visualizzate a falsi colori dove ad ogni colore corrisponde un tessuto come nella rappresentazione di destra.\n\nSupponiamo ora di acquisire lo stesso fantoccio con due sequenze MR diverse, come in figura. Il livello di grigio associato ai tre tessuti sarà diverso tra le due immagini. Ogni dato sarà quindi caratterizzato da una coppia di valori. Il funzionamento dell’algoritmo di clustering è esattamente lo stesso con la differenza che ora la distanza tra due pixel non è più la differenza assoluta tra due valori ma la distanza (euclidea o di Manhattan) tra i due pixel in uno spazio bi-dimensionale.\n\nMETTERE FIGURA QUI\n\nin questo caso i dati da fornire all’algoritmo kmeans saranno un vettore Nx2, dove N è il numero di pixel dell’immagine. L’algoritmo k-means ci restituirà il valore dei centroidi, che sarà rappresentato come un vettore 3x2, perché ogni centroide è definito in uno spazio bidimensionale, ed una maschera come in precedenza.\n\nMETTERE FIGURA QUI\n\nil grafico rappresenta la distribuzione degli N pixel in dipendenza dal valore che il segnale dei pixel assume sulle due immagini. Il kmeans fornisce come coordinate dei centroidi:\n\n14.4741   16.2826\t\t\tsfondo\n170.3552  391.8894\t\t\tacqua\n923.4150  758.6847\t\t\tolio\n\nche rappresentano i valori tipici di segnale dei tessuti nelle due immagini, e la maschera:\n\nMETTERE FIGURA QUI\n\nIl metodo può essere esteso a qualunque numero di immagini.\n\nÈ importante notare come applicare un clustering multidimensionale abbia senso solo se si hanno più misure su una stessa locazione spaziale. Le immagini 3D (in cui ogni voxel corrisponde ad una diversa locazione spaziale) vengono elaborate con un algoritmo di clustering monodimensionale esattamente come le immagini 2D, quindi come una lista di pixels (o voxel).","type":"content","url":"/c31-segmentazione#algoritmo-k-means","position":13},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Clustering non esclusivo (FCM)","lvl2":"Clustering"},"type":"lvl3","url":"/c31-segmentazione#clustering-non-esclusivo-fcm","position":14},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Clustering non esclusivo (FCM)","lvl2":"Clustering"},"content":"Nel clustering non esclusivo, spesso definito fuzzy clustering, un dato può appartenere a più cluster con diversi livelli di appartenenza. La somma dei livelli di appartenenza su tutti i possibili cluster per un dato dovrà essere 1. L’algoritmo fuzzy c-means (FCM) quindi generalizza l’algoritmo K-means, consentendo una segmentazione più graduale basata sulla teoria del set di regole fuzzy.\nLa fuzzy logic o logica sfumata è un’estensione della logica booleana, basata su un grado di verità di ciascuna proposizione. È fortemente legata alla teoria degli insiemi sfocati e, dopo essere già stata intuita da pensatori precedenti, venne concretizzata da Lotfi Zadeh.\nLa teoria degli insiemi fuzzy costituisce un’estensione della teoria classica degli insiemi poiché per essa non valgono i principi aristotelici di non contraddizione e del terzo escluso (o del Tertium non datur). Il principio di non contraddizione stabilisce che, dati due insiemi A e !A (non-A), ogni elemento appartenente all’insieme A non può contemporaneamente appartenere anche a non-A; l’intersezione di A e !A è l’insieme vuoto. Secondo il principio del terzo escluso, se un qualunque elemento non appartiene all’insieme A, esso necessariamente deve appartenere al suo complemento non-A. L’unione di un insieme A e del suo complemento non-A costituisce il dominio completo di definizione degli elementi di A.\nLa modifica introdotta dalla logica Fuzzy è di rifiutare questo assunto. Quando parliamo di grado di verità o valore di appartenenza intenderemo che una proprietà può assumere oltre che i valori vera (valore 1) o falsa (valore 0) come nella logica classica, anche valori intermedi. In logica fuzzy si può ad esempio dire che un bambino appena nato è giovane di valore 1, un diciottenne è giovane 0,8, ed un sessantacinquenne è giovane di valore 0,15 (tale valore dipende dall’età del docente che tiene il corso). Solitamente il valore di appartenenza si indica con u. È importante notare che il concetto di appartenenza fuzzy non ha nulla a che vedere con il concetto di probabilità. Nella probabilità una affermazione o è vera o è falsa con una certa probabilità, mentre nella logica fuzzy è insieme vera e falsa.\nL’algoritmo fuzzy c-means (FCM) ha quindi la particolarità di consentire ad un dato di appartenere a più cluster contemporaneamente. Il metodo è stato ideato da Dunn nel 1973 e migliorato da Bezdek nel 1981). Il metodo è basato sulla minimizzazione della funzione obiettivo:\n\nMETTERE FORMULA QUI\n\ndove m è un numero strettamente maggiore di 1, ujk è il grado di appartenenza di yj rispetto al cluster k, yj è il j-esimo di N dati d-dimensionali appartenenti all’insieme Ω, vk è il centro d-dimensionale  del cluster k, e ||*|| è una distanza. C è il numero di cluster.  m è un parametro detto fuzzyness dell’algoritmo. Di solito si pone m=2. Si noti che se u è una matrice binaria, cioè può assumere solo i valori 0 e 1, la funzione J diviene uguale a quella definita per il K-means, che si può quindi considerare un caso particolare dell’algoritmo FCM. In particolare, come si vede dalla figura al crescere del valore di m il peso dei valori di appartenenza “piccoli” nel computo di J diviene sempre minore, fino ad annullarsi per valori di m molto grandi. L’algoritmo FCM va quindi a coincidere con il k-means per m che tende ad infinito.\n\nMETTERE FIGURA QUI\n\nLa complessità computazionale del problema è equivalente a quella del K-means, per cui anche il partizionamento fuzzy è ottenuto attraverso una ottimizzazione iterativa della funzione obiettivo, in modo simile a quanto visto per il K-means, attraverso l’aggiornamento della funzione di appartenenza uik e dei centri dei cluster vk:\n\nu_(j,k)=1/(∑_(q=1)^C▒(‖y_j-v_k ‖/‖y_j-v_q ‖ )^(2/(m-1)) )        v_k=(∑_(j=1)^N▒〖u_(jk )^m y_j 〗)/(∑_(j=1)^N▒u_(jk )^m )\n\nIl processo iterativo si ferma quando la differenza tra il valore corrente di u ed il valore precedente è più piccola di una soglia. La procedura descritta converge ad un minimo locale della funzione obiettivo, JFCM. Si noti che a differenza del k-means dove la condizione di convergenza è univoca nel FCM la condizione di convergenza dipende dalla soglia utilizzata.\nL’algoritmo FCM presenta gli stessi problemi associati all’algoritmo k-means, con la possibile eccezione dello svuotamento dei cluster che è meno probabile per la natura continua (e non discreta come nel k-means) del processo di ottimizzazione.\n\nL’algoritmo di fuzzy clustering è implementato in matlab attraverso la funzione fcm (Fuzzy Logic Toolbox).\nSe applichiamo l’algoritmo allo stesso fantoccio utilizzato in precedenza otteniamo i centroidi:\n\n13.53   15.86\n170.76  392.26\n925.27  773.48\n\nchiaramente simili a quelli precedenti. Otterremo invece tre maschere, una per ogni tessuto, dove i valori delle maschere non sono più binari ma sono distribuiti tra 0 e 1. Un valore 1 (bianco) indica la massima appartenenza del tessuto al cluster.\n\nMETTERE FIGURA QUI\n\nL’approccio FCM è di interesse nella segmentazione delle immagini mediche in quanto consente di tener conto del PVE. Infatti in una immagine biomedica il segnale di alcuni pixel/voxel sarà dato da due o più tessuti che si compenetrano nella stessa regione elementare di spazio nella quale viene misurato il segnale. L’approccio FCM consente di assegnare a tali pixel/voxel un valore di appartenenza distribuito tra i tessuti presenti, interpretando nel modo corretto il PVE.\n\nA titolo di esempio, consideriamo l’immagine in figura (imageNS) formata da sei pattern omogenei, di valore [20, 70, 150, 300, 550, 750], con l’aggiunta di rumore gaussiano e l’applicazione di un filtro a media mobile per simulare il PVE.\n\nMETTERE FIGURA QUI\n\nAbbiamo evidentemente C = 6. Applichiamo all’immagini il FCM attraverso la funzione MATLAB fcm.\n\n[center,U,J] = fcm(imageNS(:),6)\n\nSi noti che abbiamo trasformato l’immagine in un vettore monodimensionale attraverso l’operatore (:). La funzione fcm restituisce il valore dei centroidi (center), la funzione di appartenenza (U) e il valore J della funzione obiettivo durante le iterazioni. Il grafico della funzione J mostra come l’algoritmo minimizzi il valore di J e si fermi quando J non varia più in modo significativo.\n\nMETTERE FIGURA QUI\n\nI valori dei centroidi risultanti sono [19.95,   69.78,  153.42,  300.54,  548.46,  748.53]  e rappresentano una stima del valore di segnale dei 6 pattern omogenei.\nU è un array 6xN dove N è il numero di pixel dell’immagine. Ognuna delle 6 componenti di U rappresenta la mappa di appartenenza per un certo cluster. La mappa di appartenenza del cluster di valore massimo (750) risulta come in figura a sinistra, con un valore massimo (1) in corrispondenza del pattern.\n\nMETTERE FIGURA QUI\n\nA destra viene riportata la mappa di appartenenza del cluster con valore 550. Come si vede i pixel di bordo del pattern che assumono un valore intermedio tra 550 e lo sfondo hanno un grado di appartenenza minore di 1 (intorno a 0.5), tranne nell’interfaccia tra il pattern “550” e il pattern “300” dove l’appartenenza assume un valore più alto in quanto il segnale risultante dal PVE è più vicino al valore di segnale del pattern. Notiamo infine che ai pixel di bordo del pattern “750” viene assegnato un grado di appartenenza elevato al cluster “550”. Tali pixel infatti risultano dal PVE tra il pattern “750” e il pattern “70” e quindi possono assumere valori simili al pattern “550”. Questa errata attribuzione è tipica degli algoritmi di segmentazione a soglia quali l’algoritmo FCM, che non avendo connotazioni topologiche non è in grado di distinguere strutture topologicamente connesse o meno.","type":"content","url":"/c31-segmentazione#clustering-non-esclusivo-fcm","position":15},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Algoritmo EM (Expectation Maximization) e Gaussian Mixture","lvl2":"Clustering"},"type":"lvl3","url":"/c31-segmentazione#algoritmo-em-expectation-maximization-e-gaussian-mixture","position":16},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Algoritmo EM (Expectation Maximization) e Gaussian Mixture","lvl2":"Clustering"},"content":"Gli algoritmi di segmentazione prima visti non assumono alcuna ipotesi sulla distribuzione probabilistica dei dati. Questo in generale può non essere corretto.\nConsideriamo ad esempio la separazione tra aria e acqua in un fantoccio MR esaminandone l’istogramma:\n\nMETTERE FIGURA QUI\n\nUn algoritmo di clustering (o il metodo di Otsu) troverà una soglia che massimizza la separazione  tra i due picchi. Questa soglia però non terra conto del fatto che a causa dell’effetto volume parziale i pixel dell’acqua possono assumere comunque livelli di grigio inferiori alla soglia sui bordi. Inoltre la distribuzione dell’aria è non gaussiana (è di Rician) e quindi i pixel dell’aria hanno maggior probabilità di superare la soglia rispetto a quelli dell’acqua. L’approccio ti tipo EM consente di tener conto di tali peculiarità introducendo informazioni sulla distribuzione di probabilità dei vari cluster.L’approccio di tipo EM introduce quindi il clustering basato su modelli (model-based approach), dove la distribuzione dei dati sui singoli cluster viene modellata attraverso funzioni probabilistiche note, tra le quali la distribuzione di tipo gaussiano è la più utilizzata. L’approccio modellistico consente di tenere in conto eventuali ipotesi sulla generazione dei dati e sul rumore che li accompagna. In pratica, ogni cluster è rappresentato matematicamente da una distribuzione di tipo parametrico, come le distribuzioni Gaussiana (Continua) o di Poisson (Discreta). L’insieme dei dati è modellato come una combinazione di queste distribuzioni. Le singole distribuzioni sono chiamate distribuzioni componenti.\n\nIl caso più semplice nel clustering probabilistico è quello della combinazione di gaussiane (mixture of Gaussian). I cluster sono modellati come gaussiane centrate sui centroidi dei cluster. In figura i cerchi in grigio rappresentano la varianza delle distribuzioni. Possiamo pensare che i due assi rappresentino il livello di grigio dei pixel di due immagini ed i punti nel piano il livello di grigio del singolo pixel. Vogliamo raggruppare gli spot in due classi, supponiamo quindi di avere due cluster (k=2). Dato un punto del piano, il punto avrà una certa probabilità di essere stato generato da ognuno dei due cluster, probabilità che dipende dai parametri della gaussiana che descrive il cluster stesso. In particolare, una gaussiana con un centro “lontano” dal punto o con una varianza piccola avrà basse probabilità di aver generato il punto, e viceversa. Nel caso delle immagini, una immagine con basso SNR avrà più possibilità di generare un pixel lontano dal suo valor medio e viceversa.\n\nMETTERE FIGURA QUI\n\nSe i parametri che descrivono le gaussiane sono noti, il problema si riduce ad utilizzare come distanza nell’algoritmo di clustering la probabilità che un certo dato sia stato generato da una certa gaussiana. Avremo quindi:\n\nMETTERE FORMULA QUI\n\nIl problema diviene più complesso quando i parametri caratterizzanti le gaussiane non sono noti. L’algoritmo di clustering dovrà quindi stimare oltre ai cluster anche i parametri delle distribuzioni componenti. Quello che vogliamo trovare sono quindi i parametri delle gaussiane che hanno la maggiore probabilità di aver generato i dati osservati.\n\nIl procedimento da utilizzare, detto EM (expectation maximization) è di tipo iterativo. L’algoritmo EM è molto generale e può essere utilizzato per le più diverse distribuzioni di probabilità. Facciamo un esempio semplice nell’ipotesi che le distribuzioni di probabilità siano Gaussiane.\n\nIl problema si può formalizzare come segue:\nSiano date K classi; nel caso della segmentazione le classi corrispondono ai pattern dell’immagine.\n\nOgni classe abbia una probabilità  “a priori” Pk; nel caso della segmentazione la probabilità a priori corrisponde al numero normalizzato dei pixel appartenenti alla classe (altezza del picco nell’istogramma).\n\nOgni classe abbia media Mk e SD k; nel caso della segmentazione media e SD descrivono la posizione e larghezza del picco dell’istogramma relativo alla classe.\n\nK (numero delle classi) è noto mentre gli altri parametri devono essere determinati.\n\nSe ciascuna classe obbedisce ad una legge di probabilità gaussiana si può definire per la distribuzione dei livelli di grigio dell’immagine (cioè l’istogramma dell’immagine):\n\nh(s)=∑_(k=1)^K▒〖P_k G(s|M_k,σ_k ) 〗\n\nSi tratta di scegliere il vettore delle probabilità a priori P, delle medie M e delle SD σ in modo da minimizzare la differenza tra istogramma e somma di gaussiane. Il problema può essere affrontato tramite l’algoritmo EM che è composto da due passi che vengono iterati. Nel caso della gaussian mixture, l’algoritmo EM è abbastanza semplice ed è composto dai due passi seguenti:\n\nSi definisce un valore iniziale dei valori P, M e σ. Essendo l’algoritmo EM un ottimizzatore locale il risultato finale dipenderà dai valori iniziali. Dati P, M e σ si calcolano le probabilità che un certo valore di segnale sia stato generato da un certa gaussiana :\n\nPasso E-step (expectation) calcolo della probabilità che il dato xj sia stato generato dalla gaussiana i.\n\np_ij=P_i G(x_j |M_i,σ_i )\n\nche è facilmente ricavabile dalla formula della gaussiana, essendo il valore della gaussiana stessa nel punto per la probabilità a priori che un pixel appartenga a quella gaussiana.\n\nIl secondo passo è detto M-step (maximization) e consiste nell’aggiornare i parametri delle gaussiane in base al passo precedente. Definiamo:\n\nMETTERE FORMULA QUI\n\nIl passo M consiste nelle operazioni:\n\nMETTERE FORMULA QUI\n\nIn questo modo abbiamo definito delle nuove gaussiane e torniamo al passo E. L’algoritmo al solito si ferma quando i valori della media, della varianza e dei pesi si stabilizzano. A questo punto i parametri trovati descrivono i cluster.\nE’ possibile dimostrare che l’algoritmo EM aumenta il grado di verosimiglianza della combinazione di gaussiane ad ogni iterazione e converge sotto certe condizioni ad un massimo locale della verosimiglianza della distribuzione di gaussiane. La dimostrazione è estremamente complessa e non viene qui riportata.   L’algoritmo EM può essere utilizzato non sono nella risoluzione del problema della gaussian mixture, ma in molte altre applicazioni.\nI problemi fondamentali in cui può incorrere l’algoritmo EM applicato alle gaussian mixture sono il fatto che due gaussiane convergano alla stessa gaussiana, con la scomparsa di un cluster, e che una gaussiana assuma varianza infinita e quindi si trasformi in un valore costante. Al solito variare le condizioni iniziali partendo da una stima ragionevole dei parametri può ottimizzare il funzionamento dell’algoritmo EM.\nL’algoritmo EM-GM è implementato in Matlab dalla funzione fitgmdist nello Statistics and Machine Learning Toolbox.\n\nMETTERE FIGURA QUI\n\nApplicato all’immagine del fantoccio MRI, il metodo fornisce tre gaussiane, con valori medi   11.7349  170.7140 905.0376 simili ai centroidi dei cluster prima trovati, e  varianze 137, 858, 12000, probabilmente per l’effetto del volume parziale tra acqua e olio che costringe a “spalmare” le due gaussiane.\n\nLa somma delle gaussiane trovate rappresenta poi un approssimazione dell’istogramma dell’immagine, per cui l’algoritmo EM può anche essere considerato un metodo per modellare l’istogramma.","type":"content","url":"/c31-segmentazione#algoritmo-em-expectation-maximization-e-gaussian-mixture","position":17},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Clustering Gerarchico","lvl2":"Clustering"},"type":"lvl3","url":"/c31-segmentazione#clustering-gerarchico","position":18},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Clustering Gerarchico","lvl2":"Clustering"},"content":"In generale, è possibile che un cluster di dati possa essere diviso a sua volta in sub-cluster più piccoli, che a loro volta possono essere divisi in sub-cluster ancora più piccoli e così via. Un esempio tipico è la classificazione degli organismi vegetali o animali, che vengono divisi in categorie sempre più specifiche (ordini, specie). Nell’imaging il clustering gerarchico può essere utilizzato per raggruppare immagini appartenenti ad esempio ad una serie temporale, come si vedrà nel capitolo dedicato agli algoritmi di registrazione. Il clustering gerarchico è una tecnica che permette di creare un albero gerarchico, in cui gli elementi dell’insieme su cui si opera il clustering sono le foglie dell’albero. Una riga orizzontale nell’albero individua una serie di cluster analoghi a quelli ottenuti nel clustering tradizionale.\nDato un insieme di N oggetti da raggruppare, ed una matrice NxN di distanze tra gli oggetti, il processo di clustering gerarchico può essere definito come (S.C. Johnson 1967):\n\nSi definiscono N cluster, uno per ogni oggetto. Abbiamo quindi N cluster che contengono ognuno un solo oggetto. Le distanze tra i cluster saranno uguali alle distanze tra gli oggetti che in questo primo passo si identificano con i cluster stessi.\n\nSi identificano i due cluster più vicini nel senso della distanza adottata, cioè più simili. Questi due cluster vengono raggruppati in un cluster unico, abbiamo così N-1 cluster, uno con due oggetti e gli altri con un solo oggetto.\n\nRicomputiamo la matrice delle distanze, che sarà ora una matrice N-1xN-1.\n\nSi ripetono i passi 2 e 3 fino a quando non rimane un solo cluster che contiene N oggetti.\n\nIl passo 3 può essere eseguito in modi diversi, in base ai diversi approcci possibili riconosciamo tre categorie di clustering gerarchico: single-linkage (singolo collegamento), complete-linkage (collegamento completo)  e average-linkage (collegamento mediato).\nNel single-linkage clustering, la distanza tra due cluster è definita come la minima distanza tra tutti gli elementi di un cluster e tutti quelli dell’altro cluster. In pratica si calcola la matrice delle distanze tra gli elementi dei due cluster e si prende come distanza tra i due cluster il minimo sulla matrice. Se invece di una funzione distanza si utilizza una funzione di similarità, cioè una funzione che è grande quando i due oggetti sono simili, si considererà il massimo della matrice di similarità.\nNel complete-linkage clustering (chiamato anche metodo del diametro o del massimo), la distanza tra due cluster sarà definita come il massimo sulla matrice delle distanze computata tra I dati sui due cluster. Nell’average-linkage clustering, la distanza tra due cluster sarà la media della matrice delle distanze computata sui dati appartenenti ai due cluster. Una variazione abbastanza usata del metodo precedente è il metodo che usa la mediana della matrice delle distanze invece che la media, che è meno sensibile a dati spuri.\nL’approccio descritto finora è di tipo agglomerativo, perche si basa sul ragruppamento progressivo di cluster sempre più grandi. Esiste anche un approccio inverso, dove si parte da un cluster che comprende tutti i dati e si procede per divisioni successive (divisive hierarchical clustering). Questo approccio è comunque molto meno diffuso.","type":"content","url":"/c31-segmentazione#clustering-gerarchico","position":19},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl4":"Esempio di Single-Linkage Clustering (FORMATTARE)","lvl3":"Clustering Gerarchico","lvl2":"Clustering"},"type":"lvl4","url":"/c31-segmentazione#esempio-di-single-linkage-clustering-formattare","position":20},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl4":"Esempio di Single-Linkage Clustering (FORMATTARE)","lvl3":"Clustering Gerarchico","lvl2":"Clustering"},"content":"Consideriamo un approccio single-linkage e descriviamo come è impostato un algoritmo che lo implementi. L’idea di base è cancellare in modo progressivo righe e colonne della matrice delle distanze dei dati coinvolti nell’operazione conglobando coppie di righe e di colonne in una sola riga o colonna.\nSia D = [d(i,j)] la matrice NxN delle distanze. Vogliamo ottenere n cluster 0,1,......, (n-1) dove L(k) è il livello gerarchico del singolo cluster. Il cluster m sarà indicato con (m) mentre la distanza tra due cluster (r) e (s) sara indicata da d [(r),(s)].\nL’algoritmo procederà nel modo seguente:Inizia con il cluster di livello L(0) = 0 e di posizione nella sequenza dei cluster m = 0.\nTrova i due cluster più simili, siano essi (r) e (s) \n\nd[(r),(s)] = min d[(i),(j)]\ndove l’operazione di minimo è estesa a tutte le possibili coppie di cluster.\nIncrementa l’indice della sequenza dei cluster: m = m +1. Combina i cluster (r) and (s) in un singolo cluster di livello\nL(m) = d[(r),(s)]\nAggiorna la matrice delle distanze D, cancellando le righe e le colonne corrispondenti ai cluster (r) e (s) e aggiungendo una nuova riga ed una nuova colonna corrispondenti al nuovo cluster ottenuto al passo precedente. La distanza tra il nuovo cluster e un vecchi cluster k è definita come:\nd[(k), (r,s)] = min d[(k),(r)], d[(k),(s)]\nSe esiste un solo cluster, la procedura si ferma. Altrimenti vai al passo 2.\n\nMETTERE FIGURA QUI\n\nE’ importante notare che il processo è dipendente dalla funzione distanza scelta. Scelte diverse della funzione distanza producono risultati anche completamente diversi.","type":"content","url":"/c31-segmentazione#esempio-di-single-linkage-clustering-formattare","position":21},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Metriche negli algoritmi di clustering","lvl2":"Clustering"},"type":"lvl3","url":"/c31-segmentazione#metriche-negli-algoritmi-di-clustering","position":22},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Metriche negli algoritmi di clustering","lvl2":"Clustering"},"content":"Negli algoritmi d clustering è importante la scelta della metrica, cioè della grandezza che misura la differenza (o distanza) tra due punti nello spazio da classificare.\nLa scelta più immediata è quella di distanze di tipo geometrico, scelte cioè nella famiglia delle metriche di Minkowski:\n\nMETTERE FORMULA QUI\n\nche comprendono la classica distanza euclidea come caso particolare per p=2. Per p=1 abbiamo la cosiddetta distanza Manhattan.\nLa tabella mostra le principali metriche utilizzate negli algoritmi di clustering come misura di distanza tra gli elementi. Come si vede la maggior parte delle distanze sono derivate dalla distanza di Minkowski. Un esempio di applicazione della correlazione come distanza può essere la segmentazione di immagini 2D+T con iniezione di mezzo di contrasto, nella quale ci interessa raggruppare tra loro i pixel che rispondono in modo simile all’all’arrivo del contrasto.\nInfine è opportuno notare come il funzionamento di un algoritmo di clustering multidimensionale dipenda dalla scala su cui sono misurati i valori dei dati su cui fare il clustering. Se i dati non sono omogenei, i dati con valori maggiori peseranno di più nel computo della distanza falsando il comportamento dell’algoritmo. Si pensi ad esempio ad un clustering in cui si utilizzano immagini su 8 bit ed immagini a 16 bit. In questi casi è opportuno normalizzare i dati su distribuzioni di tipo standard (tipicamente distribuzioni gaussiane con media zero e deviazione standard 1), un processo detto standardizzazione delle variabili. L’operazione di standardizzazione in Matlab è implementato dalle funzioni zscore o normalize (dalla versione R2018).\n\nMETTERE TABELLA QUI","type":"content","url":"/c31-segmentazione#metriche-negli-algoritmi-di-clustering","position":23},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Algoritmi di Labeling"},"type":"lvl2","url":"/c31-segmentazione#algoritmi-di-labeling","position":24},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Algoritmi di Labeling"},"content":"Come detto in precedenza, la segmentazione a soglia non è in grado di distinguere oggetti topologicamente diversi ma ai quali è associato lo stesso livello di segnale. Nell’immagine del fantoccio ad esempio abbiamo pattern topologicamente distinti che vengono estratti come un oggetto unico. Questo è un limite intrinseco della segmentazione a soglia, che può essere superato elaborando i dati della segmentazione stessa.\nIl metodo più diretto è scomporre la segmentazione usando un algoritmo detto “label region”.  L’algoritmo lavora su una immagine binaria (quindi una maschera) e funziona nel seguente modo:\n\nPer ogni pixel dell’immagine:\n\nSe è il primo pixel, crea un gruppo (blob) e aggiungi il pixel al blob\n\nPer tutti i pixel dell’intorno:\nControlla se sono già stati assegnati a un blob. Se si, vai a 2\nControlla se appartengono allo stesso blob del pixel di partenza. Se si, aggiungili al blob e vai a 2\n\nCompletato il primo blob, si prende un pixel non appartenente al blob e si ritorna a 1), creando un nuovo blob. Si itera fino a quando tutti i pixel sono stati assegnati.\nI blob ottenuti possono essere classificati per grandezza e i blob più piccoli eventualmente eliminati (come nel filtro mediano). Per pixel dell’intorno si intendono i pixel contenuti in un kernel centrato sul pixel stesso di dimensioni 3x3 e può includere o meno le diagonali.\n\nIn MATLAB la funzione da utilizzare è bwlabel o bwconncomp (Vedi esempio ES-MAT-5). Se consideriamo il fantoccio MR (sinistra) e applichiamo una segmentazione a soglia con ad esempio l’algoritmo kmeans, ricaveremo una maschera dell’acqua come al centro della figura. L’algoritmo di labeling fornirà una mappa in cui le regioni non appartenenti alla maschera dell’acqua saranno settate a 0, mentre la maschera verrà divisa in una serie di regioni topologicamente connesse identificate da un indice univoco.\n\nMETTERE FIGURA QUI","type":"content","url":"/c31-segmentazione#algoritmi-di-labeling","position":25},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Region Growing"},"type":"lvl2","url":"/c31-segmentazione#region-growing","position":26},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Region Growing"},"content":"Una estensione della segmentazione a soglia è l’algoritmo “region growing”, dove partendo da un pixel all’interno dell’oggetto da segmentare si estende la segmentazione a tutta la regione di interesse. In questo caso la soglia è quindi definita in modo locale come la differenza di segnale tra un pixel ed i pixel vicini. Questo approccio sfrutta le informazioni spaziali e garantisce la formazione di regioni tra loro collegate. Di fatto è una implementazione ricorsiva di una segmentazione effettuata per pixel adiacenti.\nLa routine prevede la definizione di un punto di partenza all’interno del pattern da riconoscere. Il valore di livello di grigio così individuato costituisce il punto di partenza (detto ‘seed pixel’, ‘pixel seme’) per la successiva elaborazione: si vanno  ad analizzare ricorsivamente i pixel adiacenti a quello selezionato inizialmente. Quelli che hanno una differenza di livello di grigio appartenente ad un intervallo prefissato vengono selezionati, gli altri scartati.\n\nL’approccio Region Growing prevede quindi i seguenti passi:\n\nSi sceglie un arbitrario ‘seed pixel’ che viene confrontato con quelli adiacenti.\n\nDal seme si passa ad una regione che ‘cresce’ aggiungendo progressivamente quei pixel adiacenti che sono ‘simili’ a quello iniziale secondo criteri di somiglianza fissati.\n\nQuando la crescita della regione si ferma il processo termina\n\nIl seed pixel può essere scelto in modo manuale o con algoritmi automatici.\nLa funzione “paint” definita in tutti i programmi di image processing utilizza questo metodo. Questo tipo di segmentazione consente di individuare in modo ottimale i confini degli oggetti individuati per osservazione.\nIl punto cruciale della tecnica “region growing” è il criterio di similarità dei pixel, che determina se un pixel debba o meno essere aggiunto alla regione. Nel caso di immagini binarie, il criterio di simiglianza si identifica con l’uguaglianza del valore dei pixel. In questo caso l’algoritmo si comporta come l’algoritmo di labeling.\nNel caso di immagini a livello di grigio, la condizione di inclusione diviene ABS(s0-s1) < T, dove T è un valore di soglia e ABS è il valore assoluto. Si include quindi un pixel se il suo valore differisce da quello di un pixel adiacente appartenente alla regione per meno di T. Per il buon funzionamento dell’algoritmo, T deve essere abbastanza grande da includere tutti i pixel nella regione omogenea che vogliamo segmentare e abbastanza piccolo da impedire che la segmentazione “debordi” nei tessuti vicini.\n\nMETTERE FIGURA QUI\n\nIn figura da sinistra a destra un esempio di segmentazione region growing con T ottimale, con T troppo basso (alcuni pixel vengono persi) e con T troppo alto.\n\nT può essere definito esplicitamente come numero di livelli di grigio (es T=50). E’ anche possibile definire T come percentuale rispetto al valore dei pixel, la condizione di inclusione diviene ABS(s0-s1)/MEAN(s0,s1) < T . In questo caso T sarà compreso tra 0 e 1 e un pixel verrà incluso se la sua differenza rispetto al pixel vicino è inferiore al 100*T %.\nUn approccio ragionevole può essere quello di definire T come proporzionale al rumore sull’immagine. Infatti, se consideriamo una zona omogenea dell’immagine, come ad esempio l’acqua o l’olio nel fantoccio, il segnale nella regione omogenea sarà S0+N(0,σ) nel caso di rumore gaussiano a media nulla. Se ad esempio poniamo T=1.96σ, otterremo di includere il 95% dei pixel della regione. Questo approccio è simile a quello visto nel design dei filtri adattivi.\nL’algoritmo di region growing è  implementato in MATLAB dalla funzione grayconnected.","type":"content","url":"/c31-segmentazione#region-growing","position":27},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Segmentazione a contorni"},"type":"lvl2","url":"/c31-segmentazione#segmentazione-a-contorni","position":28},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Segmentazione a contorni"},"content":"Un approccio diverso al problema della segmentazione consiste nell’identificare sull’immagine i contorni dei pattern, che corrispondono alle zone di transizione tra due tessuti diversi.\nConsideriamo l’immagine del fantoccio in figura ed il relativo profilo dei livelli di grigio estratto dall’immagine stessa.\n\nMETTERE FIGURA QUI\n\nI contorni visti sul profilo saranno caratterizzati da una transizione netta del segnale. Se calcoliamo la derivata del profilo, valori alti della derivata corrisponderanno ai contorni, come si vede dalla figura. Per trovare i contorni, possiamo definite una soglia sul valore assoluto della derivata, valori della derivata superiori alla soglia definiscono una transizione e quindi un contorno.\n\nMETTERE FIGURA QUI\n\nEstendendo il concetto in 2D o 3D, un mappa di gradiente dell’immagine consente di identificare i contorni. La mappa di gradiente di una immagine può essere ottenuta in vari modi, tipicamente attraverso un filtro convolutivo. Ad esempio il filtro di Sobel calcola la derivate del gradiente per righe e per colonne utilizzando i due kernel convolutivi:    -1\t0\t1\t\t1\t2\t 1  \n\nKx\t=-\t2\t0\t2\tKy=\t0\t0\t 0\n-1\t0\t1\t\t-1\t-2\t-1\n\ne il gradiente viene poi calcolato come G=SQRT(Gx^2+Gy^2)\nUn’altra possibilità è usare il filtro Laplaciano:\n\n0\t-1\t0\n-1\t4\t-1\n0\t-1\t0\n\nla dimensione del kernel può essere aumentata per ottimizzare il calcolo. Le operazioni di filtraggio convolutivo possono essere implementate attraverso la funzione conv2 di MATLAB, tuttavia è più opportuno utilizzare delle funzioni specifiche.\nIn particolare la funzione edge dell’Image Processing Toolbox trova i contorni dell’immagine attraverso una serie di filtri, che possono essere scelti tra quello di Sobel, di Roberts, Lapalciano, etc. Sull’immagine di gradiente viene applicata una soglia per tagliare i valori bassi di gradiente e evidenziare i contorni.\n\nMETTERE FIGURA QUI\n\nUna volta ottenuta la mappa dei contorni (edge map) con questo tipo di algoritmi, è necessario costruire un contorno continuo che unisca le aree ad alto valore di gradiente. Uno degli approcci più semplici per collegare i punti di discontinuità in un’immagine di gradiente è quello di analizzare le caratteristiche dei pixel in un intorno ristretto (3x3 o 5x5) del punto in esame. In altre parole si cercano delle proprietà comuni ai punti dell’intorno e successivamente si collegano i punti simili con qualche criterio. La creazione di un contorno continuo da una mappa di gradiente utilizza tipicamente due parametri (algoritmo di Canny):\n\nIntensità della mappa di gradiente. Un punto nell’intorno è simile al punto considerato se la differenza del valore di modulo del gradiente nel punto è minore di una soglia T.\n\nDirezione del gradiente.  Un punto nell’intorno è simile al punto considerato se la direzione del gradiente nel punto (data dal rapporto tra le componenti x e y del gradiente) è minor e di una soglia A\n\nIl processo può essere completato eliminando piccoli tratti di segmenti isolati e colmando piccoli intervalli tra i segmenti.","type":"content","url":"/c31-segmentazione#segmentazione-a-contorni","position":29},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Active Contours (Snakes)","lvl2":"Segmentazione a contorni"},"type":"lvl3","url":"/c31-segmentazione#active-contours-snakes","position":30},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Active Contours (Snakes)","lvl2":"Segmentazione a contorni"},"content":"I metodi prima visti hanno il limite fondamentale di non includere al loro interno un modello dell’oggetto da segmentare. Nell’imaging biomedico, ad esempio, possiamo senz’altro supporre che gli organi siano oggetti con una superficie che non presenta spigoli o punti ad elevata curvatura. Per molti organi abbiamo inoltre una informazione a priori sulla forma, ad esempio un vaso sarà simile ad un cilindro, un ventricolo visto in asse lungo ad un ellissoide, etc.\nEsistono vari algoritmi che introducono queste informazioni ottimizzando la segmentazione. L’esempio più noto sono gli algoritmi a contorni attivi o snakes.\nL’algoritmo tradizionale, introdotto per la prima volta da Kass, consiste nell’inizializzare una certa curva (snake) e nel deformarla poi in modo da farla convergere in corrispondenza di un minimo locale di energia, cioè di un contorno dell’immagine.\nIl primo problema da affrontare è quindi quello della definizione del contorno iniziale: essendo un metodo di analisi locale infatti i risultati dipenderanno dal modo in cui è stato inizializzato lo snake. Pertanto in dipendenza dell’applicazione si dovrà decidere se inizializzare automaticamente la curva oppure usare una procedura semiautomatica in cui le condizioni iniziali sono gestite direttamente da un utente esperto.\nLa curva iniziale C è definita da un insieme di N punti ordinati caratterizzati dalle loro coordinate  (xi,yi) per i=1,..,N. La curva è chiusa quindi il punto N è connesso con il punto 1.\n\nMETTERE FIGURA QUI\n\nPossiamo anche definire C come una curva parametrica in s dove s indica la posizione del punto sulla curva:\n\nC(s) = {(x(s), y(s)) : 0 ≤ s ≤ 1}\n\nAd uno snake può essere associata una energia, vista come somma di una componente interna e di una esterna o associata all’immagine:\n\nE(C)=∫_0^1▒〖E_int+E_imm 〗\n\nFacendo evolvere lo snake in modo che l’energia dello snake venga minimizzata, si ottiene una curva che evolvendosi nel tempo va ad operare una segmentazione sulle immagini. Le forze interne tendono a conservare la forma dello snake e quindi determinano il modo in cui lo snake si evolve, quelle esterne sono correlate all’immagine e determinano le caratteristiche che vengono rilevate.\nL’energia interna può essere divisa in due componenti:\n\nE_int (C(s))=α|C’(s)|^2+β|C’'(s)|^2\n\nLa prima componente è legata alla derivata prima e rappresenta la tendenza dello snake a mantenere la sua forma opponendosi alla trazione. L’energia elastica è legata alla distanza dei punti dello snake, aumentando la distanza tra due punti aumenta anche l’energia elastica.  La seconda componente (rigidità) è legata alla derivata seconda e rappresenta la tendenza dello snake ad opporsi alle modifiche della sua curvatura. Impedisce allo snake di “ingarbugliarsi”.\nManipolando la tensione o elasticità e la rigidità dello snake è possibile modificare l’importanza relativa delle due componenti dell’energia interna. Intuitivamente si può pensare allo snake come ad un elastico, che se lasciato libero riassume la sua forma originale grazie alla sua elasticità.Il secondo contributo viene di solito chiamato energia dell’immagine o energia esterna in quanto viene ricavata dall’immagine stessa in modo da assumere valore minimo nelle zone di maggiore interesse come ad esempio i bordi. Tipiche e semplici funzioni di energia dell’immagine sono legate all’edge map dell’immagine:\n\nMETTERE FORMULA QUI\n\ndove  è una maschera gaussiana bidimensionale con una certa deviazione standard  (filtro gaussiano) aumentando la quale si rendono i contorni più sfocati, ma si aumenta il range di cattura di tali contorni. Introducendo dei pesi per le forze prima definite abbiamo per l’energia dello snake:\n\nE_Snake=∝∫▒〖|C’(s)|^2 ds+〗  β∫▒〖|C’'(s)|^2 ds〗+kE_imm (s)\n\nDove α è il peso dell’energia elastica, β il peso dell’energia di curvatura, k il peso delle forze esterne relative all’immagine.\n\nL’idea è di lasciare evolvere lo snake sull’immagine fino quando non raggiunge un minimo dell’energia. Questo stato di minimo corrisponderà ad una situazione stabile in cui lo snake corrisponderà ai contorni definiti sull’immagine stessa a meno dei limiti di rigidità ed elasticità imposti. L’algoritmo per minimizzare l’energia è un processo iterativo che parte da un contorno iniziale e converge (se la differenza della posizione dello snake tra due iterazioni è abbastanza piccola) ad un minimo locale. Il processo iterativo è efficiente dal punto di vista computazionale (ogni passo è rappresentato dalla moltiplicazione di matrici numeriche) ed il suo risultato dipende dai valori ,  e . Da un punto di vista pratico l’utilizzo del metodo AC (active contours) richiede il tuning accurato dei parametri ,  e  e la possibilità di definire un contorno iniziale abbastanza vicino al risultato desiderato in modo da ottimizzare la convergenza dell’operatore locale.\nCome si osserva dall’equazione dello snake, le forze esterne sono computate solo sullo snake stesso. Questo implica che lo snake nella sua evoluzione “vede” i contorni dell’immagine solo quando gli attraversa. Una modifica dell’algoritmo implica l’uso di informazioni interne ed esterne allo snake (region-based models) in modo da favorire la convergenza. L’implementazione più nota è l’algoritmo di Chan-Vese che troviamo implementato in MATLAB.\nConsideriamo una immagine con due pattern con livello di grigio I1 e I2, a meno del rumore e di altri fattori quali il PVE e l’attenuazione. Nell’approccio di Chan-Vese l’energia da minimizzare è valutata come:\n\nF(C)=F_1 (C)+F_2 (C)=∫_(inside(C))▒〖|I-C1|^2+∫_(outside(C))▒|I-C2|^2 〗\n\nDove I è l’immagine, C1 è la media dei pixel dell’immagine dentro il contorno C, C2 è la media dell’immagine fuori dal contorno C. Chiaramente F(C) si minimizza quando C corrisponde al bordo tra i due pattern e divide l’immagine in due regioni omogenee in cui F1 e F2 sono uguali alla potenza del rumore associato all’immagine. L’approccio realmente implementato è più complesso in quando si estende ad una immagine ad N pattern omogenei.\nIn MATLAB la funzione activecontour implementa le versioni proposte da Chan (Chan-Vese) e Caselles (edge).  La funzione dei parametri alfa e beta dello snake è svolta dai parametri ContractionBias (determina se il contorno si espande o si restringe, alfa) e SmoothFactor (determina la regolarità del contorno, beta).\nGli algoritmi a contorni attivi funzionano su immagini bidimensionali. In teoria è possibile estendere l’algoritmo ad immagini 3D, in questo caso il contorno diviene una superficie chiusa (balloon) definita da una mesh geometrica come negli algoritmi di visualizzazione 3D. Nella pratica nell’elaborazione di immagini 3D si utilizza un approccio diverso detto level set.","type":"content","url":"/c31-segmentazione#active-contours-snakes","position":31},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Level set","lvl2":"Segmentazione a contorni"},"type":"lvl3","url":"/c31-segmentazione#level-set","position":32},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Level set","lvl2":"Segmentazione a contorni"},"content":"L’algoritmo level set (o implicit Active Contour) definisce il contorno C come l’intersezione di una funzione (x,y) con il piano dell’immagine che corrisponde a (x,y)=0.\n\nMETTERE FIGURA QUI\n\nHoang Ngan Le T. et al. (2020) Active Contour Model in Deep Learning Era: A Revise and Review. In: Oliva D., Hinojosa S. (eds) Applications of Hybrid Metaheuristic Algorithms for Image Processing. Studies in Computational Intelligence, vol 890. Springer, Cham. \n\nHoang Ngan Le et al. (2020)\n\nLa figura mostra come il processo iterativo che modifica  nel tempo causi l’evoluzione del contorno C sul piano. Da un punto di vista formale si definisce:\n\nC = {(x, y): φ(x, y) = 0},\n\nl’evoluzione della curva è governata dall’equazione:\n∂φ/∂t+F⌈∇φ⌉=0\n\nDove F è la “speed function” che caratterizza l’algoritmo. F deve essere zero sui contorni dell’immagine in modo da assicurare la convergenza. Ad esempio potremmo definire:\n\nF(x,y)=1/(1+λ⌈∇I(x,y)⌉ )\n\nQuindi per un valore del gradiente dell’immagine alto (transizione tra tessuti) F tenderà a 0, per un valore del gradiente molto basso (regione omogenea) F tenderà a 1 ed il contorno avanzerà velocemente. Il parametro  modula il peso del gradiente sul valore di F.\n\nPartendo da un valore iniziale di , e quindi di C, viene applicato un algoritmo iterativo che simula nel discreto l’evoluzione temporale ed il contorno si modifica seguendo l’evoluzione di  fino alla convergenza. Una potenzialità interessante dell’algoritmo level set è che il contorno definito da  si può dividere in due o più contorni durante la progressione dell’algoritmo (e viceversa) permettendo una maggiore flessibilità rispetto all’algoritmo a contorni attivi.\n\nMETTERE FIGURA QUI\n\nDa Wikipedia (Eng)","type":"content","url":"/c31-segmentazione#level-set","position":33},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Watershed transform"},"type":"lvl2","url":"/c31-segmentazione#watershed-transform","position":34},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Watershed transform"},"content":"Una importante famiglia di tecniche per la segmentazione di immagini si basa su concetto della ricerca delle componenti connesse (Connected component, CC). Il concetto alla base di queste tecniche è la cosiddetta watershed transform, (spartiacque) nella quale il valore di livello di grigio dell’immagine rappresenta la profondità di un pixel rispetto ad un valore di riferimento. Una immagine viene quindi vista come una struttura (ad esempio un lago) che viene via via riempita con acqua in una certa posizione. Man mano che si versa l’acqua, il pelo dell’acqua stessa si alza disegnando delle strutture (maschere) che individuano le regioni sotto una certa altezza. Quando si arriva ad un watershed, l’acqua trabocca in una regione vicina. L’algoritmo, concettualmente simile al region growing, individua le configurazioni “borderline” che precedono il travaso dell’acqua da una regione dove la superficie dell’acqua è “stabile” ad un’altra.\n\nMETTERE FIGURA QUI\n\nConsideriamo il fantoccio MR in figura e il relativo profilo. L’asse Y del profilo esprime in questo caso la distanza dal fondo che ha riferimento 0. Immettiamo l’acqua da un seed posto sul fondo. L’acqua riempirà i pixel connessi al seed e più “bassi” (quindi con livello di grigio minore) del seed.  Questa funzione è implementata in Matlab da grayconnected. Man mano che sale l’acqua i pixel del fondo vengono sommersi e la maschera risultante cresce.\n\nMETTERE FIGURA QUI     Th=1\t\t       Th=20\t                     Th=30                              Th=70\n\nA Th=1 viene coperto lo zero-padding, a Th=20 una parte del fondo. Ad un certo punto la maschera si stabilizza fino a quando il livello non supera lo spartiacque della parete del fantoccio, dopodiché ricomincia a crescere lentamente fino a stabilizzarsi di nuovo fino al riempimento dell’olio.\n\nMETTERE FIGURA QUI\n\nSe tracciamo il grafico dell’area della maschera in funzione del livello otteniamo la situazione in figura, dove come si è detto periodi di area stabile indicano che sono state riconosciute le strutture principali del fantoccio. L’algoritmo watershed attraverso soglie opportune riconosce le transizioni tra i periodi di riempimento e produce le relative maschere. Un metodo semplice è calcolare il gradiente della curva (in rosso in figura) e scegliere le soglie tra due picchi del gradiente.\n\nMETTERE FIGURA QUI\n\nMappe per th=200, th=500, th=650, th=1900.\n\nMETTERE FIGURA QUI\n\nMappe ottenute per sottrazione progressiva delle maschere. Notiamo che pur utilizzando un approccio a soglia, l’algoritmo riesce a distinguere regioni topologicamente non connesse.","type":"content","url":"/c31-segmentazione#watershed-transform","position":35},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Algoritmo MSERs","lvl2":"Watershed transform"},"type":"lvl3","url":"/c31-segmentazione#algoritmo-msers","position":36},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Algoritmo MSERs","lvl2":"Watershed transform"},"content":"L’algoritmo prima descritto può essere generalizzato ottenendo l’algoritmo Maximally Stable Extremal Regions (MSERs). L’algoritmo MSERs ricerca le regioni estreme massimamente stabili all’interno dell’immagine. Le regioni estreme sono definite come le regioni connesse i cui livelli di grigio sono tutti al di sopra o al di sotto dei valori dei pixel che circondano la regione. Le connessioni tra un pixel ed i pixel circostanti come al solito possono essere definiti dai 4 (6) pixel vicini o dagli 8 (24) pixel vicini in 2D o 3D, rispettivamente. Come visto in precedenza, tra le regioni estreme quelle massimamente stabili sono quelle dove, definita una soglia Δ, la variazione del numero di pixel della regione variando il livello di grigio g da g-Δ a g+Δ è un minimo locale. Il valore di Δ determina il numero di regioni che vengono trovate, che è inversamente proporzionale a Δ.L’algoritmo MSERs nella sua forma originaria prevede i seguenti passi:\n\nI pixel dell’immagine vengono ordinati secondo il valore del livello di grigio\n\nPartendo dal valore più basso di livello di grigio, i pixel vengono inseriti nell’immagine (si considera di partire da una immagine vuota) e si individuano le componenti connesse con un opportuno algoritmo di labeling.\n\nViene memorizzata per ogni livello di grigio la lista delle componenti connesse e la loro area. Se due componenti si uniscono, la più piccola viene inglobata nella più grande.\n\nSi ottiene quindi una lista che per ogni livello di grigio contiene l’area delle componenti connesse.\nPer ogni componente, si considera la curva area vs livello di grigio ed in corrispondenza dei minimi locali della curva si individuano le componenti massimamente stabili.\n\nDal punto di vista computazionale, il punto critico è il terzo. Bisogna per ogni passo ordinare le componenti per grandezza e riconoscere quali provengono da componenti esistenti al passo precedente. Per ottimizzare la procedura si memorizzano i dati con un approccio Component Tree, nel quale le componenti vengono memorizzate in un albero che consente di realizzare l’algoritmo in modo efficiente dal punto di vista computazionale.\nL’algoritmo è implementato dalla funzione Matlab detectMSERFeatures. In figura è illustrato il risultato dell’applicazione dell’algoritmo al fantoccio prima considerato.\n\nMETTERE FIGURA QUI","type":"content","url":"/c31-segmentazione#algoritmo-msers","position":37},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Component Tree","lvl2":"Watershed transform"},"type":"lvl3","url":"/c31-segmentazione#component-tree","position":38},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Component Tree","lvl2":"Watershed transform"},"content":"L’approccio Component Tree come detto in precedenza permette di memorizzate il contenuto di una immagine in un albero, permettendo di realizzare algoritmi di elaborazione dell’immagine in modo efficiente. Si tratta di un approccio molto usato e vale quindi la pena di esaminarlo in maggior dettaglio.\nConsideriamo una immagine  a quattro livelli di grigio come quella in figura. L’immagine contiene 8 o 9 pattern a seconda del tipo di connessione considerata, per un kernel di connessione a 8 elementi contiamo 8 pattern distinti. Al passo uno dell’algoritmo viene selezionata la componente a minima intensità (1) c1, che va a costituire la radice dell’albero e rappresenta il background. Al passo due, alla componente uno vengono aggiunte due componenti topologicamente distinte c2 e c3, con livello di grigio 2, che diventano due rami dell’albero. Al passo tre vengono aggiunte tre componenti (c4, c5, c6), con livello di grigio 3, delle quali c6 è contigua a c3 e quindi va nel ramo corrispondente dell’albero, mentre le altre due nell’altro essendo contigue a c2. Al passo quattro l’albero si completa con le componenti c7 e c8 relative al livello gi grigio 4. In generale i livelli dell’albero saranno pari al numero di livelli di grigio presenti nell’immagine ed il numero di nodi sarà uguale al numero di pattern presenti nell’immagine.\n\nMETTERE FIGURA QUI\n\nIn ogni nodo sono memorizzati i pixel appartenenti al nodo. Il peso delle connessioni (edge) tra nodi è dato dalla differenza di livello di grigio tra i livelli (in questo caso uno per tutte le connessioni).\n\nMETTERE FIGURA QUI\n\nUna volta effettuato il processo di conversione da immagine ad albero connesso, le operazioni di filtraggio e segmentazione possono essere effettuate sull’albero invece che sull’immagine. Ad esempio se selezioniamo i due rami principali dell’albero attraverso una funzione di ricerca depth-first search, otteniamo i due “macro-pattern” in cui è divisa l’immagine, come illustrato in figura.\nUna segmentazione a soglia si ottiene immediatamente selezionando tutti i nodi sopra un certo livello. Il region-growing è equivalente a partire da un nodo e muoversi sull’albero con la condizione che il peso di una connessione deve essere minore uguale alla soglia di tolleranza dell’algoritmo. In generale molti algoritmi di filtraggio e segmentazione possono essere implementati utilizzando in modo efficace la rappresentazione ad albero connesso.","type":"content","url":"/c31-segmentazione#component-tree","position":39},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Skeletonization"},"type":"lvl2","url":"/c31-segmentazione#skeletonization","position":40},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Skeletonization"},"content":"Adattato da HIPR (\n\nhttp://​homepages​.inf​.ed​.ac​.uk​/rbf​/HIPR2​/hipr​_top​.htm)\nNell’elaborazione di immagini biomediche, una volta definita la maschera che definisce una certa struttura anatomica, può essere utile caratterizzare la struttura da un punto di vista geometrico, estraendone i caratteri fondamentali, quali la connettività, la topologia, la lunghezza, la direzione e l’ampiezza. Questo può essere ottenuto attraverso l’estrazione del cosiddetto scheletro topologico (skeleton). Intuitivamente, lo scheletro topologico di una maschera si ottiene “assottigliando” la maschera stessa fino ad ottenerne una versione essenziale, tipicamente composta da linee.\n\nMETTERE FIGURA QUI\n\nIn figura è mostrato lo scheletro topologico dell’albero bronchiale estratto da una maschera dell’albero stesso ottenuta da immagini TAC.\nIn questo caso lo scheletro estratto può essere utile in varie applicazioni. Ad esempio lo scheletro, rappresentando la linea di equidistanza dalle pareti del bronco (center line) può essere usata come guida per applicazioni di endoscopia virtuale. Dallo scheletro è possibile individuare facilmente le biforcazioni dell’albero bronchiale, permettendo la costruzione di un albero binario che permette di confrontare il paziente in esami eseguiti a tempi diversi o pazienti diversi tra loro.\n\nL’operazione di skeletonization può essere eseguita in vari modi. I due approcci fondamentali sono il thinning (assottigliamento) morfologico e il calcolo della distance transform dell’immagine.\nL’operazione di thinning si basa sull’idea di eliminare i pixel di confine della maschera in modo progressivo (erosione) riducendone le dimensioni fino a quando non è più possibile assottigliare ulteriormente la maschera e si ottiene lo scheletro topologico.\nPer formulare in termini algoritmici la funzione di thinning è utile introdurre la hit-and-miss transform, (HAM) che, come avviene nei filtri spaziali, è basata sulla convoluzione della maschera con un kernel (structuring element). Il kernel contiene la descrizione della struttura geometrica che si vuole individuare. La struttura del kernel (supponendo di voler individuare un angolo della maschera) sarà del tipo in figura, dove 0 indica un pixel appartenente allo sfondo della maschera, 1 un pixel appartenente alla maschera e le caselle bianche indicano locazioni non di interesse. In altri termini le caselle vuote vengono ignorate ed il loro scopo è ottenere un kernel quadrato più comodo da usare a fini computazionali.\nAnalogamente alla convoluzione spaziale, il kernel viene fatto scorrere sulla maschera e se esiste una totale corrispondenza tra il kernel e la maschera sottostante il pixel della maschera risultante viene posto ad uno, altrimenti viene lasciato a zero.\nTipicamente vengono generate una serie di versioni del kernel che corrispondono alle possibili orientazioni della struttura che interessa determinare, ad esempio nel caso in oggetto potremmo avere i quattro kernel:\n\nMETTERE FIGURA QUI\n\nChe corrispondono alla ricerca di un angolo nella maschera nelle quattro orientazioni possibili. L’ OR logico delle quattro maschere risultanti permette di identificare gli angoli sulla maschera.\n\nMETTERE FIGURA QUI\n\nUn esempio di uso della hit-and-miss transform è il rilevamento dei punti isolati di una maschera, che può essere utile nella correzione degli errori di segmentazione. In questo caso il kernel da utilizzare risulta:\n\nMETTERE KERNEL QUI\n\nE si ottiene una maschera che definisce i punti isolati della maschera di input.\nLa trasformazione HAM è implementata in MATLAB dalla funzione bwhitmiss.\nNel caso del thinning si può usare una coppia di kernel del tipo:\n\nMETTERE KERNEL QUI\n\nL’applicazione di questi due kernel equivale a conservare i pixel che sono al centro di un ottagono che è totalmente incluso nella maschera. I kernel vengono ruotati nelle quattro direzioni possibili ottenendo quindi 8 trasformazioni hit-and-miss. L’operazione di thinning si ottiene calcolando:\n\nMthin = M – MHAM\n\nIn pratica il risultato della trasformazione HAM viene sottratto alla maschera originale, che quindi si ”assottiglia”. Per ottenere lo skeleton l’operazione di thinning viene iterata fino a quando la maschera originale non si modifica ulteriormente.Se consideriamo la maschera originale a sinistra l’applicazione dell’operazione di thinning porta alla maschera al centro, l’effetto del thinning è visibile nell’immagine a destra che rappresenta la differenza tra le due maschere.\n\nMETTERE FIGURA QUI\n\nCome si osserva la parte più esterna della maschera originale viene asportata. Applicando 10 volte il thinning si ottiene:\n\nMETTERE FIGURA QUI\n\nEd infine per N grande (ad esempio N=100) la procedura iterativa conduce allo skeleton:\n\nMETTERE FIGURA QUI\n\nIn MATLAB l’operazione prima descritta di skeletonization è implementata direttamente nella funzione bwmorph attraverso l’opzione ‘skel’.\n\nMETTERE FIGURA QUI\n\nIl calcolo della distance transform dell’immagine prevede invece che per ogni punto della maschera di input venga calcolata la distanza minima dal contorno della maschera stessa (o dallo sfondo della maschera). Di solito si utilizza la distanza City-block o `chessboard’ che vale il numero di pixel che bisogna attraversare per andare da un pixel all’altro dell’immagine (nella City-block non si possono usare le diagonali). Si ottiene quindi una immagine di dimensioni uguali all’originale e a cui a ciascun pixel corrisponde un livello di grigio pari alla minima distanza dai bordi. I pixel con valore più alto rappresentano lo skeleton in quando sono i più “centrali”. Utilizzando una soglia si può estrarre lo skeleton vero e proprio.In MATLAB la distance transform viene computata con la funzione bwdist che permette di definire il tipo di distanza da utilizzare. La funziona computa la distanza da un pixel non nullo, quindi per ottenere lo skeleton va invertita la maschera.\n\nMETTERE FIGURA QUI","type":"content","url":"/c31-segmentazione#skeletonization","position":41},{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/connected-tree-example","position":0},{"hierarchy":{"lvl1":""},"content":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport networkx as nx\n\n\n\n\n\n# ----------------------------\n# 1) Build the 7x7 image + map\n# ----------------------------\nimage = np.ones((7, 7), dtype=int)  # background = 1\nmp = np.zeros((7, 7), dtype=int)    # pattern map\n\n# MATLAB is 1-based; Python is 0-based -> subtract 1 from indices\n\n# level 2 patterns\nimage[1:3, 3] = 2\nmp[1:3, 3] = 2\n\nimage[5, 3:6] = 2\nimage[4, 3] = 2\nmp[5, 3:6] = 3\nmp[4, 3] = 3\n\n# level 3 patterns\nimage[1:3, 1:3] = 3\nmp[1:3, 1:3] = 4\n\nimage[1:3, 4] = 3\nimage[3, 5] = 3\nmp[1:3, 4] = 5\nmp[3, 5] = 5\n\nimage[4, 1:3] = 3\nimage[5, 2] = 3\nmp[4, 1:3] = 6\nmp[5, 2] = 6\n\n# level 4 patterns\nimage[1:3, 5] = 4\nmp[1:3, 5] = 7\n\nimage[5, 1] = 4\nmp[5, 1] = 8\n\n# background pixels -> map = 1 (like MATLAB)\nmp[image == 1] = 1\n\n# visualize image + map\nfig, ax = plt.subplots(1, 2, figsize=(10, 4))\nax[0].imshow(image, cmap=\"gray\", interpolation=\"nearest\")\nax[0].set_title(\"image\")\nax[0].axis(\"image\")\n\nax[1].imshow(mp, cmap=\"jet\", interpolation=\"nearest\")\nax[1].set_title(\"map\")\nax[1].axis(\"equal\")\nplt.tight_layout()\nplt.show()\n\n# -----------------------------------------\n# 2) Build NodeTable (names, levels, pixels)\n# -----------------------------------------\nnames = [f\"c{i}\" for i in range(1, 9)]\nlevels = [1, 2, 2, 3, 3, 3, 4, 4]\n\n# MATLAB 'find' returns linear indices (column-major).\n# We'll store BOTH:\n# - MATLAB-style linear indices (1..49, col-major)\n# - Python linear indices (0..48, row-major), if you ever need them\ndef matlab_linear_indices(mask_2d: np.ndarray) -> np.ndarray:\n    # mask_2d is boolean shape (7,7)\n    r, c = np.where(mask_2d)\n    # MATLAB linear index (1-based), column-major:\n    return (r + 1) + (c) * mask_2d.shape[0]\n\npixels_matlab = []\nfor i in range(1, 9):\n    idx = matlab_linear_indices(mp == i).tolist()\n    pixels_matlab.append(idx)\n\nnode_table = pd.DataFrame({\n    \"Name\": names,\n    \"Level\": levels,\n    \"Pixels\": pixels_matlab\n})\nprint(\"\\nNodeTable:\")\nprint(node_table)\n\n# ----------------------------\n# 3) Build directed graph (A)\n# ----------------------------\nA = np.zeros((8, 8), dtype=int)\nA[0, 1] = 1  # c1->c2\nA[0, 2] = 1  # c1->c3\nA[1, 3] = 1  # c2->c4\nA[1, 4] = 1  # c2->c5\nA[4, 6] = 1  # c5->c7\nA[2, 5] = 1  # c3->c6\nA[5, 7] = 1  # c6->c8\n\nG = nx.DiGraph()\n\n# add nodes with attributes (like NodeTable)\nfor i in range(8):\n    G.add_node(i+1, Name=names[i], Level=levels[i], Pixels=pixels_matlab[i])  # node ids 1..8\n\n# add edges from adjacency matrix\nfor i in range(8):\n    for j in range(8):\n        if A[i, j] == 1:\n            G.add_edge(i+1, j+1)\n\n# plot graph\nplt.figure(figsize=(6, 4))\npos = nx.spring_layout(G, seed=1)\nnx.draw(G, pos, with_labels=True, node_size=1200, arrows=True)\nnx.draw_networkx_labels(G, pos, labels={n: G.nodes[n][\"Name\"] for n in G.nodes})\nplt.title(\"Connected tree (digraph)\")\nplt.tight_layout()\nplt.show()\n\n# ----------------------------\n# 4) DFS order + split branches\n# ----------------------------\nv = list(nx.dfs_preorder_nodes(G, source=1))  # like dfsearch(G,1)\nlev = np.array([G.nodes[n][\"Level\"] for n in v])\n\ndiff_lev = np.diff(lev)\nprint(\"\\n[v, level, diff(level)]\")\nprint(np.column_stack([v, lev, np.r_[diff_lev, 0]]))\n\n# find where the level drops (diff < 0), like MATLAB\ndrop_idx = np.where(diff_lev < 0)[0]\nprint(\"\\nIndices where diff(level) < 0:\", drop_idx)\n\n# In your MATLAB, you used v(2:id) and v(id+1:end).\n# Here we mirror that using the first drop.\nif len(drop_idx) == 0:\n    raise RuntimeError(\"No level drop found; cannot split into two main branches.\")\nid0 = drop_idx[0]  # index in diff -> split point in v\n\n# note: MATLAB starts from v(2), skipping root\nMpattern1 = v[1:id0+1]      # inclusive up to id0\nMpattern2 = v[id0+1:]       # rest\n\nprint(\"\\nMpattern1:\", Mpattern1, [G.nodes[n][\"Name\"] for n in Mpattern1])\nprint(\"Mpattern2:\", Mpattern2, [G.nodes[n][\"Name\"] for n in Mpattern2])\n\n# collect pixel lists (MATLAB-style linear indices)\nMp1_pixels = [p for n in Mpattern1 for p in G.nodes[n][\"Pixels\"]]\nMp2_pixels = [p for n in Mpattern2 for p in G.nodes[n][\"Pixels\"]]\n\n# ---------------------------------------\n# 5) Build map1: 1 for branch1, 2 for branch2\n# ---------------------------------------\nmap1 = np.zeros((7, 7), dtype=int)\n\ndef matlab_lin_to_rc(lin: int, nrows: int) -> tuple[int, int]:\n    # lin is 1-based column-major\n    lin0 = lin - 1\n    r = lin0 % nrows\n    c = lin0 // nrows\n    return r, c\n\nfor lin in Mp1_pixels:\n    r, c = matlab_lin_to_rc(lin, 7)\n    map1[r, c] = 1\n\nfor lin in Mp2_pixels:\n    r, c = matlab_lin_to_rc(lin, 7)\n    map1[r, c] = 2\n\nplt.figure(figsize=(4, 4))\nplt.imshow(map1, cmap=\"jet\", interpolation=\"nearest\")\nplt.title(\"map1 (branch1=1, branch2=2)\")\nplt.axis(\"equal\")\nplt.tight_layout()\nplt.show()\n\n# ----------------------------\n# 6) Shortest path 2 -> 7\n# ----------------------------\nTR_nodes = nx.shortest_path(G, source=2, target=7)\nprint(\"\\nShortest path 2 -> 7:\", TR_nodes, [G.nodes[n][\"Name\"] for n in TR_nodes])\n\nTR = G.subgraph(TR_nodes).copy()\n\nplt.figure(figsize=(6, 4))\npos2 = nx.spring_layout(TR, seed=1)\nnx.draw(TR, pos2, with_labels=True, node_size=1200, arrows=True)\nnx.draw_networkx_labels(TR, pos2, labels={n: G.nodes[n][\"Name\"] for n in TR.nodes})\nplt.title(\"TR (shortest path subgraph)\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/connected-tree-example","position":1},{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/esempio-em-gmm","position":0},{"hierarchy":{"lvl1":""},"content":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import k_means\nfrom sklearn.mixture import GaussianMixture\n\n\n\ndim = 512\ntissueMask = np.zeros((dim, dim), dtype = np.float32)\ntissueMask[:,:] = 1\ntissueMask[49:189,49:189] = 2\ntissueMask[199:499,199:349] = 3\n\nfig, ax = plt.subplots()\nax.imshow(tissueMask, cmap = \"gray\")\nax.set_title('image')\n\n# create 3 guassian distributions\nm1, m2, m3 = 50, 400, 200\nsd1, sd2, sd3 = 60, 40, 20\n\ndata1 = m1+sd1*np.random.randn(dim, dim) # signal distribution\ndata2 = m2+sd2*np.random.randn(dim, dim)\ndata3 = m3+sd3*np.random.randn(dim, dim)\nid1 = np.where(tissueMask == 1)\nid2 = np.where(tissueMask == 2)\nid3 = np.where(tissueMask == 3)\n\nimage = np.zeros_like(tissueMask)\nimage[id1] = data1[id1]\nimage[id2] = data2[id2]\nimage[id3] = data3[id3]\n\nfig, ax = plt.subplots()\nax.imshow(image, cmap = \"gray\")\nax.set_title(\"image\")\n\n# probability of a tissue in the image\np1 = len(id1)/(dim*dim)\np2 = len(id2)/(dim*dim)\np3 = len(id3)/(dim*dim)\n\n# calculate histogram\nfig, ax = plt.subplots(1,2)\ncounts, bins = np.histogram(image, bins = 256)\nxcounts = bins[:-1] + np.diff(bins) / 2\nax[0].plot(xcounts,counts)\n\n# apply gaussian mixtures using kmeans as initial condition\nimage = image.reshape((-1,1))\ngmm = GaussianMixture(\n    n_components = 3, \n    max_iter = 100, \n    init_params = 'kmeans', \n    verbose = 1)\ngmm.fit(image)\n\nn_samples = len(image)\n\ndataGM1 = gmm.means_[0]+np.sqrt(gmm.covariances_[0])*np.random.randn(int(gmm.weights_[0] * n_samples)).flatten()\ndataGM2 = gmm.means_[1]+np.sqrt(gmm.covariances_[1])*np.random.randn(int(gmm.weights_[1] * n_samples)).flatten()\ndataGM3 = gmm.means_[2]+np.sqrt(gmm.covariances_[2])*np.random.randn(int(gmm.weights_[2] * n_samples)).flatten()\ndataGM = np.concatenate([dataGM1.T, dataGM2.T, dataGM3.T])\n\nhGM, edgesGM = np.histogram(dataGM, bins = 256)\nxGM = edgesGM[:-1] + np.diff(edgesGM) / 2\nax[1].plot(xcounts,counts)\nax[1].plot(xGM,hGM, color='r')\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/esempio-em-gmm","position":1},{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/esempio-fcm","position":0},{"hierarchy":{"lvl1":""},"content":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom pyclustering.cluster.center_initializer import kmeans_plusplus_initializer\nfrom pyclustering.cluster.fcm import fcm\n\n\n\nimage = np.zeros((512,512), dtype = np.float32)\nimage[:,:] = 20\nimage[49:99,49:99] = 150\nimage[100:179,100:449] = 300\nimage[199:499,199:349] = 70\nimage[229:269,229:269] = 750\nimage[4:399,449:499] = 550\n\nsigma = 10\n\nimageN = image + sigma*np.random.randn(512, 512)\n\nfig, ax = plt.subplots()\nax.imshow(imageN, cmap = \"gray\")\n\nimageN = imageN.reshape((-1,1))\n# initialize centers\n# initial_centers = kmeans_plusplus_initializer(imageN, 6, kmeans_plusplus_initializer.FARTHEST_CENTER_CANDIDATE).initialize()\ninitial_centers = kmeans_plusplus_initializer(imageN, 6, kmeans_plusplus_initializer.FARTHEST_CENTER_CANDIDATE).initialize()\n\n# create instance of Fuzzy C-Means algotirhm\nfcm_instance = fcm(imageN, initial_centers)\n\n# run cluster analysis and obtain results\nfcm_instance.process()\nclusters = fcm_instance.get_clusters()\ncenters = fcm_instance.get_centers()\n\nmask1 = np.zeros_like(imageN)\nmask1[clusters[0]]=1\nmask1 = mask1.reshape(image.shape)\nmask2 = np.zeros_like(imageN)\nmask2[clusters[1]]=1\nmask2 = mask2.reshape(image.shape)\nmask3 = np.zeros_like(imageN)\nmask3[clusters[2]]=1\nmask3 = mask3.reshape(image.shape)\nmask4 = np.zeros_like(imageN)\nmask4[clusters[3]]=1\nmask4 = mask4.reshape(image.shape)\nmask5 = np.zeros_like(imageN)\nmask5[clusters[4]]=1\nmask5 = mask5.reshape(image.shape)\nmask6 = np.zeros_like(imageN)\nmask6[clusters[5]]=1\nmask6 = mask6.reshape(image.shape)\nfig, ax = plt.subplots(1,6)\nax[0].imshow(mask1)\nax[1].imshow(mask2)\nax[2].imshow(mask3)\nax[3].imshow(mask4)\nax[4].imshow(mask5)\nax[5].imshow(mask6)\nax[0].set_title('fcm - cluster 1')\nax[1].set_title('fcm - cluster 2')\nax[2].set_title('fcm - cluster 3')\nax[3].set_title('fcm - cluster 4')\nax[4].set_title('fcm - cluster 5')\nax[5].set_title('fcm - cluster 6')\nplt.show()\n\n\n\n\n\n\n\n","type":"content","url":"/esempio-fcm","position":1},{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/hierarchical-clustering-example","position":0},{"hierarchy":{"lvl1":""},"content":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy.cluster.hierarchy import linkage, dendrogram\n\n\n\n\n# create images with different noise levels\ndim = 256\nimage = np.zeros((dim, dim), dtype =np.float32)\nimage[:,:]=20\nimage[99:199,99:199] = 200\nN = 10\nx = np.zeros((N, dim, dim), dtype = np.float32)\nnoise = np.random.permutation(N)**2\n\nfig, ax = plt.subplots(1, 10)\nfor i in range(N):\n    x[i,:,:] = image + noise[i]*np.random.randn(dim,dim)\n    ax[i].imshow(x[i,:,:], cmap=\"gray\")\n    \nxd = x.reshape(N, dim*dim)\nd = pdist(xd)\nz = squareform(d)\n\n# single linkage\nt = linkage(z, method = \"single\")\nfig, ax = plt.subplots()\ndendrogram(t)\nplt.show()\n\n# average linkage\ntav = linkage(z, method = \"average\")\nfig, ax = plt.subplots()\ndendrogram(tav)\nplt.show()\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/hierarchical-clustering-example","position":1},{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/mser-example","position":0},{"hierarchy":{"lvl1":""},"content":"# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon Sep 16 15:11:20 2024\n\n@author: alejandro\n\"\"\"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport pydicom\nfrom scipy.ndimage import label\nimport time\n\n# read file\ndb = pydicom.dcmread('../data/phantom.dcm')\nimage = db.pixel_array\n\nfig, ax = plt.subplots(1, 2)\nax[0].imshow(image, cmap = 'gray')\nax[1].plot(image[np.round(db.Rows/2).astype(int),:])\n\n# simplified MSER algorithm\npixel_list= np.sort(image.reshape(-1,1))\ntmp = np.zeros_like(image)\n\nG = np.unique(pixel_list)\n\nn_labels = np.zeros_like(G)\nlabels = np.zeros((len(G), *image.shape))\nfig, ax = plt.subplots(1,2)\nfor g in range(len(G)):\n    ax[0].cla()\n    ax[1].cla()\n    idx = np.where(image == G[g])\n    tmp[idx] = 1\n    l, num = label(tmp, structure=np.ones((3, 3))) \n    labels[g] = l\n    n_labels[g] = num\n    \n    ax[0].imshow(l)\n    ax[1].plot(np.arange(len(G)), np.log(n_labels +1))\n    # drawing updated values\n    fig.canvas.draw()\n    \n    # This will run the GUI event\n    # loop until all UI events\n    # currently waiting have been processed\n    fig.canvas.flush_events()\n\n    time.sleep(0.001)\n    \n    # show the figure\n    fig.show()\n    \n# follow background area\n# fig, ax = plt.subplots()\nmaps = np.zeros((len(G), *image.shape))\nareaBK = np.zeros(len(G))\n\nfor g in range(len(G)):\n    current_lab = np.squeeze(labels[g,:,:])\n    area = np.zeros(n_labels[g])\n    for n in range(n_labels[g]):\n        idx = np.where(current_lab == (n+1))\n        area[n] = len(idx[0])\n    idmax = np.argmax(area)\n    id1 = np.where(current_lab == (idmax+1))\n    areaBK[g] = len(id1[0])\n    maps[g][idx] = 255\n    \n    # ax.imshow(np.squeeze(maps[g,:,:]))\n    # # drawing updated values\n    # fig.canvas.draw()\n    # fig.canvas.flush_events()\n    # time.sleep(0.001)\n    \n    # fig.show()\n    \nfig, ax = plt.subplots(1, 2)\nax[0].plot(areaBK)\nvariazione = 50*np.diff(areaBK)\nax[1].plot(variazione)\n\nidx = np.where(variazione == 0)\n\n\nmser = cv2.MSER_create()\nregionsc= mser.detectRegions(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY))\ndisplay()\n\n\n\n\n\n\n\n","type":"content","url":"/mser-example","position":1},{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/seg-contorni","position":0},{"hierarchy":{"lvl1":""},"content":"import pydicom\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom skimage import filters, measure\n\n\n\n# load image\nds = pydicom.dcmread(\"../data/phantom.dcm\")\nimage = ds.pixel_array\nprint(ds)\n\n# display phantom\ntmp = image.copy()\ntmp[127,:] = 1000\nfig, axes = plt.subplots(1,2)\naxes[0].imshow(tmp, cmap = \"gray\")\n\n# display profile and profile derivative\nprofile = image[127,:]\nderiv = np.diff(profile)\naxes[1].plot(profile)\naxes[1].plot(deriv)\nplt.show()\n\n# find tissue changes\ntmp = image.copy()\nT = 100 # derivative threshold\nidx = np.argwhere(np.abs(deriv) > T) \ncont = 255*np.ones(len(profile))\nx = np.arange(0,len(profile))\ncont = cont[idx]\nx = x[idx]\ntmp[127,idx]=2000\nfig, axes = plt.subplots(1,2)\naxes[0].imshow(tmp, cmap = \"gray\")\naxes[1].plot(profile)\naxes[1].plot(np.abs(deriv))\naxes[1].scatter(x, cont, marker = \"*\", color = \"red\")\nplt.show()\n\n# working in 2d space by edge function\nfig, axes = plt.subplots()\nedge_map = filters.sobel(image)>0.005\nplt.imshow(edge_map, cmap = \"gray\")\nplt.show()\n\n# create contours from edge map\nfig, axes = plt.subplots()\nlabeled_image = measure.label(edge_map, connectivity=2) \nplt.imshow(labeled_image)\nplt.show()\n\nfig, axes = plt.subplots()\nidx = np.where(labeled_image == 1)\nmask = np.zeros(image.shape)\nmask[idx]=1\nplt.imshow(mask, cmap = \"gray\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/seg-contorni","position":1},{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/seg-soglia","position":0},{"hierarchy":{"lvl1":""},"content":"\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage.filters import threshold_otsu\nfrom sklearn.cluster import k_means\nimport pydicom\nfrom pyclustering.cluster.center_initializer import kmeans_plusplus_initializer\nfrom pyclustering.cluster.fcm import fcm\n\n\n\n# load image\nds = pydicom.dcmread('../data/phantom.dcm')\nimage = ds.pixel_array\nprint(ds)\n\n# get histogram\np = np.max(image, None)\nprint(p)\ncounts, bins = np.histogram(image, bins = p)\n\n# display phantom image and image histogram\nfig, ax = plt.subplots(1,2)\nax[0].imshow(image, cmap = \"gray\")\nax[1].plot(counts[1:]/np.sum(counts, None))\nax[0].set_title('image')\nax[1].set_title('histogram')\nplt.show()\n\n# manual segmentation of water signal (dall'isto sopra 200 e sotto 600)\nmask = np.zeros((ds.Rows, ds.Columns), None)\nprint(mask.shape)\n\nid = np.logical_and(image > 200, image<600)\nmask[id] = 255\nplt.figure()\nplt.imshow(mask, cmap = \"gray\")\nplt.title('manual')\nplt.show()\n\n# otsu algorithm\notsu_thresh = threshold_otsu(image)\nimage_otsu = image > otsu_thresh\nplt.figure()\nplt.imshow(image_otsu, cmap = 'gray')\nplt.title('otsu')\nplt.show()\n\n#### kmeans ####\nx = image.reshape((-1,1))\ncentroid, label, inertia = k_means(x, n_clusters = 3)\nimage_classes = label.reshape(image.shape)\nplt.figure()\nplt.imshow(image_classes, cmap = 'gray')\nplt.title('kmeans')\nplt.show()\nc = np.sort(centroid, axis = 0)\nT = np.array([np.mean(c[0:2]), np.mean(c[1:3])])\nprint(\"soglie = \", T)\n\n#### fuzzy cmeans ####\n# initialize centers\ninitial_centers = kmeans_plusplus_initializer(x, 3, kmeans_plusplus_initializer.FARTHEST_CENTER_CANDIDATE).initialize()\n\n# create instance of Fuzzy C-Means algotirhm\nfcm_instance = fcm(x, initial_centers)\n\n# run cluster analysis and obtain results\nfcm_instance.process()\nclusters = fcm_instance.get_clusters()\ncenters = fcm_instance.get_centers()\n\nmask1 = np.zeros_like(x)\nmask1[clusters[0]]=1\nmask1 = mask1.reshape(image.shape)\nmask2 = np.zeros_like(x)\nmask2[clusters[1]]=1\nmask2 = mask2.reshape(image.shape)\nmask3 = np.zeros_like(x)\nmask3[clusters[2]]=1\nmask3 = mask3.reshape(image.shape)\nfig, ax = plt.subplots(1,3)\nax[0].imshow(mask1)\nax[1].imshow(mask2)\nax[2].imshow(mask3)\nax[0].set_title('fcm - cluster 1')\nax[1].set_title('fcm - cluster 2')\nax[2].set_title('fcm - cluster 3')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/seg-soglia","position":1},{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/watershed-example","position":0},{"hierarchy":{"lvl1":""},"content":"\n\nimport numpy as np\nimport matplotlib\nmatplotlib.use(\"QtAgg\")  # keep if you need interactive window in VS Code\n\nimport matplotlib.pyplot as plt\nimport pydicom\nfrom skimage.segmentation import flood\n\nI = pydicom.dcmread('../data/phantom.dcm').pixel_array.astype(np.float32)\ndim = I.shape[0]\nmaxImage = int(I.max())\n\narea = np.zeros(maxImage + 1, dtype=np.int64)\nprofile = I[:, dim//2]\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\nplt.ion()\nplt.show(block=False)\n\n# --- (0) overlay panel: create once ---\naxes[0].set_title(\"Threshold Overlay\")\nim_base = axes[0].imshow(I, cmap='gray')\noverlay = np.zeros((dim, dim, 4), dtype=np.float32)   # RGBA\noverlay[..., 0] = 1.0                                 # red\noverlay[..., 3] = 0.0                                 # alpha updated each iter\nim_overlay = axes[0].imshow(overlay)\naxes[0].axis(\"off\")\n\n# --- (1) area curve: create once ---\naxes[1].set_title(\"Segmented Area vs Threshold\")\naxes[1].set_xlim(0, maxImage)\naxes[1].set_ylim(0, I.size)\nline_area, = axes[1].plot([], [], 'r', linewidth=2.0)\n\n# --- (2) profile + threshold line: create once ---\naxes[2].set_title(\"Profile with Threshold\")\naxes[2].set_xlim(0, dim-1)\naxes[2].set_ylim(profile.min(), profile.max())\naxes[2].plot(profile, 'k')\nhline = axes[2].axhline(0, color='r', alpha=0.2)\n\n# Update frequency (visualization only)\nstep = 5   # show every 5 thresholds (set 1 for every frame)\n\nfor th in range(1, maxImage):\n    BW = flood(I, (4, 4), tolerance=th)\n    area[th] = BW.sum()\n\n    # only refresh the figure sometimes (big speedup)\n    if th % step != 0:\n        continue\n\n    overlay[..., 3] = BW.astype(np.float32) * 0.30\n    im_overlay.set_data(overlay)\n\n    line_area.set_data(np.arange(1, th+1), area[1:th+1])\n    hline.set_ydata([th, th])\n\n    fig.canvas.draw_idle()\n    plt.pause(0.1)\n\nplt.ioff()\nplt.show()\ndisplay()\n\n\n","type":"content","url":"/watershed-example","position":1},{"hierarchy":{"lvl1":"Elaborazione delle Bioimmagini"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Elaborazione delle Bioimmagini"},"content":"Benvenuto! Usa il menu a sinistra per navigare i capitoli.","type":"content","url":"/","position":1}]}