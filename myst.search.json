{"version":"1","records":[{"hierarchy":{"lvl1":"Contatti"},"type":"lvl1","url":"/c10-info","position":0},{"hierarchy":{"lvl1":"Contatti"},"content":"Ing. Vincenzo Positano Dr. Alejandro Luis Callara\n\ncontact: positano [at] ftgm [dot] italejandro [dot] callara [at] unipi [dot] it","type":"content","url":"/c10-info","position":1},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità"},"type":"lvl1","url":"/c11-imm-biom","position":0},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità"},"content":"","type":"content","url":"/c11-imm-biom","position":1},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl2":"Dal Post-processing all’analisi dell’immagine"},"type":"lvl2","url":"/c11-imm-biom#dal-post-processing-allanalisi-dellimmagine","position":2},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl2":"Dal Post-processing all’analisi dell’immagine"},"content":"Nell’imaging biomedico, storicamente le immagini vengono prodotte dal dispositivo di acquisizione avendo come obiettivo l’analisi visiva da parte del radiologo o medico esperto. L’analisi visiva viene effettuata tradizionalmente attraverso la stampa radiografica e la visualizzazione su diafanoscopio. Oggi più comunemente le immagini vengono visualizzate su schermo, sia per ragioni di costo (la stampa radiografica è estremamente costosa) che di praticità (un moderno esame TAC o MRI può contenere centinaia di immagini). L’analisi visiva da parte dell’operatore esperto conduce alla compilazione di un referto diagnostico. L’analisi visiva è intrinsecamente qualitativa, cioè di tipo descrittivo. Nell’ipotesi di un referto relativo ad un esame MRI cardiovascolare, descrizioni di tipo qualitativo potranno essere del tipo “Ventricolo sinistro di normali dimensioni”, “Funzione cardiaca ridotta”, etc. In realtà nella pratica radiologica viene adottato un criterio semi-quantitativo, secondo il quale i vari livelli di una patologia vengono identificati per classi (ad esempio severo, medio, moderato, lieve, assente). Al medico refertante viene richiesto di associare quanto visualizzato dall’immagine diagnostica ad una classe di patologia. Troveremo quindi nel referto espressioni del tipo “buona cinesi segmentaria”, “severo accumulo di ferro”, etc. Dove le locuzioni “buona”, “severo” non indicano un giudizio puramente qualitativo, ma la classificazione semi-quantitativa su di una opportuna scala. L’analisi qualitativa e semi-quantitativa non richiedono in generale l’uso di algoritmi di elaborazione dell’immagine, se si esclude il possibile uso di algoritmi di filtraggio che hanno lo scopo di ottimizzare la visione dell’immagine stessa.\n\nIl passaggio ad un referto di tipo quantitativo comporta invece l’utilizzo di algoritmi di elaborazione atti a estrarre dall’immagine degli indici numerici e quindi quantitativi. Ad esempio l’uso di un programma di segmentazione delle cavità ventricolari permetterà di ottenere una misura quantitativa dei volumi ventricolari destro e sinistro, misure che potranno essere riportate a referto, ad esempio come: “VTD 66 ml/m2”, dove il Volume Telediastolico è riportato in ml normalizzato alla superficie corporea in m2. I vantaggi di una misura di tipo quantitativo sono la superiore oggettività rispetto all’analisi qualitativa o semi-quantitativa e la maggior precisione rispetto ad un sistema discreto a classi. Notiamo comunque che tipicamente nel referto la classificazione a classi viene comunque conservata, cioè le informazioni quantitative vengono “tradotte” in classi di gravità della patologia. Questo ha lo scopo di rendere il referto più immediatamente comprensibile dal medico richiedente il referto stesso che non è in generale esperto della particolare metodica di imaging utilizzata e ha quindi bisogno della traduzione dell’indice qualitativo in una classe di merito a lui nota. Le soglie di conversione vengono solitamente definite all’interno di linee guida sviluppate dalle varie società scientifiche. In Figura 1.1 è riportato un esempio di referto da immagini MRI.\n\nFigura 1.1. Esempio di referto radiologico (MRI cardiaca per lo studio dell’accumulo di ferro).\n\nA questo livello l’analisi visiva e l’elaborazione dell’immagine vengono effettuate in parallelo, quindi gli stessi insiemi di immagini vengono valutate visivamente ed elaborate al calcolatore. Nella pratica clinica odierna, tuttavia, iniziano ad essere utilizzate procedure di acquisizione destinate a produrre immagini inutili dal punto di vista dell’esame visivo e destinate in modo univoco all’elaborazione. Esempi tipici sono le immagini PC (Phase Contrast) in risonanza per la valutazione algoritmica della velocità di flusso e le immagini T1/T2/T2* multiecho per la caratterizzazione quantitativa dei tessuti in MR. In questi casi la parte di elaborazione cessa di essere un “add-on” del processo diagnostico, e ne diviene parte integrante e insostituibile. Nell’esempio di referto in figura troviamo un esempio di questo tipo nella definizione dell’accumulo di ferro attraverso la misura quantitativa del valore di T2* sul miocardio e sul parenchima del fegato. In questo caso il referto è puramente quantitativo, nel senso che l’analisi visiva non è rilevante. Nella terminologia moderna il termine “elaborazione delle bioimmagini” viene preferibilmente sostituito con il termine “analisi dell’immagine medica” (medical image analysis), proprio per sottolineare la preminenza dell’analisi software su quella visuale.\n\nÈ importante notare come in questo caso l’elaborazione dell’immagine biomedica da “ausilio al processo diagnostico” diviene “parte strutturale del processo diagnostico” e l’affidabilità del processo di elaborazione deve essere equivalente a quella di un dispositivo medico vero e proprio. Il decreto legislativo 37 del 25/01/2010 recepisce in Italia la direttiva europea 93/42/CEE sulla certificazione dei dispositivi medici, chiarendo che il software che implementa algoritmi di elaborazione di immagini se usato a fini diagnostici è da considerare un “dispositivo medico” e come tale richiede una opportuna certificazione. Sostanzialmente il software per l’analisi dell’immagine biomedica è equiparato ad un dispositivo di acquisizione o altro strumento diagnostico.\nSecondo la normativa, un Dispositivo Medico è definito come “qualunque strumento, apparecchio, impianto, software, sostanza o altro prodotto utilizzato da solo o in combinazione, compreso il software destinato dal fabbricante ad essere impiegato specificatamente con finalità diagnostiche e/o terapeutiche….”, quindi possono essere individuate quattro tipologie di software, che comprendono sia software “stand-alone”, cioè software utilizzati in modo autonomo, che software integrati in altri dispositivi medici che concorrono al funzionamento dei dispositivi stessi. Ad esempio, il software di controllo di una macchina per l’analisi di campioni biologici o di una macchina di acquisizione immagini rappresenta un software integrato, mentre un programma per l’elaborazione di immagini è un software stand-alone.\n\nA questo livello l’analisi visiva e l’elaborazione dell’immagine vengono effettuate in parallelo, quindi gli stessi insiemi di immagini vengono valutate visivamente ed elaborate al calcolatore. Nella pratica clinica odierna, tuttavia, iniziano ad essere utilizzate procedure di acquisizione destinate a produrre immagini inutili dal punto di vista dell’esame visivo e destinate in modo univoco all’elaborazione. Esempi tipici sono le immagini PC (Phase Contrast) in risonanza per la valutazione algoritmica della velocità di flusso e le immagini T1/T2/T2* multiecho per la caratterizzazione quantitativa dei tessuti in MR. In questi casi la parte di elaborazione cessa di essere un “add-on” del processo diagnostico, e ne diviene parte integrante e insostituibile. Nell’esempio di referto in figura troviamo un esempio di questo tipo nella definizione dell’accumulo di ferro attraverso la misura quantitativa del valore di T2* sul miocardio e sul parenchima del fegato. In questo caso il referto è puramente quantitativo, nel senso che l’analisi visiva non è rilevante. Nella terminologia moderna il termine “elaborazione delle bioimmagini” viene preferibilmente sostituito con il termine “analisi dell’immagine medica” (medical image analysis), proprio per sottolineare la preminenza dell’analisi software su quella visuale.\n\nÈ importante notare come in questo caso l’elaborazione dell’immagine biomedica da “ausilio al processo diagnostico” diviene “parte strutturale del processo diagnostico” e l’affidabilità del processo di elaborazione deve essere equivalente a quella di un dispositivo medico vero e proprio. Il decreto legislativo 37 del 25/01/2010 recepisce in Italia la direttiva europea 93/42/CEE sulla certificazione dei dispositivi medici, chiarendo che il software che implementa algoritmi di elaborazione di immagini se usato a fini diagnostici è da considerare un “dispositivo medico” e come tale richiede una opportuna certificazione. Sostanzialmente il software per l’analisi dell’immagine biomedica è equiparato ad un dispositivo di acquisizione o altro strumento diagnostico.\nSecondo la normativa, un Dispositivo Medico è definito come “qualunque strumento, apparecchio, impianto, software, sostanza o altro prodotto utilizzato da solo o in combinazione, compreso il software destinato dal fabbricante ad essere impiegato specificatamente con finalità diagnostiche e/o terapeutiche...”, quindi possono essere individuate quattro tipologie di software, che comprendono sia software “stand-alone”, cioè software utilizzati in modo autonomo, che software integrati in altri dispositivi medici che concorrono al funzionamento dei dispositivi stessi. Ad esempio, il software di controllo di una macchina per l’analisi di campioni biologici o di una macchina di acquisizione immagini rappresenta un software integrato, mentre un programma per l’elaborazione di immagini è un software stand-alone.\n\nLa Figura 1.2 descrive le quattro classi di software utilizzati in ambito biomedicale e soggetti a certificazione. Le procedure di elaborazione dell’immagine ricadono nella terza classe (Software sviluppato per analizzare dati acquisiti da dispositivi medici).\nÈ importante notare come il software che costituisce una cartella clinica non è un dispositivo medico, in quanto non contribuisce direttamente alla diagnosi e cura.\n\nFigura 1.2. Tipi di software utilizzati in ambito biomedicale.","type":"content","url":"/c11-imm-biom#dal-post-processing-allanalisi-dellimmagine","position":3},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl2":"Richiami sulla definizione di Immagine Biomedica"},"type":"lvl2","url":"/c11-imm-biom#richiami-sulla-definizione-di-immagine-biomedica","position":4},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl2":"Richiami sulla definizione di Immagine Biomedica"},"content":"","type":"content","url":"/c11-imm-biom#richiami-sulla-definizione-di-immagine-biomedica","position":5},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Livelli di grigio e colormap","lvl2":"Richiami sulla definizione di Immagine Biomedica"},"type":"lvl3","url":"/c11-imm-biom#livelli-di-grigio-e-colormap","position":6},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Livelli di grigio e colormap","lvl2":"Richiami sulla definizione di Immagine Biomedica"},"content":"Dal punto di vista dell’analisi dell’immagine, una immagine biomedica è una rappresentazione spazio/temporale di un fenomeno fisico legato al dispositivo di acquisizione. Le caratteristiche fisiche di una regione di spazio (voxel) rispetto ad una qualche eccitazione endogena o esogena (opacità ai raggi X, risonanza magnetica, emissione di radiazioni, etc.) vengono tradotti in un valore numerico di segnale. Nella maggior parte dei casi viene misurata una singola quantità, e quindi l’immagine presenta un singolo canale (livello di grigio). Il livello di grigio è proporzionale al fenomeno fisico misurato. Oggi sostanzialmente tutte le immagini biomediche sono immagini digitali (quindi sono espresse in forma numerica come matrici di numeri interi). Esiste uno standard di fatto di codifica che è il formato DICOM come verrà dettagliato nel seguito.\nVale la pena di notare che tra le normali immagini digitali (Figura 1.3) e le immagini biomediche (Figura 1.4) esistono alcune fondamentali differenze.\n\nUn immagine digitale è tipicamente a colori, quindi è rappresentata come un array (3,dx,dy), cioè attraverso tre canali tipicamente in formato RGB (Red, Green, Blue). I tre canali possono tipicamente assumere valori tra 0 e 255. Quindi una immagine digitale di dimensioni (dx,dy) non compressa occupa 3dxdy byte più alcuni byte di header, cioè di intestazione dell’immagine.\n\nFigura 1.3. Concetto di immagine a colori (RGB).\n\nUna immagine biomedica può essere invece vista come una immagine a livelli di grigio a singolo canale che può essere caratterizzata dal suo istogramma. Tipicamente i livelli di grigio possono essere più di 255, spesso l’immagine è codificata a 12 bit (212=4096) o a 16 bit (216=65536) nel caso di immagini a valori solo positivi. Se sono codificati valori negativi i valori possibili variano di conseguenza.\nNella figura è rappresentata una immagine di risonanza con valori che vanno dallo 0 (nessun segnale, aria) ai valori più alti relativi al grasso. È possibile visualizzare una immagine a livelli di grigio a colori attraverso una opportuna color map, cioè una trasformazione che fa corrispondere ad un certo livello di grigio una terna di valori RGB. Questa tecnica è spesso utilizzata per migliorare la qualità di visualizzazione, ad esempio, in immagini di medicina nucleare (SPECT o PET), sfruttando il fatto che l’occhio umano (o meglio il sistema occhio-cervello) è molto più efficiente nel riconoscimento dei colori rispetto ai livelli di grigio.\n\nFigura 1.4. Concetto di immagine biomedica (MRI assiale dell’addome, MRI Lab, FTGM, Pisa).\n\nDalla Figura 1.5 Notiamo che l’immagine a colori a destra non ha alcun senso dal punto di vista diagnostico. Mentre l’immagine a livelli di grigio ci informa del fatto che un tessuto (il grasso) produce un segnale MR maggiore di quello del muscolo e dell’aria, nell’immagine a colori non possiamo sapere se il blu rappresenta un segnale maggiore o minore del giallo.\n\nFigura 1.5. Immagine assiale MRI a livello della coscia (MRI Lab, Aarhus, DK) e trasformazione in RGB.\n\nPer dare un senso medico all’immagine dobbiamo associare all’immagine a colori la color map utilizzata (Figura 1.6).\n\nFigura 1.6. Esempio di colormap.\n\nLa color map ci dice che il blu corrisponde a livelli di segnale più basso rispetto al giallo e rende l’immagine utilizzabile in senso diagnostico. L’immagine, quindi, verrà visualizzata come in Figura 1.7.\n\nFigura 1.7. Visualizzazione corretta di una immagine a falsi colori in radiologia.\n\nL’immagine con associata la color map utilizzata per la rappresentazione a colori assume quindi una utilità diagnostica, in quanto consente di associare ad un colore l’intensità di segnale acquisita dalla macchina di acquisizione, in questo caso una MRI. Riassumendo:\n\nLe immagini biomediche tipicamente vengono visualizzate a livelli di grigio (US, MRI, CT). Il significato del livello di grigio dipende dalla fisica di acquisizione. Il fatto che alcuni software non specifici per l’analisi di immagini biomediche, come il MATLAB, visualizzino di default una immagine a livelli di grigio con una color map arbitraria non autorizza a ritenere che tale rappresentazione sia accettabile.\n\nIn alcuni casi (SPECT, PET) si utilizzano delle color map standard per visualizzare l’immagine a falsi colori. La color map utilizzata deve essere sempre associata all’immagine. L’immagine a colori risultante è una rappresentazione dell’immagine originale a livelli di grigio come acquisita dalla macchina di acquisizione, che è l’unica fonte reale di informazione diagnostica. Come si vedrà in seguito il formato standard di memorizzazione delle immagini biomediche consente il salvataggio della color map da utilizzare insieme all’immagine stessa.\n\nIl modo “giusto” di rappresentare una immagine dipende sostanzialmente da una convenzione tra gli utilizzatori. Se il mondo radiologico visualizza le immagini MRI a livelli di grigio è il caso di adeguarsi e non inventare rappresentazioni non standard. Non seguire la convenzione implica rendere l’immagine non utile alla comunità di medici che la utilizza e comporta tipicamente il licenziamento dell’Ingegnere Biomedico responsabile.","type":"content","url":"/c11-imm-biom#livelli-di-grigio-e-colormap","position":7},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl2":"Standard DICOM"},"type":"lvl2","url":"/c11-imm-biom#standard-dicom","position":8},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl2":"Standard DICOM"},"content":"Come già accennato le immagini biomediche sono tipicamente memorizzate in formato DICOM. Le immagini DICOM sono le uniche che conservano il valore diagnostico dell’immagine e hanno valore legale. Rappresentazioni dell’immagine DICOM (immagini a colori, salvataggio in formati a minor numero di livelli di grigio  come TIF o JPEG) alterano il contenuto diagnostico dell’immagine e non vanno utilizzati nella distribuzione o immagazzinamento di immagini mediche.\nUn file DICOM contiene tipicamente un header (intestazione) contenente informazioni sull’immagine e l’immagine vera e propria codificata come interi a 16 bit con o senza segno. Un file DICOM può contenere una singola immagine o più immagini. Le informazioni dell’header DICOM sono codificate tipicamente come:\n\nTAG\n\nDescription\n\nType\n\nLength\n\nValue\n\n0008,0022\n\nAcquisition Date\n\nDA\n\n10\n\n20120211\n\nIl campo TAG contiene un codice composto da due valori esadecimali (gruppo, elemento del gruppo) che identificano in modo univoco un elemento dello standard DICOM. Il campo description contiene la descrizione dell’elemento (in questo caso la data di acquisizione dell’immagine). Il campo type contiene il formato dell’elemento (in questo caso una data), mentre il campo Length contiene la lunghezza dell’elemento. Il campo Value contiene il valore dell’elemento stesso in un formato definito dallo standard. In Figura 1.8 viene riportato un esempio della struttura dei metadati di un file DICOM.\n\nFigura 1.8. Esempio di struttura dei metadati di un file DICOM.\n\nIn Python, l’accesso ai metadati DICOM è possibile tramite la libreria pydicom, che consente di leggere i file DICOM e di accedere ai singoli elementi informativi (tag) attraverso un’interfaccia ad oggetti.","type":"content","url":"/c11-imm-biom#standard-dicom","position":9},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Griglia di acquisizione, risoluzione e FOV","lvl2":"Standard DICOM"},"type":"lvl3","url":"/c11-imm-biom#griglia-di-acquisizione-risoluzione-e-fov","position":10},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Griglia di acquisizione, risoluzione e FOV","lvl2":"Standard DICOM"},"content":"Riepiloghiamo ora i parametri fondamentali di interesse di una immagine biomedica digitale, essendo tipicamente l’immagine memorizzata in formato DICOM nel seguito verranno forniti i nomi dei campi DICOM corrispondenti come sarà meglio dettagliato nel seguito.\nUna immagine biomedica bidimensionale può essere vista come una funzione I(x,y) che trasforma una coppia di coordinate nello spazio cartesiano in un valore di intensità di segnale. Il valore I sarà legato al principio fisico di produzione dell’immagine. Una immagine biomedica digitale (quindi discreta) e le sue coordinate saranno definite in un dominio:x = (0, 1,..., N_{x}-1) \\\\\ny = (0, 1,..., N_{y}-1) \\\\\nI = (0, 1,..., 2^{B}-1) \\\\\n\nDove N_{x} e N_{y} rappresentano il numero di righe e di colonne dell’immagine sui due assi (rows and columns nel formato DICOM di codifica) e B rappresenta la profondità dell’immagine espressa in bit (campo Bits Allocated nel DICOM). Alcune modalità di immagine, come la TAC, permettono di definire valori negativi dell’intensità di segnale. In questo caso il range dei valori possibili sarà  = (-2^{B}/2-1,...,2^{B}/2). Il formato dei dati immagazzinati è definito nel campo pixel representation del DICOM (1 signed, 0 unsigned).","type":"content","url":"/c11-imm-biom#griglia-di-acquisizione-risoluzione-e-fov","position":11},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl4":"Convenzione per gli assi","lvl3":"Griglia di acquisizione, risoluzione e FOV","lvl2":"Standard DICOM"},"type":"lvl4","url":"/c11-imm-biom#convenzione-per-gli-assi","position":12},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl4":"Convenzione per gli assi","lvl3":"Griglia di acquisizione, risoluzione e FOV","lvl2":"Standard DICOM"},"content":"Gli assi x ed y seguono una convenzione diversa da quella usuale, dove lo zero delle coordinate è in alto a sinistra e l’asse y è orientato verso il basso come riportato in Figura 1.9.\nIl numero di pixel dell’immagine sarà dato dal prodotto del numero di righe e del numero di colonne. Il numero di pixel è legato alla risoluzione spaziale dell’immagine, in quanto a parità di campo di vista (FOV) un maggior numero di pixel corrisponde ad una minore superficie rappresentata dal pixel e quindi ad una maggiore risoluzione dell’immagine. La risoluzione dell’immagine, espressa in millimetri per pixel, è un altro importante parametro di qualità non ricavabile direttamente dall’immagine vera e propria ma dalle informazioni associate all’immagine nell’header DICOM (campo Pixel Spacing). Il pixel spacing è legato alla dimensione dell’immagine e al Field of View (cioè la dimensione della regione di spazio oggetto dell’immagine) dall’ovvia relazioneFOVx = Rows*dx \\\\ \nFOVy = Columns*dy \\\\\n\ndove dx e dy rappresentano il pixel spacing. Tipicamente sulla console di acquisizione vengono settati il FOV (che ha implicazioni dirette su parametri clinici, quali la dose assorbita in TAC e gli artefatti da ribaltamento in MRI) e le dimensioni della matrice di acquisizione che ha tipicamente effetto sul tempo di acquisizione. Il pixel spacing viene definito dalla macchina di conseguenza. I pixel sono di solito quadrati (dx=dy) per la simmetria del dispositivo di acquisizione lungo il piano dell’acquisizione stessa. È importante ricordare che l’acquisizione di una immagine planare richiede comunque l’acquisizione di un segnale da una porzione finita di spazio, quindi ogni pixel corrisponde comunque ad un volume spaziale con forma a parallelepipedo con base uguale al pixel stesso e altezza dipendente dalla modalità. Ad esempio, nel caso della MRI l’altezza sarà uguale al thickness della fetta (campo Slice Thickness del DICOM). E’ importante notare che il concetto di risoluzione così espresso non è equivalente al concetto di risoluzione dell’immagine biomedica da un punto di vista fisico, che rappresenta la dimensione minima di un oggetto apprezzabile da un dato sistema di imaging. A livello di acquisizione si può scegliere di avere una risoluzione peggiore di quella potenziale del dispositivo per ottimizzare altri parametri, quali il tempo di acquisizione, la dose irradiata o il rapporto segnale rumore.\n\nFigura 1.9. Convenzione per gli assi, FOV e pixel spacing.\n\nSe l’immagine biomedica descrive un volume tridimensionale, essa sarà descritta da una funzione I(x,y,z). Indipendentemente dalla modalità di acquisizione (slice-by-slice o vero 3D) il volume acquisito viene salvato sotto forma di una serie di immagini 2D parallele perpendicolari all’asse z (Figura 1.10). La distanza tra le fette è uguale alla risoluzione lungo l’asse z (dz). E’ importante notare che mentre i pixel sono di solito quadrati, e quindi la risoluzione lungo gli assi x e y è di solito la stessa, la risoluzione lungo l’asse z è spesso diversa ed in particolare più ridotta (quindi dz > dx,dy). Gli elementi fondamentali del volume dei dati (voxel) saranno quindi in forma di parallelepipedo.\n\nFigura 1.10. Slice thickness e slice gap.\n\nLa risoluzione spaziale lungo z è uguale alla distanza tra due slice consecutive. Tale distanza è la somma dello slice thickness più un eventuale gap tra le slices (gap=0 vuol dire che il volume di interesse viene coperto interamente da slices adiacenti, gap>0 che porzioni del volume non vengono coperte, gap<0 che c’è un overlap tra le slices). Il valore del gap dovrebbe essere contenuto nel campo Spacing Between Slices del DICOM. Tuttavia l’uso di questo campo è fortemente arbitrario (alcune interpretazioni scrivono qui direttamente la distanza inter-slice), e il suo uso è quindi sconsigliato. La scelta più opportuna è valutare la distanza inter-slice come la distanza euclidea tra gli angoli superiori sinistri delle due slices le cui coordinate sono memorizzate  nel campo Image Position Patient.\nInfine, possiamo avere immagini dinamiche acquisite nel tempo per descrivere un certo fenomeno. I questo caso definiremo risoluzione temporale delle immagini la distanza in unità temporali (millisecondi o secondi) tra due immagini successive. Ricavare la risoluzione temporale dalle informazioni dell’header DICOM è non banale. Il campo image time contiene il tempo di acquisizione dell’immagine in \n\nhh.mm.ss, e quindi non è di solito adeguato per fenomeni rapidi. Il campo Trigger Time consente di memorizzare il tempo di acquisizione in millisecondi, a partire dall’inizio dell’acquisizione. Il nome del campo deriva dal fatto che l’acquisizione viene pilotata non dall’operatore ma da un segnale esterno o interno alla macchina. Ad esempio nel caso di una acquisizione sincronizzata con l’ECG cardiaco il trigger time verrà determinato dal segnale ECG acquisito sul paziente.\n\nLa posizione della slice o del volume nel sistema di riferimento della macchina o del paziente sono descritti nell’annex A del documento PS 3.17 (Explanatory Information) dello standard DICOM (riportato anche in Figura 1.11), liberamente accessibile in rete ad esempio dal sito \n\nhttp://​www​.dclunie​.com​/dicom​-status.\n\nFigura 1.11. Sistema di riferimento DICOM (Tratto da DICOM Standard Annex A PS 3.17)\n\nLe coordinate della slice sono fornite nel sistema di riferimento solidale al paziente, dove l’asse z segue la direzione F-H (Piedi-Testa), l’asse y la direzione A-P (Anteriore-Posteriore), l’asse x la direzione R-L (Destra-Sinistra). La posizione del sistema di riferimento paziente rispetto al sistema di riferimento dello scanner (asse z direzione di ingresso del paziente nella macchina, asse y alto-basso, asse x sinistra-destra) dipende dall’orientamento del paziente nella macchina, memorizzato nel campo DICOM Patient Position. Ad esempio il valore FFS (Feet First-Supine) indica che il paziente entra con i piedi in avanti e in posizione supina, quindi gli assi z e x sono invertiti. Il campo image position patient fornisce le coordinate in mm dell’angolo superiore sinistro (l’origine) dell’immagine nel sistema di riferimento del paziente. Il campo image orientation patient fornisce l’orientazione dei due versori che partono dall’origine dell’immagine lungo i due assi del sistema di coordinate solidale all’immagine. Ad esempio il valore 1.0\\0.0\\0.0\\0.0\\1.0\\0.0 indica una immagine assiale.\n\nAlcuni campi DICOM di interesse\n\nGroup Element\n\nTitle\n\nEsempio\n\n[0028-0010]\n\nRows\n\n256\n\n[0028-0011]\n\nColumns\n\n256\n\n[0028-0100]\n\nBits Allocated\n\n16\n\n[0028-0103]\n\nPixel Representation\n\n1\n\n[0028-0030]\n\nPixel Spacing\n\n1.24\\1.24\n\n[0018-0050]\n\nSlice Thickness\n\n8\n\n[0018-0088]\n\nSpacing Between Slices\n\n? Inaffidabile\n\n[0020-0032]\n\nImage Position Patient\n\n-203.1-190.1\\33.9\n\n[0020-0037]\n\nImage Orientation Patient\n\n1.0\\0.0\\0.0\\0.0\\1.0\\0.0\n\n[0018-1060]\n\nTrigger Time\n\n16\n\n[0008-0033]\n\nImage Time\n\n142850\n\n[0018-5100]\n\nPatient Position\n\nFFS","type":"content","url":"/c11-imm-biom#convenzione-per-gli-assi","position":13},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"PACS","lvl2":"Standard DICOM"},"type":"lvl3","url":"/c11-imm-biom#pacs","position":14},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"PACS","lvl2":"Standard DICOM"},"content":"Oltre alle informazioni sull’immagine prima viste lo standard DICOM (Digital Imaging and COmmunications in Medicine, immagini e comunicazione digitali in medicina) definisce i criteri per la comunicazione, la visualizzazione, l’archiviazione e la stampa. In particolare il formato DICOM contiene le informazioni utili all’identificazione del paziente, che possono essere inserite manualmente sulla console di acquisizione da parte del tecnico radiologo o nei sistemi moderni ottenute automaticamente dal sistema HIS (Hospital Information System) attraverso la lettura della cosiddetta work list (la lista degli esami da effettuare nella giornata). Una volta acquisite, le immagini DICOM vengono tipicamente memorizzate in un sistema PACS (Picture archiving and communication system), un sistema hardware e software dedicato all’archiviazione, trasmissione, visualizzazione e stampa delle immagini diagnostiche digitali. Un sistema PACS è normalmente composto da una parte di archiviazione, utilizzata per gestire dati e immagini e una di visualizzazione, che presenta l’immagine diagnostica su speciali monitor ad altissima risoluzione, sui quali è possibile effettuare la diagnosi; i sistemi PACS più evoluti permettono anche l’elaborazione dell’immagine. All’interno del PACS è possibile effettuare delle ricerche attraverso i campi DICOM (ad esempio nome del paziente, data dell’esame, tipo di esame, etc). Un sistema PACS è in grado di mantenere nella propria memoria fisica (tipicamente un array di hard disk) le immagini DICOM per un certo tempo (qualche mese) dipendente dalla quantità di immagini acquisite per giorno e dalla grandezza dell’archivio. In seguito le immagini restano disponibili su supporti di backup come DVD o magnetoottici. I dischi di un sistema PACS devono essere configurati in RAID o altra architettura ridondante in modo che la rottura di un disco non comporti una perdita di dati e possa essere sostituito “a caldo” in caso di problemi senza interrompere il funzionamento del sistema. La comunicazione tra la macchina di acquisizione e il PACS avviene attraverso le procedure definite dallo standard DICOM. Ogni macchina è definita dal proprio indirizzo di rete, dalla porta utilizzata per la comunicazione e dal proprio AETITLE (Application Entity Title). Per la comunicazione la macchina chiamante (client) deve essere dichiarata coi i propri parametri sulla macchina che funge da server. I sistemi PACS sono dispositivi medici, in quanto utilizzati per effettuare diagnosi, a meno che non rientrino nella classe I (funzione di solo archivio). Se implementano altre funzioni (anche la semplice visualizzazione) sono da considerare dispositivi medici a tutti gli effetti e come tali richiedono opportuna certificazione. Un programma che effettui analisi dell’immagine medica includerà tipicamente un client DICOM per interfacciarsi con un PACS e scaricare le immagini da elaborare. I programmi di analisi più evoluti sono in grado di salvare i risultati dell’elaborazione come “secondary DICOM’, cioè file DICOM non prodotti da un dispositivo di acquisizione ma frutto di elaborazioni successive.\n\nLa figura 1.12 mostra la tipica architettura di rete in un centro radiologico. I dispositivi di acquisizione (scanner), ad esempio TAC, MR, PET, etc convogliano le immagini acquisite nel PACS. Dal PACS è possibile stampare lastre radiografiche, produrre CD/DVD per i pazienti, effettuare il backup dei dati, etc.  Un programma di elaborazione di immagini scaricherà i dati dal PACS ed eventualmente salverà i risultati sul PACS stesso alla fine del processo.\n\nFigura 1.12. Architettura di un sistema PACS","type":"content","url":"/c11-imm-biom#pacs","position":15},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl2":"Analisi dell’immagine biomedica"},"type":"lvl2","url":"/c11-imm-biom#analisi-dellimmagine-biomedica","position":16},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl2":"Analisi dell’immagine biomedica"},"content":"Il problema dell’analisi dell’immagine biomedica è parte della cosiddetta computer vision (visione artificiale), una branca dell’intelligenza artificiale che ha origine negli anni 60 del secolo scorso e che si occupa di implementare algoritmi che riproducano le funzioni della visione umana. Un sistema di computer vision è tipicamente strutturato in una serie di operazioni classificate in ordine di complessità come:\n\nImage Acquisition. Include l’acquisizione delle immagini da elaborare attraverso fotocamere, videocamere ,etc. Nell’imaging biomedico corrisponde alla fase di acquisizione delle bioimmagini attraverso i vari dispositivi di acquisizione.\n\nImage pre-processing. Include le operazioni a basso livello sull’immagine, come la riduzione del rumore, il miglioramento del contrasto, il cambio della scala di rappresentazione. Nell’imaging biomedico questa fase include gli algoritmi di interpolazione, compressione e filtraggio.\n\nFeature extraction. Include l’estrazione dalle immagini di caratteristiche geometriche di interesse, come linee, punti, curve ed i cosiddetti blob, cioè aggregati di pixel con caratteristiche comuni.\n\nDetection/Segmentation. Include l’estrazione dall’immagine di oggetti di interesse, ad esempio separando lo sfondo dell’immagine e estraendo dalla scena gli oggetti visibili. Nell’imaging biomedico i punti 3 e 4 della computer vision vengono tipicamente raggruppati negli algoritmi di segmentazione o pattern recognition. Si noti che in questa fase gli oggetti vengono estratti ma non riconosciuti.\n\nHigh-level processing. Questa fase include le operazioni ad alto livello legate al riconoscimento degli oggetti, cioè alla loro classificazione. Nell’imaging biomedico abbiamo la corrispondenza con gli algoritmi di classificazione e registrazione delle bioimmagini.\n\nDecision Making. In questa fase le informazioni ottenute nelle fasi precedenti vengono utilizzate per determinare una azione da intraprendere. Nell’imaging biomedico questo corrisponde ad effettuare una diagnosi con metodi di intelligenza artificiale.\n\nIn questo corso analizzeremo i punti 2-5.","type":"content","url":"/c11-imm-biom#analisi-dellimmagine-biomedica","position":17},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Modello dell’immagine biomedica","lvl2":"Analisi dell’immagine biomedica"},"type":"lvl3","url":"/c11-imm-biom#modello-dellimmagine-biomedica","position":18},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Modello dell’immagine biomedica","lvl2":"Analisi dell’immagine biomedica"},"content":"Nell’ingegneria è diffusa la credenza che sia possibile sviluppare modelli matematici che approssimino il mondo reale con sufficiente accuratezza e il campo dell’imaging biomedico non fa eccezione. Definiamo quindi l’immagine biomedica ideale come un insieme di regioni non sovrapposte ognuna caratterizzata da un certo livello di grigio. Ogni livello di grigio e quindi ogni regione corrisponderà ad un tessuto. Avremo quindi:\\begin{aligned}\nI_0(x,y) &= \\sum_{i=1}^{k} P_i(x,y) ; \\quad P_i(x,y) &=\n\\begin{cases}\ns_i & \\text{se } V(x,y) \\in d_i \\\\\n0   & \\text{altrove}\n\\end{cases}\n\\end{aligned}\n\ned i domini d_{i} sono disgiunti. k è “piccolo”, nel senso che il numero di tessuti costituenti è limitato. In generale tanto più grande è k tanto più il modello sarà simile all’immagine reale, fino a k uguale al numero di pixel dell’immagine quando si ha l’identità completa tra modello e immagine ed il modello diviene totalmente inutile. Per comprendere il senso del modello, si pensi a quale è il fine diagnostico dell’imaging medico. Quello che si vorrebbe è che a ogni tessuto venga associato in modo univoco un singolo valore di segnale, in modo che ogni tessuto sia distinguibile dagli altri senza possibilità di errore. Una immagine di questo tipo che presenta la corrispondenza univoca tessuto → valore di segnale è l’immagine biomedica ideale ai fini diagnostici e coincide con I_{0}. In un certo senso l’elaborazione dell’immagine biomedica si potrebbe definire come l’insieme delle tecniche che consentono di estrarre l’immagine ideale I_{0} dall’immagine reale come acquisita dal dispositivo di imaging. Un esempio di immagine ideale e immagine reale è riportato in Figura 1.13.\n\nFigura 1.13. Sinistra: Immagine reale. Destra: immagine ideale.\n\nIntroduciamo ora nel modello i fattori che corrompono l’immagine ideale:\n\nRumore biologico.\nAlcuni tessuti non sono omogenei ma caratterizzati da un struttura interna. Ad esempio nel cuore il sangue che riempie i due ventricoli potrà in generale essere considerato omogeneo essendo un liquido (non considerando il movimento del sangue stesso) mentre il muscolo cardiaco sarà caratterizzato da una struttura a fibre che produce una disomogeneità nell’immagine. Questo fenomeno viene definito “rumore biologico” n_{B}(x,y), nel senso che le proprietà intrinseche del tessuto divergono dal modello ideale. Evidentemente il concetto di rumore biologico è legato alla risoluzione dell’immagine, che determina la grandezza delle disomogeneità rilevabili. Nell’esempio precedente, il sangue è disomogeneo a livello microscopico (globuli rossi e bianchi, piastrine, etc) ma omogeneo a livello della risoluzione MRI (> 1mm).\n\nEffetto volume parziale (partial volume effect, PVE). Il fatto che il processo di acquisizione sia discreto implica che il segnale viene acquisito in un certo volume di spazio pari alla risoluzione spaziale della metodica. Se due tessuti convivono nello stesso volume elementare, il voxel corrispondente dell’immagine assumerà un valore di livello di grigio intermedio tra i valori caratteristici dei due tessuti (effetto volume parziale, Figura 1.14). Questo effetto viene modellato attraverso la convoluzione con un kernel gaussiano h(x,y), che corrisponde ad una operazione di smoothing dei contorni dell’immagine. Infatti l’effetto volume parziale agisce solo sulle zone di transizione tra i due tessuti.\n\nFigura 1.14. Esempio di effetto di volume parziale (PVE).\n\nAttenuazione. In molti casi l’immagine biomedica sarà affetta da un processo di attenuazione del segnale, che produce una distorsione continua dell’immagine lentamente variabile. Nella MRI ad esempio avremo un effetto indotto dalla disomogeneità nel campo magnetico statico o dalla sensibilità delle bobine, mentre negli ultrasuoni l’effetto sarà legato alla distanza del tessuto dalla sonda. L’effetto di attenuazione verrà descritto da un campo moltiplicativo g(x,y). Il campo moltiplicativo è tipicamente caratterizzato dall’essere “smooth”, cioè dal fatto che varia lentamente senza brusche transizioni.\n\nRumore. Infine l’immagine sarà corrotta da rumore con una certa distribuzione n(x,y), che dipende dal processo fisico utilizzato per l’acquisizione. Ad esempio in MRI il rumore avrà distribuzione di tipo Riciano, negli US avrà distribuzione di Rayleigh, etc. Al contrario di quanto avviene per il rumore Gaussiano, il rumore non è necessariamente di tipo additivo. Un esempio di rumore non additivo verrà introdotto nel seguito.\n\nIn definitiva l’immagine osservata I(x,y) sarà descritta dalla relazione:I(x,y)=[[I_{0}(x,y)+n_{B}(x,y)]*h(x,y)]g(x,y)+n(x,y)\n\nil simbolo + che precede il rumore non va inteso in senso rigorosamente additivo come prima detto.\nLa stima dell’immagine ideale I_{0}(x,y) comporta la stima di tutti gli elementi compresi nella formula, nota I(x,y) che è l’immagine reale. Il problema è naturalmente molto complesso e non prevede una soluzione unica. In generale l’approccio utilizzato è quello di modellare le varie componenti in base a dati noti e trovare una soluzione minimizzando un qualche funzionale.","type":"content","url":"/c11-imm-biom#modello-dellimmagine-biomedica","position":19},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Istogramma","lvl2":"Analisi dell’immagine biomedica"},"type":"lvl3","url":"/c11-imm-biom#istogramma","position":20},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Istogramma","lvl2":"Analisi dell’immagine biomedica"},"content":"È di interesse interpretare quanto detto finora dal punto di vista dell’Istogramma dell’immagine. Un istogramma è una rappresentazione statistica della distribuzione dei livelli di grigio nell’immagine.\nIn ascissa abbiamo i possibili valori di livello di grigio ed in ordinata il numero di pixel che assumono quel determinato valore. Se normalizziamo i valori dell’istogramma per il numero totale di pixel, l’istogramma normalizzato rappresenterà la probabilità di trovare nell’immagine un pixel con quel valore. Spesso l’asse x viene quantizzato in un certo numero di regioni (bins) e lungo l’asse y viene riportato il numero di pixel con valore compreso nel bin per migliorare la qualità visiva dell’istogramma.\nL’istogramma dell’immagine ideale I_{0} sarà costituito da k picchi di altezza pari al numero di pixel appartenenti al dominio k. Si consideri ad esempio l’immagine in Figura 1.15 a 6 pattern e il relativo istogramma, definito da 6 picchi distinti con altezza uguale al numero di pixel appartenenti al pattern.\n\nFigura 1.15. Esempio di istogramma per un’immagine ideale.","type":"content","url":"/c11-imm-biom#istogramma","position":21},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl4":"Effetto volume parziale (PVE)","lvl3":"Istogramma","lvl2":"Analisi dell’immagine biomedica"},"type":"lvl4","url":"/c11-imm-biom#effetto-volume-parziale-pve","position":22},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl4":"Effetto volume parziale (PVE)","lvl3":"Istogramma","lvl2":"Analisi dell’immagine biomedica"},"content":"Se effettuiamo una operazione di smoothing (filtro a media mobile) simulando l’effetto volume parziale, l’istogramma si trasforma come in Figura 1.16. Come si osserva i bordi dell’immagine si sfumano e nell’istogramma compaiono dei nuovi valori dei livelli di grigio tra i picchi, distribuiti in modo uniforme. Infatti la percentuale di tessuti diversi che ricade all’interno della finestra di smoothing è distribuita casualmente secondo una distribuzione uniforme.\n\nFigura 1.16. Esempio di istogramma per un’immagine con PVE.\n\nQuesto è ciò che accade anche nella procedura di acquisizione reale a causa del PVE. Se in un voxel si trovano due tessuti con segnale S_{1} e S_{2}, il segnale risultante del voxel sarà:S = pS_{1}+(1-p)S_{2}\n\ndove p compreso tra 0 e 1 è la percentuale di voxel occupato dal tessuto 1. Evidentemente il valore di S sarà compreso tra S_{1} per (p=1) e S_{2} per (p=0). Nelle immagini biomediche reali p si può considerare distribuito in modo uniforme, in quanto l’oggetto dell’imaging ha forma irregolare e la griglia di acquisizione e la forma dell’oggetto di cui si fa l’imaging non hanno nessuna relazione particolare. Si assumerà quindi una distribuzione di probabilità uniforme tra i valori S_{1} e S_{2}. Si noti che in casi molto particolari, come quello di un fantoccio cubico con le facce parallele agli assi x e y del riferimento di acquisizione questa assunzione può non essere vera.\nQuanto detto si può estendere alla compenetrazione di tre o più tessuti nello stesso voxel.\n\nPossiamo quindi concludere che in generale:\n\nIl PVE introduce nuovi livelli di grigio, compresi tra il livello minimo e il livello massimo dei due tessuti interessati nell’immagine ideale.\n\nI livelli di grigio creati sono distribuiti uniformemente tra i picchi dei tessuti adiacenti.\n\nIl numero di livelli creati dipende dal perimetro dei pattern e non dalla loro area.\n\nIn una immagine reale, a parità di oggetto di cui si fa l’imaging, l’incidenza del PVE dipende dalla risoluzione. Usando pixel più piccoli la percentuale di pixel affetti dal PVE sarà minore. Come detto in precedenza, anche in caso di immagine planare il segnale di un pixel sarà sempre originato da un volume spaziale finito di forma parallelepipodale. Esisterà quindi anche un PVE in direzione perpendicolare al piano di acquisizione, tanto più marcato quanto maggiore è il thickness della fetta.","type":"content","url":"/c11-imm-biom#effetto-volume-parziale-pve","position":23},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl4":"Effetto dell’attenuazione","lvl3":"Istogramma","lvl2":"Analisi dell’immagine biomedica"},"type":"lvl4","url":"/c11-imm-biom#effetto-dellattenuazione","position":24},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl4":"Effetto dell’attenuazione","lvl3":"Istogramma","lvl2":"Analisi dell’immagine biomedica"},"content":"L’attenuazione dell’immagine è modellata da un processo moltiplicativo per cui lo stesso tessuto in parti diverse dell’immagine assume valori diversi. Dal punto di vista dell’istogramma avremo un allargamento e una deformazione dei picchi.  In Figura 1.17 osserviamo una immagine ideale composta di tre tessuti ed il relativo istogramma. L’immagine viene moltiplicata per un campo di attenuazione il cui andamento è mostrato in Figura 1.18, originando una nuova immagine con livelli di grigio distorti. L’istogramma dell’immagine attenuata risulta distorto.\n\nFigura 1.17. Esempio di campo di attenuazione.\n\nFigura 1.18. Istogramma immagine con effetto di attenuazione.","type":"content","url":"/c11-imm-biom#effetto-dellattenuazione","position":25},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl4":"Effetto del rumore","lvl3":"Istogramma","lvl2":"Analisi dell’immagine biomedica"},"type":"lvl4","url":"/c11-imm-biom#effetto-del-rumore","position":26},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl4":"Effetto del rumore","lvl3":"Istogramma","lvl2":"Analisi dell’immagine biomedica"},"content":"Il rumore, se gaussiano a media nulla, crea un allargamento dei picchi dell’istogramma. Infatti si creano dei livelli di grigio che deviano dal valore teorico dei pattern tanto più quanto più è grande la deviazione standard del processo di rumore. Un esempio di effetto di rumore Gaussiano additivo è riportato in Figura 1.19.\n\nFigura 1.19. Istogramma immagine con rumore Gaussiano additivo.\n\nApplicando tutti i fattori di disturbo otteniamo l’immagine reale (Figura 1.20).\nCome si osserva l’istogramma dell’immagine è modificato in modo dipendente dal peso e dalle caratteristiche dei vari fattori di disturbo.\n\nFigura 1.20. Istogramma immagine con pve, attenuazione e rumore.\n\nSe calcoliamo la differenza tra l’immagine reale e l’immagine ideale otteniamo l’immagine e l’istogramma di Figura 1.21.\n\nFigura 1.21. Istogramma dell’immagine differenza.\n\nSi nota come la differenza tra le due immagini sia più pronunciata nelle zone di transizione tra tessuti (che sono quelle di interesse clinico), mentre è minore nelle zone omogenee. L’istogramma della differenza mostra come la maggior parte degli errori siano piccoli (si addensano intorno allo zero), ma esistono anche variazioni molto grandi.","type":"content","url":"/c11-imm-biom#effetto-del-rumore","position":27},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Entropia","lvl2":"Analisi dell’immagine biomedica"},"type":"lvl3","url":"/c11-imm-biom#entropia","position":28},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Entropia","lvl2":"Analisi dell’immagine biomedica"},"content":"I vari processi di distorsione dell’immagine possono essere quantificati introducendo il concetto di Entropia dell’immagine.\nL’entropia di Shannon di una immagine I è una quantità definita dalla teoria dell’informazione ed è data da:H(I) = - \\sum_{g_i \\in G} \\log\\!\\bigl(P(I = g_i)\\bigr)\\, P(I = g_i)\n\nDove g_{i} sono i livelli di grigio dell’immagine I appartenenti all’insieme G dei possibili livelli di grigio, determinato dalla profondità dell’immagine. P(I=g_{i}) è la probabilità che un pixel dell’immagine assuma il valore g_{i}. Essendo P(I=g_{i}) sempre minore o uguale ad uno, l’entropia H assume sempre valori positivi ed assume il valore nullo solo nel caso di P(I=g_{i}) = 1 per un singolo valore di i, quindi nel caso di una immagine uniforme composta da un solo pattern. Secondo la teoria di Shannon, l’entropia esprime la quantità di informazione contenuta nell’immagine. Una immagine composta da rumore avrà massima entropia mentre una immagine composta da un singolo valore avrà entropia minima. In questo senso, stabilita la configurazione dei pattern che costituiscono l’immagine, l’immagine ideale avrà entropia minore rispetto alle immagini ottenute applicando i vari fattori di disturbo.\nLa probabilità P(I=g_{i}) utilizzata per il calcolo dell’entropia non è altro che l’istogramma normalizzato per il numero di pixel dell’immagine, quindi possiamo scrivere:H(I) = -\\frac{1}{N} \\sum_{i=1}^{N} h_I(i)\\, \\log\\!\\left(\\frac{h_I(i)}{N}\\right)\n\ndove h_{I} è l’istogramma di I e N è il numero di pixel di I.\n\nPer cui l‘entropia può essere calcolata facilmente dall’istogramma dell’immagine.In Figura 1.22 sono riportati l’istogramma ed il valore di entropia per i casi di immagine random (composta da rumore casuale, entropia massima), immagine ad un singolo pattern (entropia nulla), immagine a due pattern, immagine a due pattern corrotta da rumore gaussiano, immagine a due pattern corrotta da effetto volume parziale. Come si osserva dagli ultimi tre esempi, l’entropia cresce in dipendenza dai fattori che allontanano l’immagine reale dall’immagine ideale.\n\nFigura 1.22. Valori di entropia per diversi tipi di istogramma.","type":"content","url":"/c11-imm-biom#entropia","position":29},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl2":"Qualità dell’immagine biomedica"},"type":"lvl2","url":"/c11-imm-biom#qualit-dellimmagine-biomedica","position":30},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl2":"Qualità dell’immagine biomedica"},"content":"Il fine di una immagine biomedica è quello di permettere un diagnosi il più possibile accurata da parte del medico. La nozione di qualità dell’immagine biomedica non è facilmente definibile, visto che il modo in cui l’immagine viene utilizzata dall’operatore per definire la diagnosi è fortemente soggettivo e non descrivibile in modo rigoroso. D’altra parte la necessità di definire la qualità di immagine è un punto importante nella progettazione e “quality assessment” dei dispositivi biomedici.\nUn modo comune per rendere maggiormente oggettiva la misura di qualità è l’uso di “phantom”, cioè di fantocci che possono essere costruiti in modo riproducibile e quindi utilizzati per misure ripetute escludendo dalla misura la variabilità dovuta al paziente. Tipicamente le macchine per l’acquisizione di immagini biomediche hanno in dotazione uno o più phantom standardizzati che consentono il quality assessment periodico delle prestazioni del dispositivo. Nel seguito verranno descritti alcuni indici utilizzati comunemente nella misura di qualità dell’immagine biomedica.","type":"content","url":"/c11-imm-biom#qualit-dellimmagine-biomedica","position":31},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Qualitativa o semi-quantitativa","lvl2":"Qualità dell’immagine biomedica"},"type":"lvl3","url":"/c11-imm-biom#qualitativa-o-semi-quantitativa","position":32},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Qualitativa o semi-quantitativa","lvl2":"Qualità dell’immagine biomedica"},"content":"La valutazione viene effettuata da un operatore esperto, che può utilizzare una scala predefinita con un certo numero di livelli per standardizzare la valutazione (analisi semi-quantitativa). La valutazione risulta chiaramente fortemente dipendente dall’operatore. Per minimizzare questo fattore la valutazione può essere fatta da più operatori, mediando i risultati ed eventualmente eliminando i valori estremi.\nUna scala usata correntemente è quella a cinque punti, ad esempio:\n\nnondiagnostic study\n\npoor or suboptimal study\n\nacceptable\n\ngood\n\nexcellent\n\nLa qualità dell’immagine sarà quindi espressa da un numero compreso tra 1 e 5 (anche frazionario se sono coinvolti più osservatori) proporzionale alla qualità dell’immagine.\n\nIn questo caso il processo di valutazione comporterà l’analisi di un certo numero di immagini da parte di operatori esperti con l’attribuzione di un punteggio di qualità. La media dei punteggi (o la mediana volendo escludere i punteggi “estremi”) rappresenterà l’indice di qualità assegnato all’immagine.","type":"content","url":"/c11-imm-biom#qualitativa-o-semi-quantitativa","position":33},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"SNR e CNR","lvl2":"Qualità dell’immagine biomedica"},"type":"lvl3","url":"/c11-imm-biom#snr-e-cnr","position":34},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"SNR e CNR","lvl2":"Qualità dell’immagine biomedica"},"content":"L’SNR e il CNR rappresentano due parametri di qualità largamente utilizzati nella valutazione delle immagini. Facciamo riferimento ad un modello semplificato di immagine biomedica dove l’immagine reale è data dalla somma dell’immagine reale e del rumore.I(x,y)=I_{0}(x,y)+n(x,y)\n\nLa natura del rumore n dipenderà dal processo fisico di acquisizione, per ora supponiamo che il rumore sia gaussiano con media nulla e deviazione standard  \\sigma.In figura osserviamo a sinistra una immagine ideale composta da tre regioni con valore di segnale s1=50, s2=150, s3=200 e a destra la stessa immagine corrotta da rumore con \\sigma=10. L’immagine sintetica di Figura 1.23 imita un phantom usato per la misura di qualità in MRI, costituito da due cilindri concentrici in plastica riempiti con due liquidi diversi. Si tratta quindi di un phantom con due tessuti perfettamente omogenei.\n\nFigura 1.23. Fantoccio: caso ideale e reale e relativi istogrammi.\n\nSi nota come il rumore allarghi la distribuzione del segnale relativa ai vari tessuti, fino a far confondere tra loro pixel appartenenti a tessuti diversi. Maggiore è la deviazione standard del rumore rispetto al valore del segnale, tanto peggiore è la qualità dell’immagine. Si definisce quindi l’SNR (rapporto segnale rumore) come:SNR = \\frac{M_{i}}{\\sigma_{i}}\n\nIl rapporto segnale rumore (SNR) è quindi il rapporto tra il valor medio del segnale in una regione e la deviazione standard (\\sigma, SD) del rumore nella stessa regione. L’SNR dà quindi una misura di quanto il valore del segnale ottenuto dal dispositivo di imaging è corrotto dal rumore. A livello di acquisizione, σ è tipicamente una costante dipendendo dal rumore introdotto dal processo di acquisizione (detettori, amplificatori, etc). Quindi l’SNR dipenderà dalla risoluzione dell’immagine (nel senso che più piccolo è il voxel meno segnale verrà acquisito e quindi M sarà più piccolo diminuendo l’SNR) e dal tempo di acquisizione (maggior tempo di acquisizione incrementa M aumentando l’SNR). L’SNR ottimo è quindi un compromesso tra qualità dell’immagine, tempo di acquisizione e risoluzione.\n\nNell’uso clinico è in realtà più importante il contrasto, cioè la capacità del dispositivo di imaging di distinguere due tessuti diversi. Si introduce quindi il CNR:CNR = \\frac{2|M_{i}-M_{j}|}{\\sigma_{i}+\\sigma_{j}}\n\nIl CNR (rapporto contrasto-rumore) è il rapporto tra la differenza dei valori medi del segnale nelle due regioni e il rumore medio nelle due regioni. Sul CNR valgono le stesse considerazioni fatte per l’SNR.\n\nQuesti indici possono essere valutati in modo manuale tracciando sull’immagine delle regioni di interesse (ROI) e calcolando il valor medio e la deviazione standard del segnale nelle ROI stesse.\nE’ evidente che la misura va effettuata in regioni omogenee dell’immagine e che maggiore è l’area della ROI utilizzata maggiore sarà la precisione della misura. Un limite alle dimensioni della ROI è dato dal PVE, nel senso che i bordi della ROI devono essere lontani dai bordi dei tessuti per evitare di includere regioni dove è presente il PVE. ROI troppo grandi possono dare un valore errato di SNR/CNR se sono presenti fenomeni di attenuazione. Un esempio di tracciamento di ROI per il calcolo del CNR è riportato in Figura 1.24.\n\nFigura 1.24. Esempio di tracciamento ROI per calcolo CNR.\n\nPer ottimizzare la misura si possono far coincidere le ROI con l’intera immagine, acquisendo due immagini dello stesso oggetto con gli stessi parametri di acquisizione. L’immagine ideale non cambia perché l’oggetto è lo stesso, quindi sottraendo le due immagini avremo una immagine differenza dipendente solo dal processo di rumore:\\begin{aligned}\nI_1(x,y) &= I_0(x,y) + n_1(x,y) \\\\\nI_2(x,y) &= I_0(x,y) + n_2(x,y) \\\\\n\\Delta I &= I_2(x,y) - I_1(x,y) = n_2(x,y) - n_1(x,y) = n_d(x,y)\\\\ \n\\end{aligned}\n\nAnche se il rumore è gaussiano, la differenza delle due distribuzioni di rumore n_{d}(x,y) non sarà in generale gaussiana. Una stima superiore della SD dell’immagine differenza è   volte la SD delle due componenti. Misurando quindi la SD sull’immagine differenza e dividendo per  avremo una misura oggettiva del limite superiore del rumore dell’immagine. Notiamo che in questo caso la misura di SD può essere fatta su tutta l’immagine, migliorando la qualità della misura. Un altro vantaggio è che la misura può essere facilmente automatizzata. Questa tecnica può essere utilizzata in sede di valutazione delle apparecchiature quando è possibile acquisire immagini di oggetti inanimati, detti nel gergo phantom o fantocci. Non ha ovviamente significato su immagini acquisite da pazienti dove è impossibile avere due immagini esattamente identiche a causa del movimento del soggetto.\n\nUn altro metodo automatico utilizzabile è quello di estrarre dall’immagine una serie di regioni, ad esempio quadrati di 16x16 pixel. Estratte le regioni, si calcola la SD e si ordinano le regioni per SD crescente. Eliminate le regioni con SD più alto, che sono quelle dove sono presenti più tessuti, il valore di SD rimasto è rappresentativo del rumore.\n\nQuando si effettua una misura su fantoccio è garantita l’omogeneità del tessuto su cui viene effettuata la misura. Quando invece si esaminano immagini di un paziente reale, un problema ulteriore è dato dal fatto che i tessuti di cui si fa l’imaging non sono omogenei come nel modello ideale, ma presentano una struttura (fibre muscolari, etc.). Quindi esiste una variabilità non dovuta al rumore di acquisizione ma alla struttura stessa dell’oggetto (il cosiddetto rumore biologico). Per ovviare a questo inconveniente il rumore può essere stimato sullo sfondo dell’immagine, dove non è presente nessun tessuto. Infatti se vale l’assunzione:I(x,y)=I_{0}(x,y)+n(x,y)\n\nLa SD del rumore è la stessa su tutti i tessuti e quindi anche sul fondo, dove tra l’altro è possibile tracciare una ROI di dimensioni tali da garantire una buona stima dell’SD. In molti casi quando la misura dell’SNR e del CNR viene effettuata su immagini reali si utilizzano quindi le formule:SNR = \\frac{M_{i}}{\\sigma_{BK}}\n\neCNR = \\frac{|M_{i}-M_{j}|}{\\sigma_{BK}}\n\nDove il valor medio è misurato su una ROI nel tessuto di interesse e la SD su una ROI tracciata nel fondo dell’immagine.  Questa tecnica è molto apprezzata dai produttori di dispositivi di acquisizione in quanto consente di ottenere valori di SNR e CNR superiori a quelli reali “visti” dall’operatore come si vedrà nel seguito.","type":"content","url":"/c11-imm-biom#snr-e-cnr","position":35},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl4":"Nota sulla natura del rumore in MRI","lvl3":"SNR e CNR","lvl2":"Qualità dell’immagine biomedica"},"type":"lvl4","url":"/c11-imm-biom#nota-sulla-natura-del-rumore-in-mri","position":36},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl4":"Nota sulla natura del rumore in MRI","lvl3":"SNR e CNR","lvl2":"Qualità dell’immagine biomedica"},"content":"È importante notare come misurare la SD nel fondo dell’immagine abbia senso se e solo se il rumore è di tipo additivo. Questo non è il caso di molte modalità di imaging. Ad esempio, nella MRI le immagini sono ricostruite dal K-spazio, che rappresenta il dominio della frequenza. La parte reale ed immaginaria del K-spazio rappresentano i due canali (fase e quadratura) attraverso i quali viene demodulato il segnale. Il rumore di acquisizione si inserisce al livello dei due canali e viene modellato come gaussiano bianco con SD uguale sui due canali. Per riportare le immagini nel dominio dello spazio, si calcola la trasformata di Fourier inversa del K-spazio ottenendo una immagine complessa c(k,l):c(k,l) = p(k,l) + n_p(k,l) + j \\bigl[ q(k,l) + n_q(k,l) \\bigr]\n\ndove p e q sono la parte reale e immaginaria dell’immagine ideale e np e nq rappresentano il rumore introdotto sui due canali.\nIl modulo di c(k,l), che rappresenta l’immagine MRI comunemente usata, è dato da:z(k,l) = \\lvert c(k,l) \\rvert\n       = \\sqrt{\\bigl(p(k,l) + n_p(k,l)\\bigr)^2\n               + \\bigl(q(k,l) + n_q(k,l)\\bigr)^2}\n\nLa non linearità introdotta dall’operazione di modulo cambia la distribuzione del rumore che diviene di tipo Riciano. Il rumore Riciano non è più additivo, ma moltiplicativo, e quindi il rumore sarà diverso in regioni diverse dell’immagine.\nSe il valore dell’immagine è molto più alto del rumore, si può scrivere trascurando i termini di rumore al quadrato:\\begin{aligned}\nz(k,l) &= m(k,l) + \\frac{p(k,l)\\, n_p(k,l) + q(k,l)\\, n_q(k,l)}{m(k,l)} \\\\\n       &= m(k,l) + n_z(k,l)\n\\end{aligned}\n\ncon\\begin{aligned}\nm(k,l) &= \\sqrt{p(k,l)^2 + q(k,l)^2}, \\\\\nn_z(k,l) &= \\frac{p(k,l)\\, n_p(k,l) + q(k,l)\\, n_q(k,l)}{m(k,l)} .\n\\end{aligned}\n\nRiconducendoci al caso di m immagine di modulo ideale e nz rumore additivo.\nLa SD del rumore sarà:\\sigma_z\n= \\sqrt{\\frac{p(k,l)^2\\,\\sigma_n^2 + q(k,l)^2\\,\\sigma_n^2}{m(k,l)^2}}\n= \\sigma_n\n\nQuindi in regioni di elevata intensità di segnale il rumore si può considerare gaussiano con SD uguale alla SD sui due canali.\nNelle regioni a bassa intensità, come lo sfondo dell’immagine, la distribuzione Riciana è simile alla distribuzione di Rayleigh. Per lo sfondo (segnale nullo) abbiamo:z_B(k,l) = \\sqrt{n_p(k,l)^2 + n_q(k,l)^2}\\sigma_B^2 = \\left( 2 - \\frac{\\pi}{2} \\right) \\sigma_n^2,\n\\qquad\n\\sigma_n = 1.526\\, \\sigma_B .\n\nQuindi nelle immagini MRI, se misuriamo il rumore sullo sfondo otterremo una sottostima del rumore di un fattore 1.526. Questo in realtà è vero per una bobina a singolo canale, per bobine multicanale il fattore di conversione cambia leggerment [1]. Nel caso di immagini MR, la formula corretta da utilizzare sarà quindi:SNR = \\frac{M_{i}}{1.526\\sigma_{BK}}\n\neCNR = \\frac{|M_{i}-M_{j}|}{1.526\\sigma_{BK}}\n\nDove il valor medio è sempre misurato su una ROI nel tessuto di interesse e la SD su una ROI tracciata nel fondo dell’immagine. Questa tecnica è un po’ meno apprezzata dai produttori di dispositivi di acquisizione in quanto ottiene valori di SNR e CNR inferiori rispetto all’assunzione di rumore additivo. In conclusione, nella valutazione del rumore associato ad una immagine biomedica è importante tener conto della fisica dell’acquisizione per effettuare una valutazione corretta. Nel confrontare misure su dispositivi diversi, è importante conoscere che procedura è stata utilizzata per la misura di SNR e CNR.","type":"content","url":"/c11-imm-biom#nota-sulla-natura-del-rumore-in-mri","position":37},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Just Noticeable Difference (JND)","lvl2":"Qualità dell’immagine biomedica"},"type":"lvl3","url":"/c11-imm-biom#just-noticeable-difference-jnd","position":38},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Just Noticeable Difference (JND)","lvl2":"Qualità dell’immagine biomedica"},"content":"L’indice JND indica il valore di incremento relativo del segnale rispetto allo sfondo dopo il quale un oggetto diventa visibile. Si ha quindi JND = (F-B)/B dove B è il segnale dello sfondo e F è il segnale dell’oggetto. E’ stato dimostrato (Weber law) che il valore di JND è largamente indipendente dal valore di B è vale intorno al 2%. Questa assunzione vale nel caso di assenza di rumore, nei casi reali F e B saranno ovviamente delle stime ottenute come media del segnale su una ROI e il valore di JND utile per distinguere i due tessuti sarà più alto.\n\nIn Figura 1.25 è riportato un tipico esperimento per la valutazione del JND, la prima barra ha un JND del 3% (tratto da R Rangayyan, Biomedical Image Analysis, CRC Press 2004). Alcuni fantocci TAC implementano il test JND includendo una serie di inserti con valori JND crescenti. Il test di qualità fatto con tali fantocci prevede il riconoscimento del numero massimo possibile di inserti da parte dell’operatore. I risultati del test JND hanno evidentemente un certo grado di soggettività dipendendo dall’acume visivo dell’operatore, dal monitor usato, dall’illuminazione ambientale, etc.\n\nFigura 1.25. JND.","type":"content","url":"/c11-imm-biom#just-noticeable-difference-jnd","position":39},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Analisi dei profili","lvl2":"Qualità dell’immagine biomedica"},"type":"lvl3","url":"/c11-imm-biom#analisi-dei-profili","position":40},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Analisi dei profili","lvl2":"Qualità dell’immagine biomedica"},"content":"Come introdotto nel modello dell’immagine biomedica, un fattore importante nella valutazione della qualità di immagine è la minimizzazione dell’effetto di smoothing dei bordi dovuto all’effetto volume parziale (partial volume effect, PVE). Per valutare il PVE è possibile estrarre dall’immagine un profilo, che è il grafico dell’intensità di segnale rispetto ad una certa direzione. Per quanto sia possibile estrarre profili anche lungo direzioni oblique, solitamente per evitare problemi dovuti all’interpolazione si preferisce estrarre i profili lungo le coordinate principali (x,y, o z).Consideriamo la Figura 1.26 dove viene riportata un’immagine costituita da uno sfondo di valore 50 con all’interno un quadrato di valore 255. Se tracciamo un profilo lungo x avremo un grafico che presenta due transizioni istantanee. Se applichiamo una operazione di smoothing (che simula il PVE), il profilo apparirà smussato rispetto al caso ideale. Per un PVE ancora maggiore l’effetto di smoothing del profilo si incrementa ulteriormente. E’ quindi ragionevole utilizzare la velocità di transizione del segnale tra due regioni diverse valutata su uno o più profili come indice di qualità dell’immagine. Più rapida la transizione, minore il PVE e migliore la qualità di immagine.\n\nFigura 1.26. Profili per diversi valori di PVE.\n\nIntuitivamente, un parametro di qualità dovrebbe essere correlato all’intervallo spaziale (b-a) in cui avviene la transizione e all’altezza del gradino f(b)-f(a) (Figura 1.27). Un possibile parametro è quindi il valore del gradiente dell’immagine nella direzione x definito come:G_{x}=\\frac{f(b)-f(a)}{b-a}\n\nUn gradiente maggiore corrisponde a contorni meglio definiti e quindi ad una immagine di migliore qualità.\n\nFigura 1.27. Stima del profilo (tratto da R Rangayyan, Biomedical Image Analysis, CRC Press 2004)\n\nÈ stato verificato da Higgins e Jones che il gradiente non è ben correlato con la percezione visiva di un osservatore umano, e quindi può non essere adatto ad una valutazione di qualità delle immagini biomediche. E’ stato quindi proposto un parametro detto acutezza (acutance) definito come:A = \\frac{1}{f(b) - f(a)} \\int_{a}^{b}\n\\left[ \\frac{d}{dx} f(x) \\right]^2 \\, dx\n\nche meglio correla con la percezione visiva della definizione dei contorni. In ogni caso le misure devono essere ripetute in varie direzioni e in varie interfacce tra tessuti per essere significative.\nMentre per una immagine ideale priva di rumore è facile definire a e b come i punti a sinistra e a destra della transizione (ad esempio se i due tessuti hanno valore 50 e 200 b sarà il punto in cui f assume il valore 200 ed a il punto in cui f assume il valore 50), nelle immagini reali affette da rumore sarà necessario definire una soglia per stabilire l’inizio e la fine della transizione.\n\nFortemente connesso al concetto di acutezza è il concetto di Point Spread Function (PSF) riportata in Figura 1.28. La PSF rappresenta l’immagine reale di un punto singolo, ed è quindi in qualche modo la funzione di trasferimento del processo di formazione reale dell’immagine.\n\n*Figura. 1. 28. Point spread function (Tratto da R Rangayyan, Biomedical Image Analysis, CRC Press 2004). *\n\nDa un punto di vista matematico se definiamo un punto ideale nell’immagine ideale come un delta di Dirac, l’immagine reale in uscita dal sistema di imaging sarà la PSF del sistema.\nSe il sistema è lineare e invariante rispetto alla spazio (sistema LSI), l’immagine reale in uscita dal sistema sarà g = PSF * f dove f è l’immagine reale e * rappresenta la convoluzione spaziale.Come tipicamente rappresentato nella figura, la PSF è di solito una funzione gaussiana 2D, il che giustifica l’uso della convoluzione con una gaussiana nel modello dell’immagine biomedica per modellare il PVE. In via di principio è possibile fare l’imaging di un oggetto puntiforme (in realtà per evidenti motivi fisici un oggetto di dimensioni molto piccole) e tracciare i profili della PSF risultante che saranno delle funzioni gaussiane. La larghezza di tali profili (espressa ad esempio come SD della gaussiana) sarà un misura della bontà del sistema di imaging. E’ evidente che questo metodo richiede l’uso di un fantoccio standardizzato o di una procedura di imaging standardizzata per ottenere la PSF.\n\nInfine un profilo tracciato sull’immagine di un fantoccio omogeneo che riempia tutto o buona parte del campo visivo (FOV) del dispositivo può permettere di valutare il campo di attenuazione g introdotto nel modello di immagine biomedica. Nel caso di g unitario (quindi assenza di attenuazione) il profilo dovrebbe risultare perfettamente orizzontale, mentre per g < 1 compare una curvatura del profilo, tanto maggiore quanto maggiore è l’attenuazione.","type":"content","url":"/c11-imm-biom#analisi-dei-profili","position":41},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Artefatti","lvl2":"Qualità dell’immagine biomedica"},"type":"lvl3","url":"/c11-imm-biom#artefatti","position":42},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl3":"Artefatti","lvl2":"Qualità dell’immagine biomedica"},"content":"Le immagini biomediche possono essere corrotte da artefatti. Gli artefatti sono strutture riconoscibili, quindi con una coerenza interna, che appaiono sull’immagine anche se non esistono nell’oggetto reale. Il rumore è invece un disturbo completamente casuale, privo di struttura e distribuito uniformemente sull’immagine. La figura mostra due esempi di artefatti in immagini MRI. Gli artefatti sono tipicamente non predicibili e vengono valutati attraverso la probabilità della loro comparsa su lunghe serie di acquisizioni. Alcuni tipi di artefatto possono essere dovuti a motivi fisiologici, si pensi all’effetto del respiro del paziente o alle irregolarità del segnale ECG in acquisizioni cardiache sincronizzate con l’ECG stesso.\n\nFigura 1.29. Esempi di artefatto.","type":"content","url":"/c11-imm-biom#artefatti","position":43},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl2":"Software per l’elaborazione di immagini biomediche"},"type":"lvl2","url":"/c11-imm-biom#software-per-lelaborazione-di-immagini-biomediche","position":44},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl2":"Software per l’elaborazione di immagini biomediche"},"content":"L’elaborazione delle immagini biomediche in ambito clinico deve avvenire, come prima descritto, attraverso software certificato. Esistono numerosi software commerciali, sia prodotti dalle case costruttrici dei dispositivi di acquisizione che da ditte indipendenti dalle stesse. Nel primo caso il software può essere integrato nella macchina di acquisizione (in questo caso la certificazione si opera sull’intera apparecchiatura) o “stand alone”, quindi installato su un computer (nel gergo dei sistemi PACS workstation). Nel secondo caso il software è necessariamente stand alone.\nUn software stand alone residente su di una workstation deve integrarsi con il sistema PACS su cui risiedono le immagini e quindi è tipicamente composto da una interfaccia di base che consente di scaricare o importare immagini in formato DICOM ed inserirle in un archivio interno. Da tale archivio le immagini possono essere estratte per l’elaborazione. I risultati dell’elaborazione vengono salvati sulla workstation. I software clinici sono tipicamente molto costosi (decine di migliaia di euro) e vengono di solito distribuiti con un sistema di licenze annuali che includono l’aggiornamento ed il supporto.\n\nPer scopi di ricerca o di studio sono disponibili vari software stand alone gratuiti (chiaramente non certificati) che possono essere utilizzati per l’archiviazione e l’elaborazione di immagini biomediche.\n\nAlcuni esempi sono:\n\nimageJ (\n\nhttps://​imagej​.net​/ij​/index​.html). E’ un software multipiattaforma (Open Source, Java) sviluppato dall’ NIH (National Institutes of Health, USA). E’ un software essenziale che permette di importare immagini in formato DICOM ed eseguire numerosi tipi di elaborazione. E’ completato da una collezione di plugins (\n\nhttps://​imagej​.nih​.gov​/ij​/plugins/), cioè applicazioni sviluppate da utenti del software, che implementano vari algoritmi di elaborazione. Essendo il software open source, è possibile implementare nuovi plugins o modificare quelli esistenti.\n\nMIPAV (\n\nhttps://​mipav​.cit​.nih​.gov/) E’ un software multipiattaforma (Java) sviluppato dall’ NIH (National Institutes of Health, USA).  E’ un software che permette di importare immagini in formato DICOM ed eseguire numerosi tipi di elaborazione maggiormente evoluto rispetto ad imageJ. Il software non è Open Source ma permette di sviluppare plugins integrabili con il MIPAV.\n\n3D Slicer (\n\nhttps://​www​.slicer​.org/). E’ un software multipiattaforma (C++, Tcl/Tk). E’ un software che permette di importare immagini in formato DICOM ed eseguire numerosi tipi di elaborazione. Rispetto a imageJ e MIPAV permette di elaborare efficacemente dati di tipo volumetrico. Il software è Open Source e permette di sviluppare plugins.\n\nHoros (\n\nhttps://​horosproject​.org/). E’ un software free/open source solo su piattaforma MacOS che implementa la struttura di una vera e propria workstation radiologica, quindi permette di sperimentare un ambiente simile a quello dei prodotti clinici. E’ la versione free di Osirix (\n\nhttps://​www​.osirix​-viewer​.com/) che implementa una versione certificata ad uso clinico.\n\nEsistono molti altri software che consentono la gestione di file DICOM e implementano algoritmi di elaborazione delle bioimmagini. Una lista è disponibile su \n\nhttps://​idoimaging​.com/.\nL’elaborazione delle immagini biomediche può essere naturalmente eseguita sviluppando i propri algoritmi nei vari ambienti di programmazione. In questo corso utilizzeremo l’ambiente MATLAB, un ambiente commerciale che consente di importare immagini DICOM e di utilizzare su queste le varie funzioni di elaborazione disponibili.\nIn alternativa è possibile utilizzare ambienti free come python (\n\nhttps://​www​.python​.org/), o qualsiasi linguaggio di programmazione che possieda librerie per la gestione del formato DICOM. In python è anche disponibile lo strumento ParaView (\n\nhttps://​www​.paraview​.org/), particolarmente utile nella gestione di mesh tridimensionali. In python sono sviluppati i principali ambienti per l’elaborazione di immagini biomediche attraverso reti neurali (CNN) come Keras e Pytorch.","type":"content","url":"/c11-imm-biom#software-per-lelaborazione-di-immagini-biomediche","position":45},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl2":"Bibliografia"},"type":"lvl2","url":"/c11-imm-biom#bibliografia","position":46},{"hierarchy":{"lvl1":"Capitolo 1: Modello dell’immagine biomedica e misure di qualità","lvl2":"Bibliografia"},"content":"Constantinides CD, Atalar E, McVeigh ER. Signal-to-noise measurements in magnitude images from NMR phased arrays. Magnetic Resonance in Medicine 1997;38(5):852–857.\n\nR Rangayyan, Biomedical Image Analysis, CRC Press 2004","type":"content","url":"/c11-imm-biom#bibliografia","position":47},{"hierarchy":{"lvl1":"Esempio degli effetti di corruzione dell’immagine - Notebook 1.1"},"type":"lvl1","url":"/n-1-1-effetti-corr-imm-entr","position":0},{"hierarchy":{"lvl1":"Esempio degli effetti di corruzione dell’immagine - Notebook 1.1"},"content":"Import delle librerie necessarie per la simulazione\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage import convolve\nfrom scipy.signal.windows import gaussian\n\n\n\n# Creo il fantoccio sul quale simulare gli effetti di PVE, attenuazione e rumore\ndim = 512\nimage = np.zeros((dim, dim), dtype=np.float32)\n\nimage[:, :] = 50\nimage[49:100, 49:100] = 120\nimage[100:180, 100:450] = 200\nimage[199:500, 199:350] = 90\nimage[229:270, 229:270] = 250\nimage[4:400, 449:500] = 150\n\n\n\nVisualizzazione + istogramma\n\nfig, ax = plt.subplots(1, 2, figsize=(9, 4))\n\nax[0].imshow(image, cmap=\"gray\")\nax[0].set_title(\"Immagine ideale\")\nax[0].axis(\"off\")\n\nax[1].hist(image.ravel(), bins=50)\nax[1].set_title(\"Istogramma\")\n\nplt.show()\n\n\n\nEffetto PVE\n\n# filtro convolutivo gaussiano per simulare l'effetto PVE\nksize = 11\nsigma = 5\n\ng1d = gaussian(ksize, sigma)\nh = np.outer(g1d, g1d)\nh /= h.sum()\n\nimage_pve = convolve(image, h, mode=\"reflect\")\n\n# visualizzazione\nfig, ax = plt.subplots(1, 2, figsize=(9, 4))\n\nax[0].imshow(image_pve, cmap=\"gray\")\nax[0].set_title(\"Immagine con PVE\")\nax[0].axis(\"off\")\n\nax[1].hist(image_pve.ravel(), bins=50)\nax[1].set_title(\"Istogramma\")\n\nplt.show()\n\n\n\nEffetto attenuazione (campo parabolico)\n\n# simulazione dell'effetto di attenuazione\nx, y = np.meshgrid(np.arange(dim), np.arange(dim))\nxc = dim / 2\nyc = dim / 2\n\nsigma_att = dim / 2\nattenuation_field = 1 - ((x - xc)**2) / (2 * sigma_att**2)\nattenuation_field = np.maximum(attenuation_field, 0)\n\nimage_att = image_pve * attenuation_field\n\n# visualizzazione\nfig, ax = plt.subplots(1, 2, figsize=(9, 4))\n\nax[0].imshow(image_att, cmap=\"gray\")\nax[0].set_title(\"Immagine attenuata\")\nax[0].axis(\"off\")\n\nax[1].hist(image_att.ravel(), bins=50)\nax[1].set_title(\"Istogramma\")\n\nplt.show()\n\n\n\n\nRumore additivo Gaussiano\n\n# simulazione del rumore\nnp.random.seed(0)  # riproducibilità (opzionale)\nimage_final = image_att + 5 * np.random.randn(dim, dim)\n\n# visualizzazione\nfig, ax = plt.subplots(1, 2, figsize=(9, 4))\n\nax[0].imshow(image_final, cmap=\"gray\")\nax[0].set_title(\"Immagine finale (rumore)\")\nax[0].axis(\"off\")\n\nax[1].hist(image_final.ravel(), bins=50)\nax[1].set_title(\"Istogramma\")\n\nplt.show()\n\n\n\n\nImmagine differenza\n\ndif = image - image_final\n\nfig, ax = plt.subplots(1, 2, figsize=(9, 4))\n\nax[0].imshow(dif, cmap=\"gray\")\nax[0].set_title(\"Immagine differenza\")\nax[0].axis(\"off\")\n\nax[1].hist(dif.ravel(), bins=50)\nax[1].set_title(\"Istogramma\")\n\nplt.show()\n\n\n","type":"content","url":"/n-1-1-effetti-corr-imm-entr","position":1},{"hierarchy":{"lvl1":"Stima dell’entropia per diversi tipi di istogramma - Notebook 1.2"},"type":"lvl1","url":"/n-1-2-entropia","position":0},{"hierarchy":{"lvl1":"Stima dell’entropia per diversi tipi di istogramma - Notebook 1.2"},"content":"Import delle librerie necessarie alla simulazione\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom skimage.measure import shannon_entropy\nfrom scipy.ndimage import convolve\nfrom scipy.signal.windows import gaussian\n\n\n\nRumore uniformemente distribuito\n\nim = np.random.rand(512, 512)\n\nfig, ax = plt.subplots(1, 2, figsize=(9, 4), constrained_layout=True)\nax[0].imshow(im, cmap=\"gray\", vmin=0, vmax=1)\nax[0].axis(\"off\")\nax[1].hist(im.ravel(), bins=50)\nax[1].set_title(f\"entropia = {shannon_entropy(im):.4f}\")\nplt.show()\n\n\n\n\nImmagine a singolo livello di grigio\n\nim = 255 * np.ones((512, 512), dtype=float)\n\nfig, ax = plt.subplots(1, 2, figsize=(9, 4), constrained_layout=True)\nax[0].imshow(im, cmap=\"gray\", vmin=0, vmax=255)\nax[0].axis(\"off\")\nax[1].hist(im.ravel(), bins=50)\nax[1].set_xlim(240, 260)\nax[1].set_title(f\"entropia = {shannon_entropy(im):.4f}\")\nplt.show()\n\n\n\n\nImmagine con 2 livelli di grigio\n\nim = np.zeros((512, 512), dtype=float)\nim[49:100, 49:100] = 100 \n\nfig, ax = plt.subplots(1, 2, figsize=(9, 4), constrained_layout=True)\nax[0].imshow(im, cmap=\"gray\", vmin=0, vmax=255)\nax[0].axis(\"off\")\nax[1].hist(im.ravel(), bins=50)\nax[1].set_title(f\"entropia = {shannon_entropy(im):.4f}\")\nplt.show()\n\n\n\n\nImmagine con due livelli di grigio e rumore Gaussiano\n\nim = np.zeros((512, 512), dtype=float)\nim[49:100, 49:100] = 100\nim = im + 10 * np.random.randn(512, 512)\n\nfig, ax = plt.subplots(1, 2, figsize=(9, 4), constrained_layout=True)\nax[0].imshow(im, cmap=\"gray\")  \nax[0].axis(\"off\")\nax[1].hist(im.ravel(), bins=80)\nax[1].set_title(f\"entropia = {shannon_entropy(im):.4f}\")\nplt.show()\n\n\n\n\nImmagine con due livelli di grigio, rumore Gaussiano e PVE\n\nim = np.zeros((512, 512), dtype=float)\nim[49:100, 49:100] = 100\n\nksize = 25\nsigma = 11\n\ng = gaussian(ksize, sigma)\nh = np.outer(g, g)\nh = h / h.sum()\n\nim = convolve(im, h, mode=\"reflect\")\n\nfig, ax = plt.subplots(1, 2, figsize=(9, 4), constrained_layout=True)\nax[0].imshow(im, cmap=\"gray\")\nax[0].axis(\"off\")\nax[1].hist(im.ravel(), bins=80)\nax[1].set_title(f\"entropia = {shannon_entropy(im):.4f}\")\nplt.show()\n\n\n","type":"content","url":"/n-1-2-entropia","position":1},{"hierarchy":{"lvl1":"Esempio di rumore riciano in MRI - Notebook 1.3"},"type":"lvl1","url":"/n-1-3-mr-noise-example","position":0},{"hierarchy":{"lvl1":"Esempio di rumore riciano in MRI - Notebook 1.3"},"content":"Import delle librerie necessarie per la simulazione.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\nSimuliamo il segnale proveniente dallo sfondo come Gaussiano a media nulla. Simuliamo l’acquisizione su due canali (fase e quadratura) che rappresenteranno la parte reale e la parte immaginaria del segnale acquisito. Visualizziamo quindi il segnale su ogni canale e sulla loro somma quadratica.\n\n\n########## sfondo ############\nsigma = 10\ndim = 256\n\nimageR = np.random.randn(dim,dim)\nimageI = np.random.randn(dim,dim)\nimageM = np.sqrt(np.square(imageR)+np.square(imageI))\n\ncountsR, binsR = np.histogram(imageR, bins = 30)\ncountsI, binsI = np.histogram(imageI, bins = 30)\ncountsM, binsM = np.histogram(imageM, bins = 30)\n\nfig, axes = plt.subplots(1,3)\naxes[0].hist(binsR[:-1], binsR, weights=countsR)\naxes[1].hist(binsI[:-1], binsI, weights=countsI)\naxes[2].hist(binsM[:-1], binsM, weights=countsM)\n\nplt.show()\n\n\n\nOra invece, simuliamo il segnale proveniente da un tessuto con S = 100. Simuliamo l’acquisizione su due canali (fase e quadratura) che rappresenteranno la parte reale e la parte immaginaria del segnale acquisito. Visualizziamo quindi il segnale su ogni canale e sulla loro somma quadratica.\n\n########## immagine ############\nsigma = 10\ndim = 256\n\nimageR = 100 + np.random.randn(dim,dim)\nimageI = 100 + np.random.randn(dim,dim)\nimageM = 100 + np.sqrt(np.square(imageR)+np.square(imageI))\n\ncountsR, binsR = np.histogram(imageR, bins = 30)\ncountsI, binsI = np.histogram(imageI, bins = 30)\ncountsM, binsM = np.histogram(imageM, bins = 30)\n\nfig, axes = plt.subplots(1,3)\naxes[0].hist(binsR[:-1], binsR, weights=countsR)\naxes[1].hist(binsI[:-1], binsI, weights=countsI)\naxes[2].hist(binsM[:-1], binsM, weights=countsM)\n\nplt.show()\n\n\n\nE’ possibile notare dal confronto degli istogrammi che il segnale sullo sfondo non segue una distribuzione Gaussiana, come invece avviene per il segnale proveniente dal tessuto.","type":"content","url":"/n-1-3-mr-noise-example","position":1},{"hierarchy":{"lvl1":"Esercitazione 1: stima dei parametri di qualità di un’immagine biomedica"},"type":"lvl1","url":"/z-1-1-esercitazione","position":0},{"hierarchy":{"lvl1":"Esercitazione 1: stima dei parametri di qualità di un’immagine biomedica"},"content":"","type":"content","url":"/z-1-1-esercitazione","position":1},{"hierarchy":{"lvl1":"Esercitazione 1: stima dei parametri di qualità di un’immagine biomedica","lvl2":"Teoria"},"type":"lvl2","url":"/z-1-1-esercitazione#teoria","position":2},{"hierarchy":{"lvl1":"Esercitazione 1: stima dei parametri di qualità di un’immagine biomedica","lvl2":"Teoria"},"content":"La misura del rumore associato ad una immagine biomedica è un punto fondamentale in molte applicazioni, come l’analisi della qualità di immagine e la realizzazione di filtri adattivi. Per la misura del rumore sono stati sviluppati vari approcci, che possono essere divisi in due categorie:\n\nBasati sul tracciamento di ROI\n\nBasati sull’analisi dell’istogramma\n\nNei metodi basati su ROI, una o più ROI vengono tracciate sui tessuti di interesse, definendo delle zone omogenee ed evitando che la misura sia influenzata dal PVE o dalla presenza di attenuazione. Il rumore sul tessuto viene stimato come la deviazione standard (SD) del segnale sulla ROI. Le ROI possono essere definite manualmente o automaticamente attraverso algoritmi di segmentazione che saranno illustrati nel seguito. La definizione automatica è molto semplificata nel caso di fantocci per la misura della qualità dell’immagine di cui è nota la forma e posizione nello scanner.\n\nNella misura attraverso ROI il punto fondamentale è il compromesso tra grandezza della ROI e necessità di posizionare la ROI in una regione omogenea.\n\nIn Figura 1.30 osserviamo le misure di Media e SD del segnale su ROI di dimensioni crescenti (100 realizzazioni di rumore, ROI quadrate). Si osserva chiaramente come la misura della SD sia più critica rispetto a quella del valor medio e come per la SD siano necessarie ROI con qualche decina di pixel per avere una misura affidabile. Da queste considerazioni derivano gli approcci a sottrazione di immagini e misura del rumore sul fondo utilizzati nella pratica clinica con lo scopo di aumentare il numero di pixel disponibili per la stima della SD.\n\nFigura 1.30. Stima di Media e SD su ROI al variare della dimensione della ROI.\n\nConsideriamo ad esempio il caso in Figura 1.31 di immagini cardiache in LGE (Late Gadolinium Enhancement). Lo scopo di questo tipo di immagini con contrasto è verificare l’estensione dell’infarto miocardio cronico, dove si deposita il contrasto, che appare bianco, mentre il miocardio sano appare con segnale annullato (in realtà avremo sempre un segnale diverso da zero per la natura del rumore MR). Per valutare la validità clinica delle immagini bisogna valutare il CNR tra la zona di LGE (cicatrice da infarto, ROI 1) ed il miocardio sano (ROI 2) (Figura 1.32).\n\nFigura 1.31. Immagini cardiache in LGE.\n\nFigura 1.32. Tracciamento di ROI su immagini cardiache.\n\nCome si vede le regioni in cui tracciare le ROI sono molto piccole, l’area LGE è di 60 mm^2 e l’area miocardio di 140 mm^2, corrispondenti (pixel size = 1.5 mm) a 26 e 60 pixel circa. La misura della SD sarà quindi inaffidabile. E’ molto più affidabile una misura fatta nello sfondo che permette di tracciare una ROI di 3350 mm^2 (1550 pixel circa). In questo caso essendo il miocardio annullato (segnale zero) e quindi con distribuzione di rumore simile al fondo il valore di CNR sarà:CNR=2(M_{LGE}-M_{MIO})/(SD_{BK}+1.526S_{BK})\n\nI metodi basati sull’analisi dell’istogramma cercano invece di caratterizzare il rumore dall’immagine nel suo complesso. In questo caso si utilizza tutta l’informazione contenuta nell’immagine per cui il problema visto nelle ROI non si pone. Consideriamo l’immagine ideale a sei pattern corrotta solo da rumore gaussiano ed il suo istogramma riportati in Figura 1.33.\n\nFigura 1.33. Fantoccio a 6 pattern con rumore Gaussiano e relativo istogramma.\n\nSe tracciamo una ROI all’interno di un pattern la SD del segnale nella ROI sarà sempre la stessa dipendendo dal rumore sommato all’immagine. Se invece tracciamo una ROI a cavallo di due pattern misureremo un valore più alto dovuto alla presenza di due tessuti con segnali diversi all’interno della ROI.\n\nFigura 1.34. Mappa di SD e relativo istogramma.\n\nAutomatizzando la procedura, possiamo far scorrere sull’immagine un kernel (ad esempio 5x5) che rappresenta la ROI e valutare il valore della deviazione standard SD sulla regione di immagine coperta dal kernel. In MATLAB questa operazione è implementata dalla funzione stdfilt. Si otterranno quindi un numero di valori di SD uguale al numero di pixel dell’immagine. Nelle regioni all’interno dei pattern uniformi il valore della SD sarà pari alla deviazione standard del rumore σ, almeno nel caso di rumore gaussiano additivo. Nelle regioni a cavallo di due pattern il valore di SD sarà maggiore di σ. Come al solito la dimensione del kernel rappresenterà un compromesso tra l’esigenza di calcolare correttamente la SD e la necessità di avere la maggior parte delle misure eseguita su regioni omogenee.\nCome si vede dalla figura, l’istogramma della mappa SD avrà un picco in corrispondenza del valore di \\sigma ed una serie di valori più alti corrispondenti alle transizioni distribuiti in modo uniforme.\nSe l’immagine è composta prevalentemente da regioni omogenee (pochi pattern di forma regolare) si può supporre che il contributo delle regioni di transizione sia trascurabile e calcolare \\sigma come media della mappa:\\sigma = mean(M_{SD})\n\nIn realtà è preferibile eliminare gli outliers dovuti ai bordi e computare la mediana della mappa SD invece che la media, cosa che consente di ottenere una stima più accurata. Infatti, al contrario della media, la mediana pesa meno gli alti valori di SD nella parte destra dell’istogramma.\\sigma = median(M_{SD})\n\nSe l’immagine presenta molte transizioni il metodo della mediana può dare risultati non corretti. In questo caso è possibile utilizzare l’istogramma calcolando il valore di σ dal picco principale (tipicamente il primo) dell’istogramma che come visto contiene l’informazione sulle regioni omogenee. Il valor medio di σ misurato sarà dato dalla posizione sull’asse x del massimo valore dell’istogramma h. Cercheremo quindi il massimo dell’istogramma e porremo \\sigma uguale al valore di x corrispondente a tale massimo:\\sigma = x:h(x)=max(h)\n\nIn Figura 1.35 è riportata la stima della SD per i diversi metodi.\n\nFigura 1.35. Valori stimati di SD per diversi metodi.\n\nQuesto approccio è simile ad un altro metodo utilizzato per la stima del rumore nel quale l’immagine viene divisa in N quadrati di lato k non sovrapposti di cui viene calcolata la SD, ed il valore di \\sigma viene stimato come media degli m campioni con valore di SD più basso (al limite un singolo campione). Il metodo basato sull’istogramma è comunque in generale più accurato.\nCome si osserva dal grafico il metodo Mean sovrastima il valore del rumore, mentre gli altri due metodi danno una stima sostanzialmente corretta.\nTutti questi metodi si basano sull’assunzione di rumore gaussiano additivo. Se come avviene tipicamente nelle immagini biomediche questa assunzione non è verificata, bisogna operare sulla base della conoscenza del processo di acquisizione. Consideriamo ad esempio l’immagine MR di un phantom cilindrico, composto da tre cilindri concentrici, due riempiti con acqua e quello intermedio con olio (valore di segnale più alto). Essendo il fantoccio riempito con liquido perfettamente omogeneo il rumore biologico è trascurabile. Le dimensioni del fantoccio sono piccole rispetto al bore della macchina MR (15 cm circa) e quindi si può considerare nulla l’attenuazione. Come si osserva nell’immagine a destra ottenuta con una opportuna finestra di windowing, alcuni pixel dell’immagine non sono stati ricostruiti dal K-spazio ma rappresentano un “riempimento” per ottenere una immagine quadrata (zero-padding). A tali pixel in MR viene assegnato il valore convenzionale 0 e devono essere ignorati nell’elaborazione (Figura 1.36).\n\nFigura 1.36. Zone di zero-padding evidenziate.\n\nCome sappiamo il valore di \\sigma può essere stimato tracciando una ROI su un tessuto omogeneo oppure sul fondo introducendo un opportuno fattore di correzione che è noto trattandosi di una immagine MR.\nProcedendo in questo modo otteniamo i valori di \\sigma per acqua e olio (uguali a meno dell’errore sperimentale) e del fondo che come ci aspettavamo è più basso. Dal valore stimato sul fondo possiamo ottenere la stima corretta applicando il fattore di conversione 1.526 (Figura 1.37).\n\nFigura 1.37. Stima di SD basato su ROI.\n\nSe vogliamo applicare i metodi automatici basati sull’istogramma dobbiamo calcolare la mappa SD ed il relativo istogramma, dove abbiamo eliminato dal computo i pixel a valore nullo (zero padding) (Figura 1.38).\n\nFigura 1.38. Stima della SD automatica.\n\nCome vediamo dal grafico abbiamo un singolo picco, che è però la combinazione dei valori di SD del fondo e di quelli dei due tessuti ad alto SNR. Il calcolo della media della mappa SD sovrastima fortemente \\sigma, in quanto gli alti valori di SD sui bordi alzano la media in modo significativo. Il valore della mediana è abbastanza simile alla SD del rumore di fondo, che rappresenta una parte rilevante dell’immagine. Considerando il massimo dell’istogramma si ha un valore addirittura minore alla SD del fondo. Tali valori dipendono dal rapporto tra numero di pixel del fondo e numero di pixel ad alto SNR e non sono generalizzabili. I valori di SD per i tre metodi sono disponibili nella seguente tabella:\n\nSD mean\n\nSD median\n\nmax hist\n\n22.3108\n\n10.0968\n\n7.8155\n\nIn questo caso sarà necessario identificare in qualche modo le due classi di pixel attraverso un opportuno algoritmo di segmentazione. Banalmente, se consideriamo solo i pixel con livello di grigio superiore a 100 (quindi solo acqua e olio, vedremo in seguito come ottenere tale soglia) abbiamo come istogramma della SD quello di Figura 1.39.\n\nFigura 1.39. Mappa di deviazione standard e relativo istogramma.\n\nCon le seguenti stime per la SD:\n\nSD mean\n\nSD median\n\nmax hist\n\n69.5255\n\n15.7743\n\n12.3800\n\nCome si osserva la stima con i metodi median e max hist migliora in modo significativo. In definitiva per la valutazione automatica del rumore nell’immagine biomedica occorre una scelta oculata dell’algoritmo sulla base delle caratteristiche dell’immagine.","type":"content","url":"/z-1-1-esercitazione#teoria","position":3},{"hierarchy":{"lvl1":"Esercitazione 1: stima dei parametri di qualità di un’immagine biomedica","lvl2":"Esercitazione"},"type":"lvl2","url":"/z-1-1-esercitazione#esercitazione","position":4},{"hierarchy":{"lvl1":"Esercitazione 1: stima dei parametri di qualità di un’immagine biomedica","lvl2":"Esercitazione"},"content":"Lo scopo dell’esercitazione è replicare le misure di qualità dell’immagine che vengono eseguite in un laboratorio MR in modo routinario. L’immagine a disposizione è quella di un fantoccio sferico utilizzato per la valutazione del rapporto segnale rumore e dell’uniformità di segnale (quindi una valutazione dell’eventuale presenza di attenuazione) (Figura 1.40).\nL’immagine è stata acquisita su una macchina MR Signa HDxt General Electric a 3 Tesla, come si può osservare dall’header DICOM. Il valore del campo PatientID = ‘geservice’ indica che le immagini sono state acquisite per un controllo di qualità. L’immagine è una immagine 2D Fast Spin Echo. Il fantoccio è una sfera con diametro 26 cm acquisita in modo da ottenere la massima sezione. Il FOV dovrebbe essere centrato sul centro della sfera, in realtà è abbastanza disassato ed ha il centro intorno a (233 253).\n\nFigura 1.40. Fantoccio.\n\nPer la definizione del protocollo di misura ci riferiamo al Protocollo definito dalla CONSIP per l’esecuzione dei controlli di qualità su scanner MR. La CONSIP è la centrale acquisti per la pubblica amministrazione italiana, e gestisce quindi anche gli acquisti nella sanità pubblica. In particolare, la CONSIP dovrebbe riuscire a migliorare la qualità degli acquisti e ridurre i costi grazie all’aggregazione della domanda sul tutto il territorio nazionale. Nella pratica con cadenza periodica (tipicamente due anni) la CONSIP bandisce una gara per l’acquisizione di apparecchiature mediche e valuta la qualità delle apparecchiature oltre che il costo proposto dai fornitori partecipanti. In base alla qualità rilevata e al costo viene selezionato un fornitore al quale nei due anni successivi si dovrà rivolgere preferibilmente ogni struttura sanitaria pubblica. Il protocollo, che è pubblico, è disponibile come materiale integrativo del corso.\nPer la misura dell’SNR il protocollo è tipicamente strutturato come:\n\nDefinire sull’immagine una ROI (ROI75) posizionata al centro dell’oggetto test di dimensioni pari al 75% dell’oggetto. Determinare il valor medio del segnale nella ROI75.\n\nDefinire una ROI (ROI10) in una zona priva di segnale (fondo) di dimensioni pari al 10% dell’oggetto. Determinare il valor medio del segnale nella ROI10 (valore di baseline).\n\nDeterminare il segnale S come differenza dei valori di segnale tra ROI75 e ROI10.\n\nValutare il rumore N sull’immagine come la deviazione standard del segnale all’interno della ROI75\n\nCalcolare il valore di SNR come SNR=S/N.\n\nIl motivo per cui il protocollo prevede il calcolo della baseline (punto 2) è che il produttore della macchina potrebbe sommare arbitrariamente un valore costante all’immagine facendo salire S e quindi il valore di SNR senza modificare il contrasto. Quindi in realtà la procedura misura il valore di CNR tra il fantoccio ed il fondo.\nDovremo quindi realizzare un programma MATLAB che implementi in modo automatico la procedura del protocollo assicurando che vengano rispettate le indicazioni, in particolare quelle sulle dimensioni delle ROI e che fornisca in uscita il valore di SNR.\n\nIl MATLAB fornisce varie funzioni per il tracciamento di ROI. La più semplice è la funzione getrect che permette di tracciare una ROI rettangolare con il mouse su una immagine. Funzioni più complete sono quelle del toolbox “ROI-Based Processing” come drawfreehand che permette di tracciare una ROI a mano libera, drawcircle (cerchio), drawellipse (ellisse), drawpolygon (poligono generico), drawrectangle (rettangolo), etc. In Python, funzionalità analoghe sono fornite da librerie di visualizzazione e image processing.\nIn particolare, la libreria matplotlib permette di tracciare ROI rettangolari interattive\ntramite widget dedicati, mentre la libreria napari offre strumenti più completi per il\ndisegno e la gestione di ROI interattive. Napari consente il tracciamento di ROI a mano libera, circolari, ellittiche, poligonali e\nrettangolari, in modo analogo alle funzioni drawfreehand, drawcircle, drawellipse,\ndrawpolygon e drawrectangle del toolbox ROI-Based Processing di MATLAB.\n\nIl calcolo dell’uniformità, e quindi della presenza di un campo di attenuazione, viene effettuato con la seguente procedura:\n\nDefinire sull’immagine una ROI (ROI80) posizionata al centro dell’oggetto test di dimensioni pari al 80% dell’oggetto.\n\nDeterminare il valor medio del segnale nella ROI80 (Sm) ed il numero di pixel contenuti nella ROI80 (N)\n\nDeterminare la deviazione media assoluta AAD = \\sum _{i} |S_{i}-S_{m}|/N_{i} dove Si è il valore di segnale dei singoli pixel contenuti nella ROI80\n\nCalcolare UH = 1-ADD/Sm\n\nIn assenza di rumore, se non c’è attenuazione ADD=0 e UH=1 (massima uniformità, nessuna attenuazione). Se c’è attenuazione, ci sarà una variazione di segnale e ADD sarà maggiore di zero abbassando il valore di UH. Per un’immagine reale ci sarà presenza di rumore e ADD sarà comunque diverso da zero in quanto il segnale varierà. Il presupposto della misura è che Sm sarà molto alto, in quanto il fantoccio sarà costruito a questo scopo, per cui in assenza di attenuazione UH sarà molto vicino ad uno.\nAnche in questo caso dovremo realizzare un programma MATLAB automatico che implementi la procedura del protocollo assicurando che vengano rispettate le indicazioni, in particolare quelle sulle dimensioni delle ROI, e che fornisca in uscita il valore di UH.\nInfine, andiamo a valutare la presenza di PVE sull’immagine 1 del fantoccio calcolando l’acutezza della transizione. A questo scopo occorre definire un profilo, cioè un segmento posto sull’immagine che intersechi una transizione, ed estrarre il grafico del valore di segnale sul profilo, come esemplificato in figura per il software ImageJ.\n\nFigura 1.41. Tracciamento di un profilo.\n\nDato il profilo, il valore di acutezza si ottiene utilizzando la formula:A = \\frac{1}{f(b) - f(a)} \\int_{a}^{b}\n\\left[ \\frac{d}{dx} f(x) \\right]^2 \\, dx\n\nDove f è il profilo estratto e a e b sono l’inizio e la fine della transizione. Essendo l’immagine discreta, l’integrale viene approssimato come:A = \\frac{1}{f(b) - f(a)} \\sum_{a}^{b}\n\\left[ \\frac{f(i+1)-f(i)}{d}\\right]^2\n\nDove d è la dimensione del pixel. I punti a e b (inizio e fine della transizione) possono essere definiti manualmente, ma è opportuno automatizzare la procedura per ridurre la variabilità dovuta all’osservatore. Da un punto di vista teorico, all’inizio del profilo il segnale avrà un valore dato dalla baseline (valor medio del fondo) più il rumore con SD sigma. Potremmo definire una soglia conservativa (come baseline + 4\\sigma) al di sopra della quale è estremamente improbabile che il segnale misurato sul profilo sia originato da un pixel del fondo. Analogamente potremmo definire una soglia per la fine della transizione. Nella pratica si utilizza un approccio più semplice, ad esempio, possiamo convenire che a e b siano i punti in cui il segnale raggiunge il 10% ed il 90% del suo valore massimo teorico misurato in precedenza (Sm), rispettivamente. Andranno quindi fissate due soglie 0.1*Sm e 0.9*Sm in modo da trovare a e b.\n\nDovremo realizzare quindi un programma Python che implementi il calcolo dell’acutezza su di un profilo definibile dall’utente o calcolato in modo automatico.\n\nRisultati attesi:\n\nSNR\n\nUH\n\nAcutezza\n\n8\n\n0.90\n\n415","type":"content","url":"/z-1-1-esercitazione#esercitazione","position":5},{"hierarchy":{"lvl1":"Effetto del PVE sul profilo - Notebook 1.4"},"type":"lvl1","url":"/zn-1-4-profile","position":0},{"hierarchy":{"lvl1":"Effetto del PVE sul profilo - Notebook 1.4"},"content":"Import delle librerie necessarie per la simulazione\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.ndimage as ndi\n\n\n\nSimuliamo un oggetto di intensità 255 su sfondo a intensità nulla e aumentiamo il fattore PVE tramite filtro gaussiano per apprezzarne l’effetto sul profilo.\n\nIl profilo viene tracciato orizzontalmente\n\n# immagine iniziale\ndim = 512\nim0 = np.zeros((dim, dim))\nim0[200:400, 200:400] = 255\n\n# kernel\nksize = 9\nh = np.ones((ksize, ksize)) / (ksize**2)\n\n# numero di volte in cui viene applicato il filtraggio, per simulare un PVE sempre maggiore\nn_iters = [0, 1, 5, 20, 50]\n\nfig, ax = plt.subplots(\n    len(n_iters), 2,\n    figsize=(8, 2.5 * len(n_iters)),\n    constrained_layout=True\n)\n\nfor i, n in enumerate(n_iters):\n\n    im = im0.copy()\n    for _ in range(n):\n        im = ndi.convolve(im, h)\n\n    # immagine\n    ax[i, 0].imshow(im, cmap='gray', vmin=0, vmax=255)\n    ax[i, 0].set_title(f\"Iterazioni filtro: {n}\")\n    ax[i, 0].axis(\"off\")\n\n    # profilo\n    ax[i, 1].plot(im[300, :])\n    ax[i, 1].set_ylim(0, 260)\n    ax[i, 1].set_title(\"Profilo riga 300\")\n\nplt.show()\n\n\n","type":"content","url":"/zn-1-4-profile","position":1},{"hierarchy":{"lvl1":"Effetto della dimensione della ROI sulla stima di M e SD - Notebook 1.5"},"type":"lvl1","url":"/zn-1-5-test-m-sd","position":0},{"hierarchy":{"lvl1":"Effetto della dimensione della ROI sulla stima di M e SD - Notebook 1.5"},"content":"Import delle librerie necessarie alla simulazione\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.ndimage as ndi\n\n\n\n\n# simulated SD\nTsd = 10\n\n# simulated mean\nTm = 300\n\n# plot mean/SD dispersion vs ROI dimension\nN = 100\n\nfigure, ax = plt.subplots(1,2, constrained_layout=True)\nax[0].set_xlabel('ROI dim')\nax[0].set_ylabel('Mean (imposed 300)')\nax[0].set_ylim(250,350)\nax[1].set_xlabel('ROI dim')\nax[1].set_ylabel('sd (imposed 10)')\nax[1].set_ylim(0,20)\n\nfor k in range(N):\n    dim = np.array([],int)\n    M   = np.array([])\n    sd  = np.array([])\n    for i in range(1,10):\n        # ROI area\n        new_el = (i)*(i)\n        dim = np.append(dim, new_el) \n\n        # generate image with gaussian noise\n        image = Tm + Tsd*np.random.randn(dim[i-1])\n        M  = np.append(M, np.mean(image, None))\n        sd = np.append(sd, np.std(image, None))\n    ax[0].plot(dim,M)\n    ax[1].plot(dim,sd)\nplt.show()\n\n\n\n","type":"content","url":"/zn-1-5-test-m-sd","position":1},{"hierarchy":{"lvl1":"Esempio calcolo SD su fantoccio e immagine reale - Notebook 1.6"},"type":"lvl1","url":"/zn-1-6-esempio-calcolo-sd","position":0},{"hierarchy":{"lvl1":"Esempio calcolo SD su fantoccio e immagine reale - Notebook 1.6"},"content":"Import delle librerie necessarie per la simulazione.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.ndimage as ndi\nimport pydicom\n\n\n\n","type":"content","url":"/zn-1-6-esempio-calcolo-sd","position":1},{"hierarchy":{"lvl1":"Esempio calcolo SD su fantoccio e immagine reale - Notebook 1.6","lvl2":"Stima SD su immagine simulata."},"type":"lvl2","url":"/zn-1-6-esempio-calcolo-sd#stima-sd-su-immagine-simulata","position":2},{"hierarchy":{"lvl1":"Esempio calcolo SD su fantoccio e immagine reale - Notebook 1.6","lvl2":"Stima SD su immagine simulata."},"content":"\n\n# create ideal image with 6 patterns\ndim = 512\nimage = np.zeros([dim,dim])\nimage[:,:] = 50\nimage[49:100, 49:100] = 120\nimage[100:180, 100:449] = 200\nimage[199:499, 199:349] = 90\nimage[229:269, 229:269] = 250\nimage[4:399, 449:499] = 150\n\n# add gaussian noise\nsigma = 5\nimageN = image + sigma*np.random.randn(dim,dim)\n\n# histogram\ncounts, bins = np.histogram(imageN, bins = 256)\n\nfig, ax = plt.subplots(1,2)\nax[0].imshow(imageN, cmap = 'gray')\nax[1].hist(bins[:-1], bins, weights=counts)\n\n# get map of standard deviation\nimageF = ndi.generic_filter(imageN, np.std, size = 5)\ncountsF, binsF = np.histogram(imageF, bins = 256)\n\nfig, ax = plt.subplots(1,2)\nax[0].imshow(imageF, cmap = 'gray')\nax[1].hist(binsF[:-1], binsF, weights=countsF)\n\n# print estimated values for SD\nsigmaMean = np.mean(imageF, None)\nsigmaMedian = np.median(imageF, None)\nid = np.argmax(countsF)\nsigmaMax = binsF[id]\nplt.show()\n\nprint(\"Assigned sigma: \",sigma)\nprint(\"Sigma Mean: \",sigmaMean)\nprint(\"Sigma Median: \",sigmaMedian)\nprint(\"Sigma Hist: \",sigmaMax)\n\n\n\n\n\n\n\n\n","type":"content","url":"/zn-1-6-esempio-calcolo-sd#stima-sd-su-immagine-simulata","position":3},{"hierarchy":{"lvl1":"Esempio calcolo SD su fantoccio e immagine reale - Notebook 1.6","lvl2":"Stima SD su immagine DICOM."},"type":"lvl2","url":"/zn-1-6-esempio-calcolo-sd#stima-sd-su-immagine-dicom","position":4},{"hierarchy":{"lvl1":"Esempio calcolo SD su fantoccio e immagine reale - Notebook 1.6","lvl2":"Stima SD su immagine DICOM."},"content":"\n\nds = pydicom.dcmread('../data/phantom.dcm')\nprint(\"Patient ID\", ds.PatientID)\n\nI = ds.pixel_array\n\nnz = np.where(I != 0)[0]\n\n# ROI based analysis\ncenterO = (120,80)\ncenterW = (120,120)\ncenterB = (50,50)\nradius = 8\n\nfig, ax = plt.subplots()\nax.imshow(I, cmap = 'gray')\ncircleO = plt.Circle(centerO, radius, color = 'r', fill = False)\nax.add_patch(circleO)\ncircleO = plt.Circle(centerW, radius, color = 'g', fill = False)\nax.add_patch(circleO)\ncircleO = plt.Circle(centerB, radius, color = 'b', fill = False)\nax.add_patch(circleO)\n\nx, y = np.meshgrid(np.arange(ds.Columns), np.arange(ds.Rows))\nmaskO = (x - centerO[0])**2 + (y - centerO[1])**2 <= radius**2\nmaskW = (x - centerW[0])**2 + (y - centerW[1])**2 <= radius**2\nmaskB = (x - centerB[0])**2 + (y - centerB[1])**2 <= radius**2\n\nid = np.where(maskO != 0)\nstdO = np.std(I[id], None)\n\nid = np.where(maskW != 0)\nstdW = np.std(I[id], None)\n\nid = np.where(maskB != 0)\nstdB = np.std(I[id], None)\n\nplt.show()\nprint(\"Stima basata su ROI:\")\nprint(\"Sigma Oil: \",stdO)\nprint(\"Sigma Water: \",stdW)\nprint(\"Sigma Background: \",stdB)\nprint(\"Sigma Background corrected: \",stdB*1.526)\n\n# histogram based analysis\nsdmap = ndi.generic_filter(I, np.std, size = 3)\ncounts, bins = np.histogram(sdmap[nz], bins = 128)\n\nfig, ax = plt.subplots(1,2)\nax[0].imshow(sdmap, cmap = 'gray')\nax[1].hist(bins[:-1], bins, weights=counts)\n\nsigmaMean = np.mean(sdmap[nz], None)\nsigmaMedian = np.median(sdmap[nz], None)\nid = np.argmax(counts)\nsigmaMax = bins[id]\nplt.show()\nprint(\"stima basata sulla mappa di SD:\")\nprint(\"Sigma Mean: \",sigmaMean)\nprint(\"Sigma Median: \",sigmaMedian)\nprint(\"Sigma Hist: \",sigmaMax)\n\n\n# use only high signal part of the data\nnHigh = np.where(I>100)[0]\nsdmap = ndi.generic_filter(I, np.std, size = 9)\ncounts, bins = np.histogram(sdmap[nHigh], bins = 128)\n\nsigmaMean = np.mean(sdmap[nHigh], None)\nsigmaMedian = np.median(sdmap[nHigh], None)\nid = np.argmax(counts)\nsigmaMax = bins[id]\nplt.show()\n\nprint(\"stima basata sulla mappa di SD utilizzando solo le parti ad alto valore di SD:\")\nprint(\"Sigma Mean: \",sigmaMean)\nprint(\"Sigma Median: \",sigmaMedian)\nprint(\"Sigma Hist: \",sigmaMax)\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/zn-1-6-esempio-calcolo-sd#stima-sd-su-immagine-dicom","position":5},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione"},"type":"lvl1","url":"/c21-interpolazione-filtraggio-e-compressione-compl","position":0},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione"},"content":"Come visto in precedenza, il primo passo dopo l’acquisizione dell’immagine biomedica è l’esecuzione di operazioni a basso livello che hanno lo scopo di migliorare l’immagine. Queste operazioni corrispondono al secondo livello nella classificazione degli algoritmi di computer vision.","type":"content","url":"/c21-interpolazione-filtraggio-e-compressione-compl","position":1},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl2":"Interpolazione"},"type":"lvl2","url":"/c21-interpolazione-filtraggio-e-compressione-compl#interpolazione","position":2},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl2":"Interpolazione"},"content":"Una immagine biomedica ha dimensioni definite a livello di acquisizione o del processo di ricostruzione implementato nello scanner. Le dimensioni sono tipicamente legate alla risoluzione spaziale, nel senso che a parità di campo di vista (FOV), modalità con una migliore risoluzione spaziale saranno in grado di acquisire pixel di dimensioni minori e quindi la dimensione dell’immagine sarà maggiore. Le dimensioni tipiche di una immagine radiologica possono andare da 64x64 (immagini SPECT) a 256x256 (MR cardiaca) fino a 512x512 (CT e MRI) o 1024x1024 (CT). La radiografia classica ha risoluzioni più alte, fino a 5000x5000 per la mammografia.Nell’elaborazione delle immagini biomediche, può essere necessario cambiare la dimensione delle immagini, aumentando o diminuendo il numero di pixel componenti (Figura 2.1).\n\nUn tipico esempio è la visualizzazione dell’immagine su di uno schermo per la visualizzazione e refertazione. Tipicamente la dimensione di una immagine radiologica è minore (o comunque diversa) della dimensione del supporto digitale (stampa o schermo) su cui deve essere visualizzata. Un monitor tipico per refertazione radiologica ha risoluzione di 1600x1200 pixel, che è comunque una risoluzione usuale anche nei monitor di uso generale. È necessario quindi interpolare l’immagine in modo da permetterne la visualizzazione in maniera corretta adattando le dimensioni delle immagini a quelle dello schermo (o della porzione di schermo) in cui vanno visualizzate.\nUn altro esempio, come sarà visto nel seguito, è il cambio di risoluzione spaziale nelle immagini 3D per ottenere dei voxel cubici e quindi delle immagini isotrope.\nCome già visto in precedenza, l’immagine biomedica può essere vista come una serie di campioni di un processo fisico acquisiti in uno spazio discreto, la cui risoluzione è uguale alla dimensione del pixel. In pratica l’immagine ci fornisce i campioni in NxM locazioni spaziali poste al centro dei pixel dell’immagine (seguendo la convenzione DICOM).\nConsideriamo l’immagine in Figura 2.1 con FOV 340x340 mm e dimensioni 256x256 e il suo ingrandimento a destra. Il pixel size è 340/256=1.32 mm, e questo è anche l’intervallo di campionamento spaziale.\nOgni pixel corrisponde ad un punto della griglia (le crocette in figura). Se vogliamo aumentare il numero di righe e colonne dell’immagine dobbiamo aumentare la risoluzione della griglia. Supponiamo di raddoppiare la risoluzione interpolando l’immagine a 512x512.\n\nFigura 2.1. L’operazione di interpolazione prevede di stimare il valore dell’intensità del pixel su una nuova grigila.\n\nIn base alla nuova griglia dovremo definire dei nuovi pixel nella posizione corrispondente ai nuovi punti della griglia. A seconda della procedura usata per definire dei nuovi valori avremo vari metodi di interpolazione. Tipicamente il nuovo valore dei pixel verrà dedotto da quello dei pixel vicini, nell’esempio di  Figura 2.1 dobbiamo ricampionare l’immagine sulla griglia rossa utilizzando i valori dei pixel originali (in bianco). Tipicamente per ottimizzare il tempo di elaborazione si utilizzano solo i pixel più “vicini” al pixel da ricomputare.","type":"content","url":"/c21-interpolazione-filtraggio-e-compressione-compl#interpolazione","position":3},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl3":"Nearest neighbor","lvl2":"Interpolazione"},"type":"lvl3","url":"/c21-interpolazione-filtraggio-e-compressione-compl#nearest-neighbor","position":4},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl3":"Nearest neighbor","lvl2":"Interpolazione"},"content":"Un primo approccio (NN, nearest neighbor)  assegna al nuovo pixel il valore del pixel più vicino (Figura 2.2). Il metodo è molto veloce e ha il vantaggio di non introdurre nuovi livelli di grigio, cosa che può essere rilevante da un punto di vista diagnostico. D’altra parte, il metodo NN tende a creare dei gruppi di pixel con lo stesso valore, dando l’impressione visiva di una immagine “pixellata”.\n\nFigura 2.2. Interpolazione NN.","type":"content","url":"/c21-interpolazione-filtraggio-e-compressione-compl#nearest-neighbor","position":5},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl3":"Interpolazione bilineare","lvl2":"Interpolazione"},"type":"lvl3","url":"/c21-interpolazione-filtraggio-e-compressione-compl#interpolazione-bilineare","position":6},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl3":"Interpolazione bilineare","lvl2":"Interpolazione"},"content":"Un metodo più perfezionato è la cosiddetta interpolazione bilineare (Figura 2.3), nella quale il nuovo valore del pixel viene stimato come la media pesata dei valori dei 4 (o 8 in 3D) pixel più vicini. Il valore del pixel nel punto P è la somma dei valori dei pixel in Q_{ij} pesati per l’area normalizzata del rettangolo opposto a Q_{ij}.\n\nFigura 2.3. Interpolazione bilineare.\n\nFormalmente si ha:\\begin{aligned}\nf(x,y) \\approx {} &\n\\frac{f(Q_{11})}{(x_2 - x_1)(y_2 - y_1)} (x_2 - x)(y_2 - y) \\\\\n&+ \\frac{f(Q_{21})}{(x_2 - x_1)(y_2 - y_1)} (x - x_1)(y_2 - y) \\\\\n&+ \\frac{f(Q_{12})}{(x_2 - x_1)(y_2 - y_1)} (x_2 - x)(y - y_1) \\\\\n&+ \\frac{f(Q_{22})}{(x_2 - x_1)(y_2 - y_1)} (x - x_1)(y - y_1).\n\\end{aligned}\n\nL’interpolazione bilineare offre solitamente una qualità visiva migliore dell’interpolazione NN.\nL’interpolazione bilineare può essere vista come una stima del valore atteso del segnale in P modellando la transizione del segnale tra pixel come una retta ed utilizzando solo i due campioni più vicini per stimare i parametri della retta stessa.","type":"content","url":"/c21-interpolazione-filtraggio-e-compressione-compl#interpolazione-bilineare","position":7},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl3":"Interpolazione bicubica e spline","lvl2":"Interpolazione"},"type":"lvl3","url":"/c21-interpolazione-filtraggio-e-compressione-compl#interpolazione-bicubica-e-spline","position":8},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl3":"Interpolazione bicubica e spline","lvl2":"Interpolazione"},"content":"L’idea può essere estesa utilizzando modelli non lineari (ad esempio funzioni polinomiali di grado crescente o funzioni sinusoidali) ed aumentando il numero di campioni utile a stimare dette funzioni. Un esempio è l’interpolazione bicubica (bicubic interpolation) che utilizza polinomi di terzo grado e una griglia di 16 pixel.Esistono poi metodi ancora più perfezionati (spline) in cui l’intera distribuzione dei pixel sull’immagine viene modellata come una superficie ed i valori interpolanti ottenuti di conseguenza.\nNella visualizzazione di immagini mediche, oltre alla qualità visiva percepita dell’interpolazione è importante anche il tempo di elaborazione, in quanto il sistema software deve rispondere in tempo reale ai comandi dell’utente, ad esempio quando viene effettuato uno zoom.","type":"content","url":"/c21-interpolazione-filtraggio-e-compressione-compl#interpolazione-bicubica-e-spline","position":9},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl3":"Reslicing","lvl2":"Interpolazione"},"type":"lvl3","url":"/c21-interpolazione-filtraggio-e-compressione-compl#reslicing","position":10},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl3":"Reslicing","lvl2":"Interpolazione"},"content":"Il processo di interpolazione è di particolare rilevanza nell’elaborazione di immagini 3D, nelle quali tipicamente la distanza inter-fetta (asse z) e maggiore della risoluzione del pixel sulla singola fetta (piano x-y). In questo caso per ottenere un volume isotropo (cioè con risoluzione spaziale uguale su tutti gli assi) è necessario eseguire una operazione di interpolazione 3D che crei delle nuove slice con distanza inter-fetta uguale alla risoluzione del pixel. Tale operazione è detta reslicing (Figura 2.4). L’operazione di interpolazione deve mantenere costante il campo di vista (FOV), cioè la regione di spazio coperta dal volume.\n\nFigura 2.4. Reslicing.\n\nSupponiamo di avere un volume definito da Nz fette con un FOV = [dimX, dimY, dimZ], dove dimX = dx Nx   (pixel spacing x colonne) dimY = dy Ny\t (pixel spacing x righe) dimZ = dz Nz\t (slice spacing x numero slice)\n\nse dz > dx=dy come avviene di solito, dobbiamo creare delle nuove slice con il reslicing in modo da avere la distanza inter-slice dz1 del nuovo volume uguale a dx/dy senza cambiare il FOV. Quindi:\n\ndimZ = dz Nz = dz1 Nz1 = dx Nz1\n\ne quindi\n\nNz1 = dz Nz/dx\n\nIl reslicing viene anche utilizzato per ricostruire piani di vista diversi da quelli acquisiti, ad esempio per ottenere un piano sagittale, coronale o obliquo da una acquisizione assiale come viene fatto usualmente nella TAC.\nSi noti che gli algoritmi standard di interpolazione presenti nei software generici dedicati alla elaborazione di immagini (come Matlab/python) presuppongono che i pixel dell’immagine siano quadrati e i voxel cubici. Nel caso di immagini biomediche questa assunzione può essere errata, specialmente nel caso di immagini 3D. Il processo di interpolazione deve quindi tener conto della posizione reale dei pixel o voxel nello spazio, cosa che si può fare accedendo ai campi DICOM opportuni.\n\nIn MATLAB è possibile utilizzare la funzione interp3 in combinazione con la funzione meshgrid. La funzione interp3 permette anche di scegliere il metodo di interpolazione da utilizzare attraverso opportune opzioni.\nNel caso di immagini isotrope e fattore di interpolazione uguale per tutti gli assi è possibile utilizzare le funzioni imresize e imresize3.\n\nIn Python è possibile utilizzare la funzione interpn della libreria SciPy (Notebook 2.1).\nLa funzione interpn permette anche di scegliere il metodo di interpolazione da utilizzare attraverso opportune opzioni.\nNel caso di immagini isotrope e fattore di interpolazione uguale per tutti gli assi è possibile utilizzare la funzione RegularGridInterpolator.\n\nCome detto in precedenza un’applicazione tipica degli algoritmi di interpolazione è l’estrazione da volumi di dati 3D di una immagine con orientamento diverso da quello di acquisizione (reslicing). Nella visualizzazione triplanar (proiezione ortogonale) dal volume dei dati vengono estratti tre piani tra loro perpendicolari attraverso una operazione di interpolazione. L’origine dei tre piani può essere definita in modo interattivo dall’operatore, che così può esplorare il volume dei dati. La visualizzazione triplanar è tipicamente disponibile su tutte le workstation per l’analisi di bioimmagini. Un esempio è riporato in Figura 2.5.\n\nFigura 2.5. Triplanar view di un fantoccio ottenuta con imagej.\n\nIn Python la visualizzazione triplanare di dati volumetrici (piani assiale, sagittale e coronale)\npuò essere effettuata tramite il visualizzatore multidimensionale napari, che consente\nl’esplorazione interattiva delle slice lungo i tre assi principali.","type":"content","url":"/c21-interpolazione-filtraggio-e-compressione-compl#reslicing","position":11},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl2":"Filtraggio dell’immagine biomedica"},"type":"lvl2","url":"/c21-interpolazione-filtraggio-e-compressione-compl#filtraggio-dellimmagine-biomedica","position":12},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl2":"Filtraggio dell’immagine biomedica"},"content":"Gli algoritmi di filtraggio consentono l’elaborazione dell’immagine biomedica in modo da modificarne le caratteristiche. Non è banale definire in modo univoco cosa si intende per operazione di filtraggio dal punto di vista radiologico. Seguendo l’approccio della computer vision, possiamo definire filtraggio una operazione che cambia il contenuto informativo dell’immagine senza estrarre informazioni topologiche (in questo differente dal processo di segmentazione). Quindi un filtro potrà evidenziare i contorni dell’immagine, mentre un processo di segmentazione convertirà i contorni evidenziati in una entità geometrica, come una curva o una maschera.\nIl filtraggio ha essenzialmente due possibili scopi:\n\nMigliorare la visualizzazione dell’immagine per ottimizzare l’analisi visiva della stessa.\n\nPredisporre l’immagine per ottimizzare una elaborazione successiva, tipicamente una segmentazione.\n\nPer quanto riguarda il secondo punto, l’operazione di filtraggio avviene tipicamente in modo trasparente rispetto all’utente, che osserva solo il risultato dell’operazione successiva che rappresenta il fine del processo di elaborazione. Il primo caso è invece critico rispetto al processo diagnostico, in quanto un errore nell’algoritmo di filtraggio può causare la perdita di informazione utile e fini clinici e quindi indurre un errore diagnostico. L’introduzione di algoritmi di filtraggio in fase diagnostica è quindi effettuato con estrema cautela.\n\nLe operazioni di filtraggio sull’immagine si possono classificare in tre categorie principali:\n\nPuntuali: Un’operazione puntuale opera solo sul singolo pixel dell’immagine, trasformandolo in base ad una funzione g(v) dove v è il valore dell’intensità del pixel. Esempio di operazioni puntuali sono il negativo dell’immagine e in generale le lookup table compresa l’operazione di windowing e l’applicazione della gamma function. In una operazione puntuale il valore del pixel sull’immagine filtrata dipende solo dal valore dello stesso pixel nell’immagine originale.\n\nLocali: una operazione locale elabora un pixel in base al valore del pixel stesso e dei pixel circostanti. Esempio di operazione locale è la convoluzione spaziale che vedremo in dettaglio nel seguito.\n\nGlobali: Le operazioni globali operano sull’immagine nel suo complesso. Esempio di operazioni globali sono le operazioni sull’istogramma, come l’equalizzazione.","type":"content","url":"/c21-interpolazione-filtraggio-e-compressione-compl#filtraggio-dellimmagine-biomedica","position":13},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl3":"Operazioni puntuali","lvl2":"Filtraggio dell’immagine biomedica"},"type":"lvl3","url":"/c21-interpolazione-filtraggio-e-compressione-compl#operazioni-puntuali","position":14},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl3":"Operazioni puntuali","lvl2":"Filtraggio dell’immagine biomedica"},"content":"Le operazioni puntuali sono equivalenti ad una trasformazione dei livelli di grigio. In pratica un livello di grigio v dell’immagine viene trasformato in un nuovo livello u secondo una trasformazione u=t(v). t può essere utilmente codificata in una lookup-table, cioè una tabella con una sola colonna ed un numero di righe uguale alla profondità dell’immagine. Il vantaggio della lookup-table è che il filtraggio può essere eseguito in modo molto efficiente da un punto di vista computazionale. Una volta definita la tabella come un array T di N elementi dove N sono i possibili valori che può assumere v, il valore di u sarà semplicemente u=T(v), e quindi il nuovo valore di u verrà computato senza nessuna operazione algebrica ma attraverso un semplice accesso in memoria. Le color map per la visualizzazione a falsi colori sono un esempio di una lookup-table a tre colonne dove la tripletta (R,G,B) è data da (R,G,B) = LT(:,v) dove v è il livello di grigio.\n\nEsempi di trasformazioni puntuali sono (si consideri una immagine a 256 livelli):\n\nl’inversione dei livelli di grigio u = (255-v)\n\nil Windowing\n\nle operazioni di tipo gamma u=v^{\\gamma}\n\nla binarizzazione. Viene decisa una soglia T e l’immagine viene ridotta ad una immagine bimodale (u = A se v>T, u = B se v <T)\n\nLa Figura 2.6 riassume alcune operazioni di filtraggio puntuale. Come si osserva, una operazione di filtraggio puntuale può essere sempre espressa come una curva (non necessariamente continua) in un piano bidimensionale con due assi di dimensioni pari alla profondità dell’immagine originale (in questo caso 255, 8 bit) e dell’immagine filtrata (in questo caso normalizzata a 1).\n\nFigura 2.6. Esempi di filtri locali puntuali.","type":"content","url":"/c21-interpolazione-filtraggio-e-compressione-compl#operazioni-puntuali","position":15},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl4":"Windowing","lvl3":"Operazioni puntuali","lvl2":"Filtraggio dell’immagine biomedica"},"type":"lvl4","url":"/c21-interpolazione-filtraggio-e-compressione-compl#windowing","position":16},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl4":"Windowing","lvl3":"Operazioni puntuali","lvl2":"Filtraggio dell’immagine biomedica"},"content":"Esaminiamo in dettaglio l’operazione di Windowing che è di cruciale importanza nella visualizzazione delle immagini biomediche. Una immagine biomedica è in generale codificata su 16 bit, cioè ogni pixel dell’immagine può assumere 2^{16}= 65536 valori, corrispondenti ad altrettanti livelli di grigio. Di fatto il numero di livelli che vengono utilizzati è più basso, ad esempio nella risonanza magnetica solitamente qualche migliaio. Ad ogni modo, gli strumenti di visualizzazione utilizzati nella pratica clinica, come schermi video tradizionali o LCD, sono in grado di visualizzare solo 256 livelli di grigio. D’altra parte, risoluzioni superiori sarebbero inutili in quanto l’occhio umano è in grado di distinguere un numero limitato di livelli di grigio contemporaneamente, dell’ordine di qualche decina. Poiché l’occhio è invece capace di separare un numero molto maggiore di colori diversi, la rappresentazione delle immagini in falsi colori, cioè facendo corrispondere ad ogni livello di grigio un determinato colore attraverso una mappa prestabilita può essere utilizzata per caratterizzare immagini mediche. Questa tecnica trova però applicazione solo in medicina nucleare e nella creazione di mappe parametriche.\nTutti gli strumenti di visualizzazione devono quindi affrontare il problema di rappresentare un numero di livelli di grigio superiore a quello che lo strumento di visualizzazione può sostenere. Le immagini visualizzate o stampate sono quindi fortemente caratterizzate dal modo in cui questo problema è risolto, ed essendo la soluzione del problema tutt’altro che univoca, bisogna tener conto del fatto che ogni immagine stampata o visualizzata su schermo può rappresentare solo parzialmente l’informazione globale contenuta nei dati effettivamente acquisiti dallo scanner.\n\nIl processo mediante il quale i livelli di grigio dell’immagine reale vengono rappresentate sullo schermo o stampate viene solitamente detto windowing. I pixel che giacciono in un certo intervallo di valori (una finestra o window), che può essere impostato dall’utente, vengono rappresentati utilizzando tutti i livelli disponibili (tipicamente 256). I valori al di sotto della finestra selezionata vengono tutti posti a zero, e verranno quindi rappresentati in nero. A tutti i valori superiori verrà invece assegnato il valore massimo visualizzabile (tipicamente 255) e quindi essi appariranno bianchi. La Figura 2.7 mostra un esempio delle differenze di visualizzazione ottenibili attraverso diverse scelte della finestra da utilizzare. Nell’immagine superiore i livelli di grigio vengono mappati in modo proporzionale. Quindi i livelli visualizzati (da 0 a 255, asse y) vengono ottenuti normalizzando i valori originali rispetto al valor massimo dell’immagine originale. Nell’immagine in basso è stata effettuata una opportuna operazione di windowing per visualizzare al meglio la zona di interesse. Si può notare come alcune zone abbiano perso notevolmente di risoluzione a causa dello schiacciamento dei livelli di grigio relativi.\n\nFigura 2.7. Effetto dell’operazione di windowing sulla visualizzazione di una immagine.\n\nL’operazione di Windowing è implementata in tutte le stazioni radiografiche e viene tipicamente pilotata dal movimento del mouse. Ad esempio il movimento orizzontale del mouse può pilotare la larghezza della finestra e il movimento verticale la posizione del centro della finestra sull’asse x. Comunque il metodo con cui viene implementato il filtraggio è in realtà molto raffinato e mira a permettere una regolazione del windowing veloce ma allo stesso tempo precisa. La transizione dal livello 0 al livello massimo della finestra non è necessariamente lineare, ma può assumere varie forme. Il formato DICOM contiene alcuni parametri che pilotano l’operazione di windowing come riportato in Tabella 2.1.\n\nTabella 2.1. Alcuni parametri DICOM che pilotano l’operazione di windowing.\n\nGroup Element\n\nTitle\n\nEsempio\n\n[0028-0106]\n\nSmallest Image Pixel Value\n\n0\n\n[0028-0107]\n\nLargest Image Pixel Value\n\n4177\n\n[0028-0150]\n\nWindow Centre\n\n163\n\n[0028-0151]\n\nWindow Width\n\n327\n\nI primi due parametri, “Smallest Image Pixel Value” e “Largest Image Pixel Value” permettono di leggere il massimo e minimo valore dell’immagine e di creare una funzione di windowing lineare, evitando di comprendere nell’operazione di windowing valori al di fuori del campo di definizione dell’immagine, anche se compresi nella massima profondità teorica. I parametri successivi “Window Centre” e “Window Width” definiscono una finestra di Windowing da utilizzare nella visualizzazione dell’immagine. Questa finestra viene tipicamente salvata nel DICOM al momento dell’acquisizione in base ad una ottimizzazione fatta dal produttore. Il programma di visualizzazione leggerà dal DICOM la finestra e produrrà una visualizzazione iniziale dell’immagine che l’utente potrà poi modificare.\nCome illustrato in Figura 2.8, l’operazione di Windowing per quanto semplice in via di principio è estremamente importante nella pratica clinica, e necessita quindi di una accurata implementazione.\nVale la pena di notare che le lastre radiografiche tradizionali o le immagini digitali in formato standard (jpeg, tiff) allegate ad un referto elettronico rappresentano il prodotto di una precisa operazione di Windowing eseguita dal medico refertante, con l’obiettivo di fornire la migliore informazione iconografica al paziente ed al medico inviante. Esse non hanno però valore legale, in quanto l’operazione di Windowing può cancellare alcune componenti delle immagini se eseguita in modo inappropriato. L’unica fonte certa sono quindi le immagini digitali in formato DICOM, che vengono quindi immagazzinate e sempre più spesso consegnate al paziente su supporto digitale (tipicamente un DVD contenente un visualizzatore DICOM).\n\nFigura 2.8 (tratto da R Rangayyan, Biomedical Image Analysis, CRC Press 2004); esempio di Windowing a fini diagnostici su immagini CT.","type":"content","url":"/c21-interpolazione-filtraggio-e-compressione-compl#windowing","position":17},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl3":"Operazioni locali e filtraggio spaziale","lvl2":"Filtraggio dell’immagine biomedica"},"type":"lvl3","url":"/c21-interpolazione-filtraggio-e-compressione-compl#operazioni-locali-e-filtraggio-spaziale","position":18},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl3":"Operazioni locali e filtraggio spaziale","lvl2":"Filtraggio dell’immagine biomedica"},"content":"L’operazione di base nel filtraggio locale delle immagini è l’operazione di convoluzione spaziale detta anche a finestra mobile. Definiamo una matrice KxK, detta anche kernel o nucleo del filtro. Di solito il kernel è quadrato e di dimensioni dispari. Siano w_{ij} gli elementi del kernel e f_{mn} gli elementi dell’immagine di dimensioni MxN.  Allora il punto g_{mn} dell’immagine filtrata sarà dato da:g_{mn} =\n\\sum_{i=-\\frac{k-1}{2}}^{\\frac{k+1}{2}}\n\\sum_{j=-\\frac{k-1}{2}}^{\\frac{k+1}{2}}\nf_{m+i,\\,n+j}\\, w_{i,j}\n\nIn pratica l’operazione di convoluzione spaziale consiste nel far scorrere il kernel sull’immagine e sostituire volta per volta il pixel dell’immagine corrispondente al pixel centrale del kernel con un valore che è la somma dei pixel dell’immagine coperti dal kernel pesati per gli elementi del kernel stesso. Solitamente il kernel è normalizzato, e quindi la somma degli elementi del kernel è unitaria in modo da non mutare sostanzialmente la dinamica dell’immagine dopo il filtraggio. Questo punto è importante nell’imaging medico in quanto il valore del segnale può avere un significato diagnostico (come nella TAC). Inoltre essendo le immagini codificate come interi a 16 bit mutare la dinamica può causare problemi di overflow numerico.\nIl risultato di un’operazione di convoluzione spaziale è un’immagine di dimensione pari alla somma dell’immagine più la dimensione del kernel. Solitamente si considera solo la parte centrale dell’immagine risultante per ottenere una immagine delle stesse dimensioni dell’immagine originale.\n\nL’operazione di filtraggio spaziale è implementata in MATLAB tramite la funzione conv2 e, nel caso tridimensionale, tramite la funzione conv. L’opzione mode='same' consente di mantenere la dimensione dell’immagine filtrata. In Python, le operazioni equivalenti sono fornite dalla libreria SciPy mediante le funzioni\nscipy.signal.convolve2d e scipy.signal.convolve.\nIn alternativa, è possibile utilizzare la funzione scipy.ndimage.convolve, che presenta\ncaratteristiche analoghe a imfilter di MATLAB.\nI principali filtri possono essere definiti in modo automatico tramite funzioni dedicate,\ncome scipy.ndimage.gaussian_filter, oppure mediante kernel definiti manualmente.\n\nCambiando il kernel, si possono ottenere i vari tipi di filtraggio. Il primo esempio è il filtro di smoothing, che è in grado di ridurre il rumore. Il filtro di smoothing KxK è definito come:W_{ij}=\\frac{1}{K^{2}} \\quad \\forall \\quad i,j\n\nUn esempio 3x3 è\\frac{1}{9}*\\begin{bmatrix}\n1 & 1 & 1 \\\\\n1 & 1 & 1 \\\\\n1 & 1 & 1 \n\\end{bmatrix}\n\nNotiamo che per semplicità si preferisce esprimere il filtro con valori interi, spesso sottintendendo l’operazione di normalizzazione che comunque viene sempre eseguita.\nIl filtro effettua una operazione di media mobile, sostituendo ad un pixel dell’immagine il valore della media dei pixel in un intorno. Il filtro riduce efficacemente il rumore nelle regioni omogenee, mentre introduce un “ammorbidimento” dei contorni nelle regioni di confine tra i pattern, cosa non desiderabile nelle immagini biomediche. I due effetti sono tanto più rilevanti quanto più è grande il kernel.\nUna variante del filtro a media mobile è il filtro pillbox di Figura 2.9 (opzione disk della funzione\nfspecial in MATLAB), che realizza una media mobile limitata a una regione circolare.\nIn Python, un filtro equivalente può essere ottenuto tramite un kernel circolare normalizzato,\noppure utilizzando la funzione scipy.ndimage.uniform_filter in combinazione con una maschera\ncircolare.\n\nFigura 2.9. Filtro “pillbox”.\n\nUn altro esempio di filtri dedicati alla riduzione del rumore sono i filtri di tipo gaussiano, dove il kernel è definito come una gaussiana bidimensionale. Un esempio di filtro gaussiano 3x3 il seguente:\\frac{1}{16}*\\begin{bmatrix}\n1 & 2 & 1 \\\\\n2 & 4 & 2 \\\\\n1 & 2 & 1 \n\\end{bmatrix}\n\nI filtri gaussiani riducono il rumore riducendo l’effetto di sfuocaσmento dell’immagine. Esempi di filtri gaussiani con diversa deviazione standard sono:\\sigma = 0.391, \\quad dim=3 \\times 3 \\\\\n\\begin{bmatrix}\n1 & 4 & 1 \\\\\n4 & 12 & 4 \\\\\n1 & 4 & 1 \n\\end{bmatrix}\\sigma = 0.625, \\quad dim=5 \\times 5 \\\\\n\\begin{bmatrix}\n 1 & 2 & 3 & 2 & 1 \\\\\n 2 & 7 & 11 & 7 & 2 \\\\\n 3 & 11 & 17 & 11 & 3 \\\\\n 2 & 7 & 11 & 7 & 2 \\\\\n 1 & 2 & 3  & 2 & 1 \\\\\n\\end{bmatrix}\\sigma = 1, \\quad dim = 9 \\times 9 \\\\\n\\begin{bmatrix}\n 0 &0 &1 & 1 & 1 & 1 &1 &0 &0 \\\\\n 0 &1 &2 & 3 & 3 & 3 &2 &1 &0 \\\\\n 1 &2 &3 & 6 & 7 & 6 &3 &2 &1 \\\\\n 1 &3 &6 & 9 &11 & 9 &6 &3 &1 \\\\\n 1 &3 &7 &11 &12 &11 &7 &3 &1 \\\\\n 1 &3 &6 & 9 &11 & 9 &6 &3 &1 \\\\\n 1 &2 &3 & 6 & 7 & 6 &3 &2 &1 \\\\\n 0 &1 &2 & 3 & 3 & 3 &2 &1 &0 \\\\\n 0 &0 &1 & 1 & 1 & 1 &1 &0 &0\n \\end{bmatrix}\n\nNel filtro gaussiano, la somma del valore del peso centrale è maggiore degli altri e i pesi sono proporzionali alla distanza dal centro secondo l’equazione della gaussiana.\nUn filtro gaussiano è definito quindi da due fattori. Il primo è la grandezza del kernel, il secondo è il valore della deviazione standard \\sigma che definisce il valore degli elementi del kernel stesso. Si noti che per \\sigma molto più grande delle dimensioni del kernel il filtro gaussiano collassa in un filtro a media mobile, per \\sigma molto piccolo si ottiene un kernel diverso da zero solo nel punto centrale che lascia l’immagine inalterata.\n\nCome visto in precedenza, l’effetto dei filtri di smoothing (media mobile o gaussiano) è simile al partial volume effect (PVE) che avviene a livello di acquisizione. Tali filtri possono quindi essere utilizzati per simulare il PVE nel modello dell’immagine biomedica.\n\nI filtri a convoluzione spaziale possono essere applicati efficacemente anche per evidenziare i contorni in una immagine. Il gradiente di un’immagine f(x,y) nel punto (x,y) è il vettore:\\nabla f = \n\\begin{bmatrix}\nG_{x} \\\\\nG_{y} \n\\end{bmatrix} =\n\\begin{bmatrix}\n\\frac{\\partial f}{\\partial x} \\\\\n\\frac{\\partial f}{\\partial y} \n\\end{bmatrix}\n\nE’ noto che il vettore gradiente punta nella direzione di massima velocità di variazione di f nei punti (x,y). Pertanto nel problema della rivelazione dei bordi è importante l’ampiezza di questo vettore data da:\\nabla f =\n\\begin{bmatrix}\nG_{x}^{2} & G_{y}^{2} \n\\end{bmatrix}^{1/2}\n\nAnche la direzione del gradiente \\alpha(x,y) è una quantità importante:\\alpha(x,y) = tan^{-1} \n\\begin{bmatrix}\n\\frac{G_{x}}{G_{y}} \n\\end{bmatrix}\n\ndove l’angolo è misurato rispetto all’asse x. Notare che il calcolo del gradiente di un’immagine è basato sul calcolo delle derivate \\frac{\\partial f}{\\partial x} e \\frac{\\partial f}{\\partial y} per ogni pixel dell’immagine. Quindi il calcolo del gradiente di un’immagine deve essere fatto in due passi utilizzando due kernel: uno per la direzione x ed uno per la direzione y (immagini 2D) o tre passi (immagini 3D). E’ possibile anche definire un gradiente temporale in caso di immagini 2D+T o 3D+T.\nIn calcolo del gradiente implica il calcolo del valore della derivata da campioni discreti vista la natura discreta delle immagini. La derivata può essere approssimata in diversi modi. Per una maschera 3x3 il modo più semplice è il gradiente di Sobel:\\frac{\\partial f}{\\partial x} = \n\\begin{bmatrix}\n-1 & 0 & 1 \\\\\n-2 & 0 & 2 \\\\\n-1 & 0 & 1 \n\\end{bmatrix}\\frac{\\partial f}{\\partial y} = \n\\begin{bmatrix}\n1 & 2 & 1 \\\\\n0 & 0 & 0 \\\\\n-1 & -2 & -1 \n\\end{bmatrix}\n\nL’operazione di derivata basata sull’operatore di Sobel è data da:G_y = (w_{31} + 2w_{32} + w_{33}) - (w_{11} + 2w_{12} + w_{13})G_x = (w_{13} + 2w_{23} + w_{33}) - (w_{11} + 2w_{12} + w_{13})\n\nNotiamo che la formulazione precedente dei kernel è valida se \\Delta x=\\Delta y, cioè se la risoluzione spaziale sull’asse x è uguale a quella sull’asse y (pixel quadrato). Questa ipotesi è di solito verificata per immagini 2D, mentre se espandiamo il filtro in 3D non è in generale vera e si deve esplicitare nel kernel il valore della dimensione del voxel o interpolare il volume prima del filtraggio.\n\nUn’altra implementazione del filtro derivativo è il filtro di Prewitt.\\frac{\\partial f}{\\partial x} = \n\\begin{bmatrix}\n-1 & 0 & 1 \\\\\n-1 & 0 & 1 \\\\\n-1 & 0 & 1 \n\\end{bmatrix}\\frac{\\partial f}{\\partial y} = \n\\begin{bmatrix}\n1 & 1 & 1 \\\\\n0 & 0 & 0 \\\\\n-1 & -1 & -1 \n\\end{bmatrix}\n\nAnalogamente si può definire il Laplaciano che implementa la derivata seconda dell’immagine:\\nabla^2 f(x,y) = \\frac{\\partial^2 f}{\\partial x^2} +\n\\frac{\\partial^2 f}{\\partial y^2}\n\nche corrisponde al kernel discreto:\\nabla^2 =\n\\begin{bmatrix}\n0 & -1 & 0 \\\\\n-1 & 4 & -1 \\\\\n0 & -1 & 0\n\\end{bmatrix}\n\nL’immagine di gradiente ha la caratteristica di avere un valore elevato sui contorni dell’immagine e valore nullo nelle regioni ad intensità costante. Per eliminare il problema di pixel negativi che possono essere introdotti dal filtro possiamo normalizzare l’immagine o sommare un valore costante. Uno dei possibili usi dell’immagine di gradiente/laplaciano è di fungere da guida nel filtraggio con un filtro di smoothing, in pratica il filtraggio viene effettuato solo nelle regioni nelle quali il valore dell’immagine di gradiente è basso come vedremo in seguito.\nSe sommiamo all’immagine il laplaciano abbiamo un filtro che produce una maggiore definizione dei contorni (sharpening operator):\\nabla =\n\\begin{bmatrix}\n0 & -1 & 0 \\\\\n-1 & 5 & -1 \\\\\n0 & -1 & 0\n\\end{bmatrix}\n\nL’efficacia di questo filtro è dovuta alla capacità del sistema occhio-cervello di concentrarsi sui bordi eliminando le zone a intensità costante.\nFiltri laplaciani più complessi sono i filtri “a sombrero”.\n\nFigura 2.10. Esempi di filtri a sombrero.\n\nIn MATLAB i filtri derivativi possono essere ottenuti tramite la funzione fspecial.\nIl gradiente dell’immagine può inoltre essere calcolato direttamente mediante la funzione\nimgradient, che consente di utilizzare diversi metodi per il calcolo delle derivate. In Python non esiste una funzione unica equivalente a imgradient; tuttavia, funzionalità\nanaloghe sono fornite dalle librerie SciPy e scikit-image. In particolare, gli operatori\ndi gradiente più comuni sono disponibili in scipy.ndimage e skimage.filters, tra cui:\n\nSobel (scipy.ndimage.sobel)\n\nPrewitt (scipy.ndimage.prewitt)\n\nRoberts (skimage.filters.roberts)\n\nDifferenze centrali, calcolabili tramite derivate discrete o convoluzioni\n\nDifferenze in avanti, ottenibili mediante operatori di differenza finita\n\nIl modulo e la direzione del gradiente possono essere calcolati a partire dalle componenti\nlungo le direzioni orizzontali e verticali, rispettivamente come|\\nabla f| = \\sqrt{G_x^2 + G_y^2}, \\qquad\n\\theta = \\arctan\\left(\\frac{G_y}{G_x}\\right).","type":"content","url":"/c21-interpolazione-filtraggio-e-compressione-compl#operazioni-locali-e-filtraggio-spaziale","position":19},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl4":"Filtraggio adattivo","lvl3":"Operazioni locali e filtraggio spaziale","lvl2":"Filtraggio dell’immagine biomedica"},"type":"lvl4","url":"/c21-interpolazione-filtraggio-e-compressione-compl#filtraggio-adattivo","position":20},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl4":"Filtraggio adattivo","lvl3":"Operazioni locali e filtraggio spaziale","lvl2":"Filtraggio dell’immagine biomedica"},"content":"I filtri visti finora agiscono nello stesso modo su tutta l’immagine. Una classe di filtri più complessa è quella dei filtri anisotropici, che funzionano in modo diverso in regioni diverse dell’immagine. Questi filtri vengono detti anche adattivi in quanto adattano il loro comportamento sulla base del contenuto dell’immagine. Tipicamente un filtro anisotropico utilizza una informazione di gradiente per localizzare i bordi dell’immagine che devono essere preservati nell’operazione di filtraggio. Il filtraggio viene modulato in base al valore locale del gradiente.Un esempio semplice di un filtro adattivo è un filtro di smoothing che effettua lo smoothing solo dove il gradiente locale dell’immagine è inferiore ad un certo valore di soglia, mentre dove il valore è superiore lascia l’immagine inalterata.\nNella pratica per realizzare un semplice filtro adattivo si computa il gradiente dell’immagine I_{OR} attraverso un filtraggio locale come descritto in precedenza. Si ottiene così una immagine G di dimensioni uguale all’originale che contiene il gradiente dell’immagine stessa. Stabilita una soglia T_{g} si applica a G un filtro binario puntuale con soglia T_{g}, ottenendo una immagine mask_{HG}, che vale uno se il gradiente è maggiore di T_{g} e zero altrove. Una immagine binaria di questo tipo è spesso detta maschera (mask). Applicando un filtro puntuale di inversione a mask_{HG} si ottiene la sua inversa mask_{LG}, che rappresenta la maschera delle regioni dell’immagine con gradiente inferiore o uguale a T_{g}.\nA questo punto si applica un filtraggio locale a media mobile (smoothing) o gaussiano all’immagine I_{OR} ottenendo l’immagine filtrata I_{MM}, dove lo smoothing è applicato su tutta l’immagine. L’immagine filtrata in modo adattivo I_{FA} si ottiene dalla formula:I_{FA} = I_{OR} * mask_{HG} + I_{MM} * mask_{LG}\n\nInfatti essendo le due maschere mutuamente esclusive (la loro somma da uno su tutti i pixel) l’immagine risultante è uguale all’immagine originale nelle zone ad alto gradiente ed alla immagine filtrata nelle zone a basso gradiente.Le prestazioni di un filtro adattivo così realizzato dipendono ovviamente dal valore di T_{g}. Se T_{g} è troppo alto alcuni contorni verranno sfumati, se T_{g} è troppo basso regioni uniformi dell’immagine non verranno filtrate. Per individuare il valore ottimo di T_{g} è utile esprimere tale soglia in funzione delle statistiche del rumore associato all’immagine. Facciamo riferimento ad un modello semplificato di immagine biomedica dove l’immagine reale è data dalla somma dell’immagine reale e di rumore gaussiano con media nulla e deviazione standard \\sigma.I(x,y)=I_{0}(x,y)+n(x,y)\n\nSu un pattern omogeneo dell’immagine (dove vogliamo che venga applicato il filtro) il segnale sarà distribuito il modo gaussiano con media Sp (valore del segnale sul pattern) e deviazione standard \\sigma. Il valore del gradiente sul pattern intuitivamente dipenderà da \\sigma e dal tipo di filtro utilizzato per calcolare le due componenti del gradiente stesso. Nel caso semplificato in cui si computi la derivata come differenza tra il valore di due pixel adiacenti (kernel [1 0 -1]) il valore di Gx sara Ga(Sp,\\sigma)-Gb(Sp, \\sigma) dove Ga e Gb sono due realizzazioni indipendenti di un processo gaussiano con  media Sp e deviazione standard \\sigma. Sapendo che il 95\\% dei valori generati da un processo gaussiano è compreso nella finestra [-1.96\\sigma,1.96\\sigma] approssimabile a [-2\\sigma,2\\sigma] nel caso peggiore |Gx| potrà assumere il valore 4\\sigma e così per Gy, dando un gradiente complessivo massimo G = |Gx| + |Gy| = 8\\sigma. Si tratta evidentemente di un caso limite molto improbabile, specialmente nel caso che il kernel usato per computare il gradiente sia di dimensioni non ridottissime, ma che giustifica l’idea di imporre:T_{g} = k \\sigma\n\nEliminando quindi la dipendenza di T_{g} dal rumore. Il valore di \\sigma può essere calcolato in modo manuale o automatico utilizzando le tecniche viste nel calcolo del SNR e CNR. Il valore di k consente di regolare il funzionamento del filtro, il valore di k varia tipicamente tra 1 e 4.\nUn filtro adattivo così definito ha lo svantaggio principale di transire in modo brusco dalla zona in cui viene effettuato lo smoothing alla zona in cui l’immagine originale viene preservata, a causa dell’utilizzo di un filtro a gradino. L’altro svantaggio è di richiedere la definizione di k per stabilire la soglia.\n\nUn’alternativa più evoluta è rappresentata dal filtro di Wiener.\nIn MATLAB tale filtro è implementato tramite la funzione wiener2. In Python, un filtro equivalente è disponibile nella libreria SciPy\nmediante la funzione scipy.signal.wiener.\nIl metodo si basa sul calcolo delle statistiche locali dell’immagine:\nl’immagine viene inizialmente filtrata con un filtro a media mobile,\nottenendo I_{MM}, che rappresenta la media locale per ciascun pixel.\nSuccessivamente viene calcolata la mappa della varianza locale\nI_{VAR}, stimata sullo stesso kernel utilizzato per il filtro a media mobile.\n\nL’immagine filtrata risulta quindi:I_W = I_{MM} +\n\\frac{I_{VAR} - \\sigma^2}{I_{VAR}}\n\\left( I_{OR} - I_{MM} \\right)\n\nSe la varianza dell’immagine è uguale a quella del rumore (pattern uniforme) il secondo termine si annulla e l’immagine risultante è uguale a quella filtrata con il filtro a media mobile. Se I_{VAR} è molto grande rispetto al rumore dell’immagine l’immagine filtrata risulta uguale all’immagine originale. Nei due casi estremi il filtro di Wiener si comporta quindi come il filtro adattivo visto prima. Il vantaggio del filtro di Wiener è che gestisce in modo graduale la transizione tra le zone omogenee e le zone ad alto gradiente dove sono presenti i contorni. Inoltre non è richiesto di definire il valore di k ma solo di valutare il rumore sull’immagine \\sigma.\n\nUn possibile approccio per la stima automatica di \\sigma consiste nel far scorrere\nsull’immagine un kernel (ad esempio 5 \\times 5) e nel calcolare il valore della\ndeviazione standard SD sulla regione di immagine coperta dal kernel.\nIn MATLAB tale operazione è implementata tramite la funzione stdfilt. In Python, una funzionalità equivalente può essere ottenuta utilizzando la libreria\nSciPy, mediante la funzione scipy.ndimage.generic_filter, oppure calcolando la\ndeviazione standard locale a partire da media e varianza locali.\n\nL’operazione produce una mappa di valori di SD avente la stessa dimensione\ndell’immagine di partenza. Nelle regioni corrispondenti a pattern uniformi,\nil valore di SD approssima \\sigma nel caso di rumore gaussiano additivo.\nAl contrario, nelle regioni in prossimità dei bordi tra pattern differenti,\nil valore di SD risulta maggiore di \\sigma. Un esempio è riportato in Figura 2.11.\n\nFigura 2.11. Sinistra: immagine originale. Destra: mappa di deviazione standard.\n\nIn Figura 2.11. osserviamo una immagine formata da pattern omogenei con rumore gaussiano a media nulla e \\sigma=10 e la corrispondente mappa della SD. Come si nota dalla figura il valore di SD sulla mappa è massimo i corrispondenza dei bordi ed assume valori bassi e variabili nelle regioni omogenee. Questo è confermato dall’istogramma della mappa SD riportato in Figura 2.12. Che presenta un picco evidente in corrispondenza della SD misurata nelle zone omogenee con il massimo in corrispondenza della σ del rumore gaussiano imposto. Dalla mappa SD è possibile quindi valutare il valore di \\sigma utile per la configurazione del filtro adattivo. L’approccio più semplice trascura la SD delle regioni di transizione e stima σ come la media delle SD sulla mappa (nel caso in figura si ottiene \\sigma=11.4). Volendo eliminare gli outliers dovuti ai bordi è preferibile computare la mediana della mappa SD  (nel caso in Figura 2.12 si ottiene \\sigma=9.2). Un approccio più accurato estrae il primo picco dell’istogramma e ne computa la posizione del massimo.\nSe il rumore non è gaussiano si dovrà tenere conto di ciò nel calcolo di \\sigma. Ad esempio nelle immagini MR ci attendiamo che l’istogramma della SD map presenti due picchi, uno in corrispondenza dello sfondo ed un altro spostato a destra di un fattore 1.526 in corrispondenza alle regioni omogenee ad alto SNR. In questo caso il valore di σ andrà valutato direttamente sul secondo picco o sul primo introducendo l’opportuno fattore correttivo.\n\nFigura 2.12. Istogramma della mappa di deviazione standard.","type":"content","url":"/c21-interpolazione-filtraggio-e-compressione-compl#filtraggio-adattivo","position":21},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl4":"Adaptive Template Filtering","lvl3":"Operazioni locali e filtraggio spaziale","lvl2":"Filtraggio dell’immagine biomedica"},"type":"lvl4","url":"/c21-interpolazione-filtraggio-e-compressione-compl#adaptive-template-filtering","position":22},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl4":"Adaptive Template Filtering","lvl3":"Operazioni locali e filtraggio spaziale","lvl2":"Filtraggio dell’immagine biomedica"},"content":"Per “Adaptive Template Filtering” si indica un filtraggio basato sulla scelta adattiva di un determinato template (cioè un kernel di filtraggio) in base alle caratteristiche dell’immagine sottostante. Una implementazione possibile è quella proposta da C. B. Ahn.  L’idea di base è quella di avere una collezione di possibili template e di scegliere per ogni locazione dell’immagine un particolare template ottimizzato. L’obiettivo dell’algoritmo è quello di migliorare l’SNR evitando allo stesso tempo la perdita di definizione dei contorni. Il numero dei possibili template dipende dalla grandezza del template stesso ed è dato da:NT = \\sum_{k=2}^{N} C_N^{k}\n   = \\sum_{k=2}^{N} \\frac{n!}{k!(n-k)!}\n\nper un template 3 \\times 3 abbiamo chiaramente  N=9 e un numero totale di NT=255 configurazioni possibili. La Figura 2.13 mostra la collezione dei template 3x3. Essendo m il numero di 1 nel template, si ha un template per m=9, 8 template per m=8, 28 template per m=7, etc.\n\nFigura 2.13. Esempio di banco di filtri per filtraggio adattivo.\n\nLa scelta del template ottimo viene effettuata attraverso il calcolo di un opportuno indice. In particolare, viene calcolata la deviazione standard (SD) del valore dei pixel sul template, data da:\\sigma(k,l) =\n\\sqrt{\n\\frac{1}{m-1}\n\\sum_{i,j \\in T}\n\\left[ x(i,j) - \\bar{x}(k,l) \\right]^2\n}\\bar{x}(k,l) =\n\\frac{1}{m}\n\\sum_{i,j \\in T} x(i,j)\n\ndove x(i,j) sono i valori dei pixel nel template, (k,l) sono le coordinate del pixel da computare (e quindi le coordinate del centro del template), T_{j} è il template corrente e N_{j} è la dimensione m del template T_{j} . Per ogni pixel dell’immagine da filtrare, la SD viene calcolata per ogni possibile template. I template vengono divisi in due classi in base alla SD: I template con SD minore di una certa soglia (plane templates) e quelli con SD superiore alla soglia (edge templates).\nSe vengono riconosciuti uno o più plane template, viene scelto quello con dimensione maggiore. Se non ci sono plane template, viene selezionato  l’edge template con SD minima.\nIn realtà per diminuire i tempi di calcolo si procede esaminando i template in ordine di dimensione e fermandosi appena viene trovato un plane template. Nella sostanza l’algoritmo individua per ogni pixel la distribuzione spaziale dei pixel circostanti “simili” al pixel corrente, ed effettua l’operazione di smoothing tenendo conto solo di detti pixel. Il punto fondamentale dell’algoritmo è la scelta della soglia sulla SD. Similmente a quanto visto in precedenza la soglia è definita come:\\tau = a\\sigma_{n}\n\ndove è la SD del rumore sull’immagine e a è un fattore di scala, che assume un valore tra 1.2 e 1.6. La stima di \\sigma_{n} può essere effettuata come visto in precedenza per il filtro adattivo.","type":"content","url":"/c21-interpolazione-filtraggio-e-compressione-compl#adaptive-template-filtering","position":23},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl3":"Operatori globali ed equalizzazione dell’istogramma","lvl2":"Filtraggio dell’immagine biomedica"},"type":"lvl3","url":"/c21-interpolazione-filtraggio-e-compressione-compl#operatori-globali-ed-equalizzazione-dellistogramma","position":24},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl3":"Operatori globali ed equalizzazione dell’istogramma","lvl2":"Filtraggio dell’immagine biomedica"},"content":"Un esempio di filtraggio globale dell’immagine è rappresentato della procedura di equalizzazione dell’istogramma, che ha lo scopo di aumentare il contrasto percepito dell’immagine. Nella procedura di equalizzazione l’istogramma dell’immagine viene modificato in modo da divenire costante. Quindi se consideriamo l’immagine f con istogramma h(f), il filtro produrrà una immagine g con istogramma h(g) tale che h(g) = C per tutti i valori di livello di grigio di g. Si ha chiaramente C = N/p  dove N è il numero di pixel dell’immagine e p la profondità dell’immagine stessa.\nInoltre, deve essere conservato l’ordinamento dei livelli di grigio, per cui se f1< f2 deve risultare g1<g2, quindi la trasformazione deve essere monotona.\nIn pratica il filtro deve trovare “l’inversa” dell’istogramma di f, in modo che la funzione monotona T che caratterizza il filtro sia tale che:T(h(f)) = h^{-1}(f)h(f)  = C\n\nTale funzione è la distribuzione cumulativa (CDF) dell’istogramma da equalizzare, definita come:g_k = \\frac{p-1}{N} \\sum_{j=0}^{k} f_k,\n\\qquad k = 0,1,\\ldots,p-1\n\nConsideriamo il fantoccio TAC in Figura 2.14 ed il relativo istogramma (in blu). Per chiarezza è stata eliminata la prima riga dell’istogramma che risulta molto alta contenendo la zona di zero padding.\n\nFigura 2.14. Sinistra: fantoccio TAC. Destra: Istogramma (blu) e relativa CDF (rosso).\n\nLa CFD relativa all’istogramma è visualizzata in rosso. Come si osserva la CDF è “piatta” nelle regioni dove l’istogramma ha valori bassi mentre cresce ripidamente dove l’istogramma ha valori alti. Il filtro usa la CDF come “hash table” per definire la trasformazione implementata. Quindi ad esempio, i valori di livello di grigio dell’immagine tra 2000 e 3000 verranno sostituiti da valori molto simili dati dal valore della CDF che varia lentamente tra i due picchi a 2000 e 3000, “appiattendo” l’istogramma.\nL’immagine filtrata risulta come in Figura 2.15.\n\nFigura 2.15. Esempio di equalizzazione dell’istogramma.\n\nSi osserva come l’equalizzazione dell’istogramma “allarghi” i picchi e introduca una quantizzazione\ndei livelli di grigio, concentrandoli nelle regioni in cui la CDF risulta pressoché costante.\n\nIn Python l’equalizzazione dell’istogramma può essere implementata tramite la libreria\nscikit-image usando la funzione skimage.exposure.equalize_hist.\nA differenza di histeq di MATLAB, l’operazione può essere applicata anche a immagini a\nprofondità maggiore (ad esempio 16 bit), a patto di gestire correttamente la normalizzazione\ndell’intervallo di intensità. L’esempio è implementato nel Notebook 2.3.","type":"content","url":"/c21-interpolazione-filtraggio-e-compressione-compl#operatori-globali-ed-equalizzazione-dellistogramma","position":25},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl2":"Compressione dell’immagine biomedica"},"type":"lvl2","url":"/c21-interpolazione-filtraggio-e-compressione-compl#compressione-dellimmagine-biomedica","position":26},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl2":"Compressione dell’immagine biomedica"},"content":"Accenniamo infine al problema della compressione delle immagini biomediche. La compressione ha lo scopo di ridurre il numero di bit necessari per l’immagazzinamento dell’immagine biomedica. Il vantaggio è quello di ridurre la dimensione dei supporti informatici necessari all’immagazzinamento a breve e lungo termine e di migliorare la velocità di trasmissione delle immagini stesse nelle reti di comunicazione. Quest’ultima applicazione è quella oggi più rilevante. Lo svantaggio della compressione è che per usufruire dell’immagine compressa è necessario un algoritmo di decompressione che ripristini l’immagine originaria. In altri termini il software con cui si utilizza l’immagine deve implementare le caratteristiche DICOM di compressione, altrimenti l’immagine sarà inutilizzabile. La compressione viene quindi solitamente utilizzata o quando strettamente necessaria (telemedicina) o quando tutte le operazioni sull’immagine sono sotto il controllo di una singola struttura (ad esempio backup di un PACS). L’argomento verrà trattato più in dettaglio nel seguito quando si descriveranno i sistemi PACS.L’efficacia di una operazione di compressione si misura attraverso il rapporto  di  compressione che è  il rapporto tra le dimensioni dell’immagine dopo la compressione e le dimensioni originali. Il rapporto di compressione si può indicare in termini percentuali (25% o 0.25 indica che la dimensione dell’immagine compressa è il 25% di quella originale) o come X:1, dove X indica quante volte l’immagine originale è più grande di quella compressa (ad esempio un rapporto di compressione 4:1 indica che l’immagine compressa è 4 volte più piccola e corrisponde ad un rapporto percentuale del 25%).\nUna distinzione fondamentale è quella tra compressione  senza  perdite (lossless), che è reversibile nel senso che dall’immagine compressa è possibile ricostruire l’immagine originale, e compressione con perdite nella quale una parte dell’informazione associata all’immagine viene persa nel processo di conversione. La compressione con perdite permette rapporti di compressione molto maggiori, di almeno un ordine di grandezza. Un esempio di compressione senza perdite sono gli archivi digitali (zip, rar, etc). L’esempio tipico di compressione con perdite è il formato mp3 per i file audio e MPEG per i file video.In ogni caso perché sia possibile una compressione deve esistere una ridondanza, cioè il numero di bit che codificano l’immagine deve essere maggiore del numero di bit che descrivono il contenuto informativo dell’immagine stessa. Questo accade pressoché sempre nell’imaging biomedico, si pensi ad esempio al segnale di fondo che non apporta nessun contenuto informativo. Inoltre nelle immagini biomediche pixel vicini hanno spesso valori simili e questa caratteristica può utilmente essere usata in fase di compressione. Questo è vero anche in senso temporale nelle immagini dinamiche, dove si possono utilizzare algoritmi come l’MPEG.\nEsistono innumerevoli algoritmi di compressione con perdite e senza perdite. Tra i classici ricordiamo l’RLE (Run Lenght Encoding) che sfrutta le ripetizioni di byte uguali, l’LZW (Lempel Ziv Welch) che impiega le ripetizioni di stringhe di byte uguali, l’algoritmo di Huffman che utilizza codici di rappresentazione più brevi per i pixel che appaiono più frequentemente.\nTra gli algoritmi di compressione con perdite ricordiamo la DCT (Discrete Cosine Transform) e la compressione Wavelets che sfruttano la scomposizione dell’immagine in sub immagini con un diverso grado di dettaglio. Le immagini con grado di dettaglio molto elevato (corrispondenti a variazioni tra pixel molto vicine) vengono considerate rumore ed eliminate.\nNella compressione con perdite si definisce l’errore di compressione come l’errore quadratico medio misurato tra l’immagine originale I e l’immagine dopo la compressione/decompressione D:\\overline{e^2} =\n\\frac{1}{N^2}\n\\sum_{i=1}^{N}\n\\sum_{j=1}^{N}\n\\left( I(i,j) - D(i,j) \\right)^2\n\nsi definisce anche il rapporto segnale/errore di compressione (Figura 2.16) come:\\mathrm{PSNR} =\n10 \\log_{10}\n\\left(\n\\frac{\n\\displaystyle\n\\sum_{i=1}^{N}\n\\sum_{j=1}^{N}\nI^2(i,j)\n}{\n\\overline{e^2}\n}\n\\right)\n\nDi solito esisterà un rapporto inverso tra errore percentuale di compressione (PSNR) e rapporto di compressione (CR).\n\nFigura 2.16. Rapporto segnale/errore di compressione.\n\nNaturalmente nell’imaging biomedico è fondamentale anche il giudizio “visivo” dell’operatore. Il formato DICOM supporta JPEG, JPEG Lossless, JPEG 2000, and Run-length encoding (RLE). Il Lossless JPEG (compressione JPEG senza perdite) viene usato tipicamente in immagini angiografiche che sono di dimensioni elevate e sostanzialmente bimodali. Ottiene rapporti di compressione di qualche unità (fino a 4:1). Il formato JPEG 2000 supporta la compressione con perdite e senza perdite con tecnica DCT o Wavelet. Inoltre permette di scegliere livelli di compressione diversi in diverse regioni dell’immagine. È usato soprattutto in applicazioni di telemedicina.\nNell’ambito dell’imaging biomedico è importante notare come la compressione con perdite comporti la perdita di una parte del contenuto informativo, e va quindi utilizzata con estrema cautela.","type":"content","url":"/c21-interpolazione-filtraggio-e-compressione-compl#compressione-dellimmagine-biomedica","position":27},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl2":"Super-Resolution"},"type":"lvl2","url":"/c21-interpolazione-filtraggio-e-compressione-compl#super-resolution","position":28},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl2":"Super-Resolution"},"content":"Gli algoritmi di interpolazione descritti in precedenza possono ridurre la dimensione del voxel ma non possono ovviamente migliorare la risoluzione “fisica” dell’immagine che è limitata dall’effetto volume parziale. Modelliamo il caso di un nodulo di piccole dimensioni nel fegato e studiamo le conseguenze del PVE sulla possibilità di riconoscere il modulo in un sistema di imaging. Consideriamo una sfera di raggio r (ad esempio 1mm), formata da un materiale che produca un segnale S1 in un certo sistema di imaging, immersa in un altro materiale con associato un segnale S2, ed eseguiamo una scansione con voxel cubico di dimensione 2r \\times 2r \\times 2r. Il valore di CNR tra sfera e tessuto contenente come visto in precedenza sarà:CNR =\n\\frac{|M_1 - M_2|}{\\sigma}\n=\n\\frac{|p S_1 + (1-p) S_2 - S_2|}{\\sigma}\n=\n\\frac{p\\,|S_1 - S_2|}{\\sigma}\n\nDove M_1 e M_2 sono i segnali misurati. Infatti M_2 sarà uguale a S_2 mentre M_1 sarà dato dal contributo dei due tessuti nel voxel in dipendenza da p. La relazione ci dice che il CNR misurato si ottiene moltiplicando il valore di CNR ideale per il fattore p di riempimento del voxel. Nel caso migliore della sfera perfettamente inscritta nel cerchio  p=\\pi/6 mentre nel caso peggiore di sfera inclusa in modo simmetrico in otto voxel p=\\pi/48. Quindi il CNR rispetto al valore teorico si riduce di circa il 50% nel caso migliore fino a diventare circa il 6% nel caso peggiore. Se la soglia di CNR che assicura la visibilità della sfera è intermedia tra i due casi, non essendo prevedibile a livello di acquisizione il valore di p, la sfera risulterà visibile o meno in modo casuale in dipendenza dalla posizione della sfera stessa nel sistema di imaging. Consideriamo l’esempio semplificativo di Figura 2.17.\n\nFigura 2.17. Esempio di diversi valori di riempimento di un pixel per una cerchio di raggio r.\n\nA sinistra osserviamo una immagine ad alta risoluzione (HR) 2048x2048 di un fantoccio circolare. Il cerchio ha valore di segnale S_1=1000 mentre il fondo ha valore di segnale S=100. Il raggio del cerchio r è 128. Se interpoliamo a bassa risoluzione (LR) con passo 256 otteniamo una immagine 8x8, con dimensioni del pixel dell’immagine LR uguale alle dimensioni del cerchio. Se il pixel LR corrisponde esattamente al cerchio il cerchio si riduce ad un unico pixel di valore\n\\frac{\\pi r^{2}}{4\\pi r^{2}}S_{1} + \\frac{(4-\\pi)r^{2}}{4\\pi r^{2}}S_{2} \\approx 806 (immagine centrale). Se il cerchio è equi-diviso in quattro pixel LR avremo quattro pixel LR con valore diverso dal fondo pari a  valore \\frac{\\pi r^{2}}{16\\pi r^{2}}S_{1} + \\frac{(4-\\pi/4)r^{2}}{4\\pi r^{2}}S_{2} \\approx 276  (immagine a destra).\nFacendo variare lo “sfasamento” tra voxel LR e cerchio, si ottengono tutti i possibili valori di contrasto in dipendenza dallo sfasamento come riportato in Figura 2.18.\n\nFigura 2.18. Andamento del contrasto al variare dello sfasamento.\n\nTornando al caso reale, un nodulo con dimensioni dell’ordine della dimensione del voxel potrà essere visibile o meno sulla base della posizione del paziente rispetto al sistema di imaging in modo del tutto imprevedibile. La “certezza” di osservare l’oggetto si ha solo quando le sue dimensioni sono tali da saturare con certezza almeno un intero voxel. In teoria, se ripetiamo l’acquisizione un numero grande di volte variando la posizione dell’oggetto e/o la posizione della griglia di acquisizione, prima o poi otterremo il caso ottimale di sfera inscritta nel voxel e quindi visibile almeno in un set di immagini. Questa idea è alla base del concetto di super-imaging.\n\nL’idea alla base delle tecniche di Super-Resolution è quella di combinare immagini diverse della stessa struttura anatomica per ottenere una migliore risoluzione spaziale. Consideriamo  l’esempio di Figura 2.19, dove viene riportata una tipica acquisizione cardiaca MRI, nella quale vengono ottenute immagini cardiache lungo gli assi principali del cuore (asse corto, due camere, tre camere, quattro camere).\n\nFigura 2.19. Immagini multipiano del cuore in MR (4C, 2C, 3C, SA).\n\nOgni tipo di immagine è acquisita su di un piano diverso e presenta una risoluzione spaziale fortemente anisotropica (tipicamente 1.7x.17x8 mm). Nelle tecniche di super-resolution si cerca di combinare i voxel anisotropi in modo da migliorare la risoluzione spaziale sull’asse perpendicolare al piano di acquisizione (tecnicamente in MRI la slice-select direction). In Figura 2.20 sono riportati due esempi per tre acquisizioni su piani perpendicolari e per tre acquisizioni “interallacciate” in cui la distanza inter-slice è una frazione del thickness.\n\nFigura 2.20. Concetto di super-resolution (E van Reeth et al, Concepts in Magnetic Resonance part A 2012;40A:306-325).\n\nLo stesso concetto si può applicare ad una serie temporale di immagini, con la fondamentale differenza che si ha una deformazione della struttura dell’organo che va compensata con algoritmi di registrazione come sarà descritto nel seguito.\nIn generale il problema della super-resolution si può descrivere come (k=1,...,N):Y_k=D_k T_k h_k X+n_k\n\nY_k sono le N immagini a bassa risoluzione che abbiamo disponibili dal sistema di imaging (le immagini reali nel modello di immagine biomedica introdotta nel primo capitolo). D_k rappresenta in processo di “downsampling” che riduce il numero di pixel dell’immagine X. T_k rappresenta la trasformazione geometrica che descrive la posizione della griglia di acquisizione più l’eventuale deformazione indotta dal processo di acquisizione, ad esempio per i movimenti del paziente tra una acquisizione e l’altra. hk rappresenta l’effetto volume parziale che può essere modellato come la convoluzione con un kernel gaussiano in 2D o 3D. Nel caso di slice spacing nullo h_k è un filtro gaussiano 3D con kernel anisotropico con dimensione legata alla risoluzione spaziale lungo i tre assi. X è l’immagine “ideale” ad alta risoluzione. n_k è il rumore indotto dal sistema di imaging per il quale possono essere fatte le considerazione viste in precedenza per il modello dell’immagine biomedica.\nLo scopo di un algoritmo di super-resolution è stimare X note le immagini a bassa risoluzione Y_k.\nSe la risoluzione spaziale delle immagini Y_k è la stessa, h_1=h_2=...=h_N = h e D_1=D_2=...=D_N=D. Tipicamente anche il rumore avrà la stessa distribuzione in tutte le acquisizioni, quindi avremo:Y_k= D T_k h X+n\n\nIn generale il problema di stimare X da  Y_k è un tipico problema inverso, che è tipicamente “mal posto” e quindi non ha una soluzione univoca. Il modo più semplice di risolvere il problema è attraverso un algoritmo di “iterated back-projection”, in cui viene fatta una stima di X e si cerca di minimizzare in modo iterativo la differenza tra gli Y_k prodotti dalla stima di X e gli Y_k misurati (M Irani et al CVGIP 1991, doi: 10.1016/1049-9652(91)90045-L).\nPreliminarmente alla soluzione del problema inverso, è necessario stimare il disallineamento tra le immagini Y_k. In alcuni casi il disallineamento può essere noto, ad esempio dalle informazioni DICOM. Altrimenti il disallineamento deve essere stimato attraverso un algoritmo di registrazione di immagini che verrà introdotto nei capitoli successivi.\nL’idea di base dell’algoritmo è quella di partire da una stima dell’immagine HR X0 (ottenuta ad esempio dall’interpolazione di una immagine LR). A questo punto da X0 si stimano le immagini LR sulla base della conoscenza di D_k, che come prima detto si considera nota, e di h che è un filtro opportuno che tiene conto del PVE. Si ottiene così la stima di Y_k al passo 1 come:\\hat{Y}_k^{(1)}=D T_k h X_0\n\nQuesto passaggio simula il processo di acquisizione delle immagini LR, quindi h e D devono essere definiti in modo congruente al rapporto tra le risoluzioni spaziali delle immagini HR e LR.\nA questo punto l’algoritmo calcola la differenza tra l’immagine LR stimata e quella reale:G_k=Y_k-\\hat{Y}_k^{(1)}\n\nIl gradiente G utilizzato per ottenere la nuova stima di X è:G = \\sum_{k=1}^{N} D T_k^{-1} G_k\n\nDove T_k^{-1} rappresenta la trasformazione geometrica inversa che riallinea le mappe G. Nota G, si calcola il nuovo valore di X come:\\hat{X}_1 = \\hat{X}_0 + \\lambda G*H\n\nDove \\lambda determina la velocità della convergenza (un valore alto rischia di non assicurare la convergenza, un valore basso aumenta il numero di iterazioni necessarie ad ottenere una soluzione). H rappresenta il kernel di un filtro che ottimizza la convergenza, e che può essere scelto in modo arbitrario. Per le immagini biomediche standard la scelta ottima è di solito H = h.\nIl processo prosegue in modo iterativo\\hat{X}^{(n+1)} = \\hat{X}^{(n)} + \\lambda G^{(n)}*H\n\nFino a quando la differenza percentuale tra i valori di X stimati a due passi successivi non scende sotto una certa soglia. Si dimostra che l’algoritmo minimizza l’errore quadratico medio tra le Y_k misurate e quelle stimate:\\varepsilon^{(n)} =\n\\sqrt{\n\\sum_{k=1}^{N}\n\\left( Y_k - \\hat{Y}_k^{(n)} \\right)^2\n}\n\nEssendo il problema mal posto, la soluzione non è unica e quindi può dipendere dalla scelta delle condizioni iniziali \\hat{X}^{(0)}. Tipicamente \\hat{X}^{(0)} viene ottenuta interpolando una immagine a bassa risoluzione Y_k riportandola alle dimensioni dell’immagine ad alta risoluzione. Per risolvere il problema si possono inserire delle condizioni di regolarizzazione, che introducono delle ipotesi a priori sul tipo di soluzione che si desidera raggiungere. L’ipotesi tipica è che X sia “smooth”, cioè che non siano possibili transizioni troppo brusche tra i livelli di grigio. In questo caso viene minimizzata la metrica:\\varepsilon =\n\\sum_{k=1}^{N}\n\\left( Y_k - \\hat{Y}_k^{(n)} \\right)^2\n+ \\gamma \\lVert C \\hat{X} \\rVert^2\n\nDove C rappresenta un filtraggio di tipo passa-alto, quindi ad esempio il gradiente dell’immagine, e γ è una costante che rappresenta il peso della regolarizzazione nel processo di minimizzazione della metrica. Chiaramente la presenza del filtro C penalizza le soluzioni con bruschi cambiamenti di segnale ottenendo la regolarizzazione.\n\nSono stati proposti molti altri algoritmi per la soluzione del problema inverso in super-resolution, quali il  Deterministic Regularized Approach,  lo Statistical Regularized Approach e altri. Per maggiori informazioni si può fare riferimento alle review di Van Reeth et al (doi: 10.1002/cmra.21249) e H Greenspan et al (doi:10.1093/comjnl/bxm075).","type":"content","url":"/c21-interpolazione-filtraggio-e-compressione-compl#super-resolution","position":29},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl2":"Bibliografia"},"type":"lvl2","url":"/c21-interpolazione-filtraggio-e-compressione-compl#bibliografia","position":30},{"hierarchy":{"lvl1":"Capitolo 2: Interpolazione, filtraggio e compressione","lvl2":"Bibliografia"},"content":"R Rangayyan, Biomedical Image Analysis, CRC Press 2004\n\nM Irani et al CVGIP 1991, doi: 10.1016/1049-9652(91)90045-L\n\nVan Reeth et al (doi: 10.1002/cmra.21249) e H Greenspan et al (doi:10.1093/comjnl/bxm075)","type":"content","url":"/c21-interpolazione-filtraggio-e-compressione-compl#bibliografia","position":31},{"hierarchy":{"lvl1":"Esempi di interpolazione - Notebook 2.1"},"type":"lvl1","url":"/n-2-1-esempi-interpolazione","position":0},{"hierarchy":{"lvl1":"Esempi di interpolazione - Notebook 2.1"},"content":"Import delle librerie necessarie per la simulazione\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.ndimage as ndi\nfrom scipy.interpolate import interpn\nfrom scipy.spatial.distance import euclidean\nfrom scipy.ndimage import zoom\nimport pydicom\nimport os\nimport imageio\nimport napari\n\n\n\n","type":"content","url":"/n-2-1-esempi-interpolazione","position":1},{"hierarchy":{"lvl1":"Esempi di interpolazione - Notebook 2.1","lvl2":"Lettura file DICOM"},"type":"lvl2","url":"/n-2-1-esempi-interpolazione#lettura-file-dicom","position":2},{"hierarchy":{"lvl1":"Esempi di interpolazione - Notebook 2.1","lvl2":"Lettura file DICOM"},"content":"Import dei dati DICOM da cartella specifica e visualizzazione 3D del dataset tramite la libreria napari.\n\n# set directory\ndirectory = '../data/Phantom_CT_PET/2-CT 2.5mm-5.464/'\n\n# get all files in directory\nfiles = sorted(os.listdir(directory))\nprint(files)\n\n# get info from dicom\nds = pydicom.dcmread(directory + files[0])\nprint(\"Patient ID:\", ds.PatientID)\n\n# get size along 3 axis\nxSize = ds.Rows\nySize = ds.Columns\nzSize = len(files)\nprint(xSize, ySize, zSize)\n\n# get resolution\ndx = ds.PixelSpacing[0]\ndy = ds.PixelSpacing[1]\nloc1 = ds.ImagePositionPatient\nds2  = pydicom.dcmread(directory + files[1])\nloc2 = ds2.ImagePositionPatient\ndz = euclidean(loc1, loc2)\n\n# import volume\nvol = imageio.volread(directory, 'DICOM')\nFOV = [dx*xSize,dy*ySize,dz*zSize]\nprint(\"FOV:\", FOV)\n\n# check  size\nprint(\"dimensioni del dato:\", vol.shape)\n\n#use napari\nviewer = napari.view_image(vol, name = \"my volume\", colormap = \"gray\")\n\n#run the viewer\nnapari.run()\n\n\n\n\n\n","type":"content","url":"/n-2-1-esempi-interpolazione#lettura-file-dicom","position":3},{"hierarchy":{"lvl1":"Esempi di interpolazione - Notebook 2.1","lvl2":"Interpolazione tramite interpn"},"type":"lvl2","url":"/n-2-1-esempi-interpolazione#interpolazione-tramite-interpn","position":4},{"hierarchy":{"lvl1":"Esempi di interpolazione - Notebook 2.1","lvl2":"Interpolazione tramite interpn"},"content":"Esempio di interpolazione tramite l’utilizzo della funzione interpn e visualizzazione 3D tramite la libreria napari.\n\n# original coordinates\nx = np.arange(0,xSize*dx,dx)\ny = np.arange(0,ySize*dy,dy)\nz = np.arange(0,zSize*dz,dz)\n\n# new coordinates\nnew_x = np.arange(0,xSize*dx,dx)\nnew_y = np.arange(0,ySize*dy,dy)\nnew_z = np.arange(0,(zSize-1)*dz,dx)\n\n# mesh of new coordinates\nnew_coords = np.meshgrid(new_z,new_x, new_y, indexing = 'ij');\nvolumeInt = interpn((z,x,y), vol, np.stack(new_coords,axis=-1))\n\n# check new size\nprint(\"dimensioni dopo interpolazione:\", volumeInt.shape)\n\n# use napari\nviewer = napari.view_image(volumeInt, name = \"my volume interpolated\", colormap = \"gray\")\n\n# run the viewer\nnapari.run()\n\n\n\n\n\n","type":"content","url":"/n-2-1-esempi-interpolazione#interpolazione-tramite-interpn","position":5},{"hierarchy":{"lvl1":"Esempi di interpolazione - Notebook 2.1","lvl2":"Interpolazione tramite zoom"},"type":"lvl2","url":"/n-2-1-esempi-interpolazione#interpolazione-tramite-zoom","position":6},{"hierarchy":{"lvl1":"Esempi di interpolazione - Notebook 2.1","lvl2":"Interpolazione tramite zoom"},"content":"Interpolazione nel piano tramite l’utilizzo della funzione zoom\n\n# plane interpolation\nimg = vol[33,:,:]\n\n# original image\nfig, ax = plt.subplots()\nax.imshow(img, cmap = \"gray\")\nax.set_title(\"Original\")\nplt.show()\n\n# set scaling factor\nscaling_factors = (2, 2)\n\n\n\n","type":"content","url":"/n-2-1-esempi-interpolazione#interpolazione-tramite-zoom","position":7},{"hierarchy":{"lvl1":"Esempi di interpolazione - Notebook 2.1","lvl2":"Interpolazione NN"},"type":"lvl2","url":"/n-2-1-esempi-interpolazione#interpolazione-nn","position":8},{"hierarchy":{"lvl1":"Esempi di interpolazione - Notebook 2.1","lvl2":"Interpolazione NN"},"content":"\n\n# nearest neighbor\nresized_image = ndi.zoom(img, zoom = scaling_factors, order = 0)\nfig, ax = plt.subplots()\nax.imshow(img, cmap = \"gray\")\nax.set_title(\"NN\")\nplt.show()\n\n\n\n","type":"content","url":"/n-2-1-esempi-interpolazione#interpolazione-nn","position":9},{"hierarchy":{"lvl1":"Esempi di interpolazione - Notebook 2.1","lvl2":"Interpolazione bilineare"},"type":"lvl2","url":"/n-2-1-esempi-interpolazione#interpolazione-bilineare","position":10},{"hierarchy":{"lvl1":"Esempi di interpolazione - Notebook 2.1","lvl2":"Interpolazione bilineare"},"content":"\n\n# bilinear\nresized_image = ndi.zoom(img, zoom = scaling_factors, order = 1)\nfig, ax = plt.subplots()\nax.imshow(img, cmap = \"gray\")\nax.set_title(\"Bilinear\")\nplt.show()\n\n\n\n\n","type":"content","url":"/n-2-1-esempi-interpolazione#interpolazione-bilineare","position":11},{"hierarchy":{"lvl1":"Esempi di interpolazione - Notebook 2.1","lvl2":"Interpolazione bicubica"},"type":"lvl2","url":"/n-2-1-esempi-interpolazione#interpolazione-bicubica","position":12},{"hierarchy":{"lvl1":"Esempi di interpolazione - Notebook 2.1","lvl2":"Interpolazione bicubica"},"content":"\n\n# bicubic\nresized_image = ndi.zoom(img, zoom = scaling_factors, order = 3)\nfig, ax = plt.subplots()\nax.imshow(img, cmap = \"gray\")\nax.set_title(\"Bicubic\")\nplt.show()\n\n\n","type":"content","url":"/n-2-1-esempi-interpolazione#interpolazione-bicubica","position":13},{"hierarchy":{"lvl1":"Esempi di filtraggio - Notebook 2.2"},"type":"lvl1","url":"/n-2-2-esempi-filtraggio","position":0},{"hierarchy":{"lvl1":"Esempi di filtraggio - Notebook 2.2"},"content":"Import delle librerie necessarie per la simulazione\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.ndimage as ndi\nimport pydicom\nimport os\nimport imageio\nfrom scipy.spatial.distance import euclidean\n\n\n\n","type":"content","url":"/n-2-2-esempi-filtraggio","position":1},{"hierarchy":{"lvl1":"Esempi di filtraggio - Notebook 2.2","lvl2":"Lettura file DICOM"},"type":"lvl2","url":"/n-2-2-esempi-filtraggio#lettura-file-dicom","position":2},{"hierarchy":{"lvl1":"Esempi di filtraggio - Notebook 2.2","lvl2":"Lettura file DICOM"},"content":"Esempio di import di uno o più file DICOM e visualizzazione di una fetta del fantoccio\n\n# set directory\ndirectory = '../data/Phantom_CT_PET/2-CT 2.5mm-5.464/'\n\n# get all files in directory\nfiles = sorted(os.listdir(directory))\n\n# get info from dicom\nds = pydicom.dcmread(directory + files[0])\nprint(\"Patient ID:\", ds.PatientID)\n\n# get size along 3 axis\nxSize = ds.Rows\nySize = ds.Columns\nzSize = len(files)\nprint(xSize, ySize, zSize)\n\n# get resolution\ndx = ds.PixelSpacing[0]\ndy = ds.PixelSpacing[1]\nloc1 = ds.ImagePositionPatient\nds2  = pydicom.dcmread(directory + files[1])\nloc2 = ds2.ImagePositionPatient\ndz = euclidean(loc1, loc2)\nprint(\"dimesioni del dato:\", dx, dy, dz)\n\n# import volume\nvol = imageio.volread(directory, 'DICOM')\nFOV = [dx*xSize,dy*ySize,dz*zSize]\nprint(\"FOV:\", FOV)\nprint(vol.shape)\n\n# get plane\nimg = vol[9,:,:]\nprint(img.shape)\n\n# original image\nfig, ax = plt.subplots()\nax.imshow(img, cmap = 'gray')\nax.set_title(\"immagine originale\")\nplt.show()\n\n\n\n\n\n","type":"content","url":"/n-2-2-esempi-filtraggio#lettura-file-dicom","position":3},{"hierarchy":{"lvl1":"Esempi di filtraggio - Notebook 2.2","lvl2":"Filtro a media mobile"},"type":"lvl2","url":"/n-2-2-esempi-filtraggio#filtro-a-media-mobile","position":4},{"hierarchy":{"lvl1":"Esempi di filtraggio - Notebook 2.2","lvl2":"Filtro a media mobile"},"content":"è possibile cambiare la dimensione del filtro per apprezzarne l’effetto sull’immagine filtrata.\n\n# moving average filter\nweights = [[1/9, 1/9, 1/9],\n           [1/9, 1/9, 1/9],\n           [1/9, 1/9, 1/9]]\n\nimg_ma = ndi.convolve(img, weights)\n\nfig, ax = plt.subplots(1,2)\nax[0].imshow(img_ma, cmap = 'gray')\nax[0].set_title(\"immagine filtrata con media mobile\")\nax[1].imshow(img-img_ma, cmap = 'gray')\nax[1].set_title(\"immagine differenza\")\nplt.show()\n\n\n\n\n\n","type":"content","url":"/n-2-2-esempi-filtraggio#filtro-a-media-mobile","position":5},{"hierarchy":{"lvl1":"Esempi di filtraggio - Notebook 2.2","lvl2":"Filtro Gaussiano"},"type":"lvl2","url":"/n-2-2-esempi-filtraggio#filtro-gaussiano","position":6},{"hierarchy":{"lvl1":"Esempi di filtraggio - Notebook 2.2","lvl2":"Filtro Gaussiano"},"content":"è possibile variare la dimensione del filtro e la \\sigma per apprezzarne l’effetto sull’immagine filtrata.\n\n# gaussian average filter\nimg_gaussian = ndi.gaussian_filter(img, sigma = 3)\n\nfig, ax = plt.subplots(1,2)\nax[0].imshow(img_gaussian, cmap = 'gray')\nax[0].set_title(\"immagine filtrata con media gaussiana\")\nax[1].imshow(img-img_gaussian, cmap = 'gray')\nax[1].set_title(\"immagine differenza\")\nplt.show()\n\n\n\n\n\n","type":"content","url":"/n-2-2-esempi-filtraggio#filtro-gaussiano","position":7},{"hierarchy":{"lvl1":"Esempi di filtraggio - Notebook 2.2","lvl2":"Filtro mediano"},"type":"lvl2","url":"/n-2-2-esempi-filtraggio#filtro-mediano","position":8},{"hierarchy":{"lvl1":"Esempi di filtraggio - Notebook 2.2","lvl2":"Filtro mediano"},"content":"notare il mantenimento dei bordi rispetto al filtro a media mobile o a quello Gaussiano.\n\n# median filter\nimg_median = ndi.median_filter(img, size = 3)\n\nfig, ax = plt.subplots(1,2)\nax[0].imshow(img_median, cmap = 'gray')\nax[0].set_title(\"immagine filtrata con mediana\")\nax[1].imshow(img-img_median, cmap = 'gray')\nax[1].set_title(\"immagine differenza\")\nplt.show()\n\n\n\n\n\n","type":"content","url":"/n-2-2-esempi-filtraggio#filtro-mediano","position":9},{"hierarchy":{"lvl1":"Esempi di filtraggio - Notebook 2.2","lvl2":"Filtro di Sobel"},"type":"lvl2","url":"/n-2-2-esempi-filtraggio#filtro-di-sobel","position":10},{"hierarchy":{"lvl1":"Esempi di filtraggio - Notebook 2.2","lvl2":"Filtro di Sobel"},"content":"esempio di filtro derivativo per enfatizzare i contorni.\n\n# sobel filter\nimg_sobel_0 = ndi.sobel(img, axis = 0)\n\nfig, ax = plt.subplots(1,2)\nax[0].imshow(img_sobel_0, cmap = 'gray')\nax[0].set_title(\"immagine filtrata con Sobel lungo l'asse 0\")\nax[1].imshow(img-img_sobel_0, cmap = 'gray')\nax[1].set_title(\"immagine differenza\")\nplt.show()\n\nimg_sobel_1 = ndi.sobel(img, axis = 1)\n\nfig, ax = plt.subplots(1,2)\nax[0].imshow(img_sobel_1, cmap = 'gray')\nax[0].set_title(\"immagine filtrata con Sobel lungo l'asse 1\")\nax[1].imshow(img-img_sobel_1, cmap = 'gray')\nax[1].set_title(\"immagine differenza\")\nplt.show()\n\nimg_sobel = np.sqrt(img_sobel_0**2 + img_sobel_1**2)\n\nfig, ax = plt.subplots(1,2)\nax[0].imshow(img_sobel, cmap = 'gray')\nax[0].set_title(\"immagine filtrata lungo entrambi gli assi\")\nax[1].imshow(img-img_sobel, cmap = 'gray')\nax[1].set_title(\"immagine differenza\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/n-2-2-esempi-filtraggio#filtro-di-sobel","position":11},{"hierarchy":{"lvl1":"Esempi di filtraggio - Notebook 2.2","lvl2":"Filtro laplaciano"},"type":"lvl2","url":"/n-2-2-esempi-filtraggio#filtro-laplaciano","position":12},{"hierarchy":{"lvl1":"Esempi di filtraggio - Notebook 2.2","lvl2":"Filtro laplaciano"},"content":"altro tipo di filtro derivativo. Notare le differenze con il filtro di Sobel.\n\n# laplace filter\nimg_laplace = ndi.laplace(img)\nfig, ax = plt.subplots(1,2)\nax[0].imshow(img_laplace, cmap = 'gray')\nax[0].set_title(\"immagine filtrata con Laplace\")\nax[1].imshow(img-img_laplace, cmap = 'gray')\nax[1].set_title(\"immagine differenza\")\nplt.show()\n\n\n\n","type":"content","url":"/n-2-2-esempi-filtraggio#filtro-laplaciano","position":13},{"hierarchy":{"lvl1":"Equalizzazione dell’istogramma - Notebook 2.3"},"type":"lvl1","url":"/n-2-3-hist-equalization","position":0},{"hierarchy":{"lvl1":"Equalizzazione dell’istogramma - Notebook 2.3"},"content":"Import delle librerie necessarie per la simulazione\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport imageio\n\n\n\n","type":"content","url":"/n-2-3-hist-equalization","position":1},{"hierarchy":{"lvl1":"Equalizzazione dell’istogramma - Notebook 2.3","lvl2":"Import dati DICOM"},"type":"lvl2","url":"/n-2-3-hist-equalization#import-dati-dicom","position":2},{"hierarchy":{"lvl1":"Equalizzazione dell’istogramma - Notebook 2.3","lvl2":"Import dati DICOM"},"content":"\n\nI = imageio.v2.imread(\"../data/phantom_15.dcm\")\n\n\n\n\n","type":"content","url":"/n-2-3-hist-equalization#import-dati-dicom","position":3},{"hierarchy":{"lvl1":"Equalizzazione dell’istogramma - Notebook 2.3","lvl2":"Equalizzazione dell’istogramma"},"type":"lvl2","url":"/n-2-3-hist-equalization#equalizzazione-dellistogramma","position":4},{"hierarchy":{"lvl1":"Equalizzazione dell’istogramma - Notebook 2.3","lvl2":"Equalizzazione dell’istogramma"},"content":"\n\n# (opzionale) se I può avere negativi, porta il minimo a 0\nI = I - I.min()\n\n# livelli (assumo immagine discreta intera)\np = int(I.max()) + 1\nN = I.size\nprint(\"N =\", N)\n\n# --- escludo gli zeri prima di equalizzare ---\nmask = I > 0\nNnz = int(mask.sum())\nprint(\"N (non-zero) =\", Nnz)\n\n# histogram SOLO sui pixel > 0\ncounts = np.bincount(I[mask].astype(np.int64), minlength=p)\ncounts[0] = 0\n\n# CDF normalizzata\nCDF = np.cumsum(counts)\nCDF = CDF / CDF[-1]  # ora è in [0,1]\n\n# visualizza (PDF e CDF)\nfig, ax = plt.subplots(1, 2, figsize=(10, 4))\nax[0].imshow(I, cmap=\"gray\")\nax[0].set_title(\"Immagine originale (shifted)\")\n\nax[1].plot(counts / counts.sum(), label=\"PDF (I>0)\")\nax[1].plot(CDF, label=\"CDF (I>0)\")\nax[1].set_title(\"Istogramma e CDF (esclusi gli zeri)\")\nax[1].legend()\nplt.show()\n\n# LUT per equalizzazione\nLUT = np.floor((p - 1) * CDF).astype(np.uint16)\n\n# applico solo dove I>0\nIeq = np.zeros_like(I)\nIeq[mask] = LUT[I[mask].astype(np.int64)]\n\n# histogram equalizzata \ncounts_eq = np.bincount(Ieq[mask].astype(np.int64), minlength=p)\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 4))\nax[0].imshow(Ieq, cmap=\"gray\")\nax[0].set_title(\"Immagine equalizzata (solo I>0)\")\n\nax[1].plot(counts_eq / counts_eq.sum())\nax[1].set_title(\"Istogramma equalizzato (solo I>0)\")\nplt.show()\n\n\n\n\n\n\n\n\n","type":"content","url":"/n-2-3-hist-equalization#equalizzazione-dellistogramma","position":5},{"hierarchy":{"lvl1":"Esercitazione 2: stima dell’effetto di diversi tipi di filtraggio su alcune misure di qualità dell’immagine biomedica"},"type":"lvl1","url":"/z-2-1-esercitazione","position":0},{"hierarchy":{"lvl1":"Esercitazione 2: stima dell’effetto di diversi tipi di filtraggio su alcune misure di qualità dell’immagine biomedica"},"content":"Scopo dell’esercitazione è realizzare un programma che consenta di valutare vari algoritmi di filtraggio 3D su di un fantoccio MR (Figura 2.21) in termini di SNR e conservazione delle transizioni.\n\nFigura 2.21. Fantoccio MR.\n\nIl fantoccio che utilizziamo è un fantoccio MR costituito da tre cilindri concentrici disassati. L’intercapedine tra il secondo ed il terzo cilindro è riempita con un liquido ad alto contrasto, le altre due con acqua. E’ stata acquisita una serie di fette assiali rispetto al fantoccio contenute nella directory volume3D.\n\nPoiché vogliamo applicare un filtraggio 3D, il primo passo è quello di verificare se il volume di dati è isotropo leggendo gli opportuni campi DICOM e nel caso non lo sia operare una interpolazione per ottenere un volume isotropo. La procedura di interpolazione dovrà conservare il FOV del volume originale. In MATLAB è possibile utilizzare le funzioni interp3 in combinazione con meshgrid,\noppure la funzione imresize3 con le opportune opzioni di interpolazione.\nIl volume ottenuto può essere visualizzato tramite volumeViewer\n(disponibile a partire da MATLAB R2017a). In Python, l’interpolazione tridimensionale può essere effettuata utilizzando le funzioni\nRegularGridInterpolator o interpn della libreria SciPy, oppure, nel caso di volumi\nisotropi con fattore di scala uniforme, tramite funzioni di ridimensionamento come\nscipy.ndimage.zoom o skimage.transform.resize.\n\nIl volume risultante può essere visualizzato mediante viewer tridimensionali dedicati,\ncome napari, che consente l’esplorazione interattiva di dati volumetrici.\n\nUna volta ottenuto il volume interpolato, vogliamo sperimentare vari tipi di filtraggi in 3D e valutare l’SNR e la conservazione delle transizioni sul cilindro a massimo segnale. Per semplicità calcoleremo l’SNR e l’acutezza su una singola fetta in 2D (quella centrale) attraverso la definizione di una opportuna ROI sul cilindro centrale (acqua) e di un profilo verticale posto al centro dell’immagine.\n\nGli algoritmi di filtraggio da sperimentare includono:\n\nUn filtro a media mobile con kernel 7x7x7\n\nUn filtro gaussiano con kernel 7x7x7 e valore di sigma ottimizzato per massimizzare l’SNR e conservare le transizioni.\n\nUn filtro adattivo di Wiener con kernel 7x7x7\n\nI primi due filtri sono di tipo convolutivo e, in MATLAB, possono essere implementati tramite\nle funzioni fspecial3 (MATLAB ≥ R2018b) e imfilter, oppure mediante la funzione conv3\ndefinendo opportuni kernel tridimensionali. Per il filtro gaussiano è inoltre disponibile\nla funzione imgaussfilt3 (MATLAB ≥ R2018b), che esegue direttamente il filtraggio senza la\nnecessità di utilizzare fspecial. In Python, filtraggi convolutivi tridimensionali possono essere implementati utilizzando la funzione scipy.ndimage.convolve o scipy.signal.convolve, definendo esplicitamente\ni kernel 3D. Nel caso del filtro gaussiano, è disponibile una funzione dedicata, scipy.ndimage.gaussian_filter, che applica direttamente il filtraggio gaussiano al volume senza la necessità di definire manualmente il kernel.\n\nIl filtro di Wiener bidimensionale è implementato in MATLAB dalla funzione wiener2.\nTuttavia, non è disponibile una versione tridimensionale della funzione, che deve quindi\nessere implementata esplicitamente a partire dalla formulazione teorica del filtro di Wiener. In Python, una funzione dedicata per il filtro di Wiener 3D non è disponibile come routine\nad alto livello; pertanto, il filtraggio deve essere realizzato calcolando esplicitamente\nle statistiche locali del volume.\n\nLa formulazione del filtro di Wiener è data da:I_W = I_{\\mathrm{MM}} +\n\\frac{\\lvert I_{\\mathrm{VAR}} - \\sigma^2 \\rvert}{I_{\\mathrm{VAR}}}\n\\left( I_{\\mathrm{OR}} - I_{\\mathrm{MM}} \\right)\n\ndove I_{\\mathrm{OR}} rappresenta il volume originale tridimensionale,\nI_{\\mathrm{MM}} è il volume filtrato tramite media mobile (calcolato al passo precedente), I_{\\mathrm{VAR}} è la mappa della varianza locale, e \\sigma è la deviazione standard\ndel rumore associato all’immagine. In Python, l’implementazione del filtro di Wiener 3D può essere ottenuta combinando funzioni di convoluzione tridimensionale della libreria SciPy (ad esempio\nscipy.ndimage.convolve o scipy.ndimage.uniform_filter) per il calcolo della media e\ndella varianza locali, seguite dall’applicazione diretta della formula del filtro.\n\nI_{\\mathrm{VAR}} è la mappa della varianza sull’immagine.\nIn teoria, essendo I_{\\mathrm{VAR}}>\\sigma^2, il fattore di modulazione del filtro di Wiener \\frac{I_{\\mathrm{VAR}}-\\sigma^2}{I_{\\mathrm{VAR}}} dovrebbe variarare tra 0 e 1. In pratica tale valore diverge per valori di I_{\\mathrm{VAR}} nulli (regioni di zero padding) o per valori di I_{\\mathrm{VAR}} molto piccoli. Tali regioni vanno quindi escluse dal filtraggio se presenti oppure bisogna porre \\frac{I_{\\mathrm{VAR}}-\\sigma^2}{I_{\\mathrm{VAR}}}=1 dove assume valori non definiti (isnan, isinf) o maggiori di uno. Inoltre, sul fondo dell’immagine MR I_{\\mathrm{VAR}} sarà minore di \\sigma^2 per le proprietà del rumore riciano, per cui il fattore di modulazione del filtro \\frac{I_{\\mathrm{VAR}}-\\sigma^2}{I_{\\mathrm{VAR}}}  non si annullerà riducendo il filtraggio, cosa comunque irrilevante visto che interessa filtrare le regioni dove è presente un segnale.\n\nPer il calcolo della mappa di varianza locale I_{\\mathrm{VAR}}, in MATLAB è possibile\nutilizzare la funzione stdfilt, tenendo conto che il parametro nhood deve essere definito\nopportunamente per ottenere un filtraggio tridimensionale (funzionalità non documentata);\nin caso contrario, il filtraggio viene effettuato in modalità bidimensionale, slice-by-slice.\n\nIn Python, il calcolo della varianza locale tridimensionale può essere implementato\nesplicitamente utilizzando la libreria SciPy, ad esempio tramite la funzione\nscipy.ndimage.uniform_filter. In particolare, la varianza locale può essere ottenuta\na partire dalla relazione\\mathrm{Var}(X) = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2,\n\ncalcolando media e media dei quadrati su una finestra tridimensionale.\n\nPer il calcolo di \\sigma vogliamo implementare un metodo automatico che non richieda il tracciamento di ROI o una conoscenza a priori della geometria del fantoccio, quindi applicheremo un metodo automatico basato sull’analisi dell’istogramma di I_{\\mathrm{VAR}}.\nPer quanto riguarda la valutazione dell’acutezza, un profilo estratto dal centro dell’immagine del fantoccio non filtrato apparirà come in Figura 2.22, infatti il segnale in corrispondenza della parete dei cilindri (plexiglass) in MR è nullo a meno del rumore. Per semplicità valuteremo il valore dell’acutezza alla transizione tra fondo e acqua (aria->cilindro esterno).\n\nFigura 2.22. Esempio di profilo.\n\nIn conclusione, dato il volume MR fornito il programma da realizzare deve eseguire i tre tipi di filtraggio e fornire i valori di SNR e acutezza calcolati nella fetta centrale del fantoccio per il volume originale ed il volume filtrato con i filtri a media mobile, gaussiano e di Wiener. Il filtro Gaussiano va ottimizzato per migliorarne le prestazioni. Per visualizzare l’effetto dei vari algoritmi, è utile visualizzare la differenza tra l’immagine filtrata e quella originale.\n\nIMMAGINE\n\nSNR CLINDRO CENTRALE\n\nACUTEZZA TRANSIZIONE FONDO - CILINDRO ESTERNO\n\nORIGINALE\n\n\n\n\n\nFILTRATA MEDIA MOBILE\n\n\n\n\n\nFILTRATA FILTRO GAUSSIANO\n\n\n\n\n\nFILTRATA FILTRO DI WIENER\n\n\n\n","type":"content","url":"/z-2-1-esercitazione","position":1},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica"},"type":"lvl1","url":"/c31-segmentazione","position":0},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica"},"content":"Seguendo la classificazione della Computer Vision, la segmentazione dell’immagine biomedica è una operazione di medio livello, intermedia tra le operazioni a basso livello (interpolazione e filtraggio) e quelle ad alto livello (classificazione e registrazione). La segmentazione, o pattern recognition, realizza l’estrazione dall’immagine di regioni di interesse. Se consideriamo l’immagine biomedica in figura, la segmentazione riconosce i sei pattern fondamentali (le quattro ossa, i tessuti molli, lo sfondo) assegnandogli una label (i numeri in questo caso). Il riconoscimento dei pattern (quindi l’associazione label/nome anatomico è invece tipica di operazioni al livello superiore.\n\nFigura 3.1. Esempio di immagine biomedica. Atlante anatomico disponibile alla pagina \n\nhttp://​www​.info​-radiologie​.ch\n\nCome introdotto nel capitolo 1, l’immagine biomedica può essere modellata come una immagine “ideale”, formata da una serie di pattern omogenei, corrotta da vari processi quali l’effetto volume parziale, l’attenuazione, il rumore.I(x,y)=[[I_{0}(x,y)+n_{B}(x,y)]*h(x,y)]g(x,y)+n(x,y)\n\nlo scopo della segmentazione è estrarre dall’immagine reale I(x,y), che è quella a noi nota, l’immagine ideale I_{0}(x,y) sulla base della conoscenza del processo di formazione dell’immagine e identificare i pattern che compongono I_{0}(x,y) associando ogni pattern una etichetta o label.\nDall’osservazione dell’equazione che descrive il modello generale di immagine biomedica è facile capire come il problema sia fortemente indeterminato, in quanto i fattori che corrompono l’immagine reale sono solo parzialmente noti.\nE’ importante notare come i pattern in cui vogliamo raggruppare i pixel/voxel dell’immagine non siano “oggettivi”, ma dipendano dal quesito clinico a cui vogliamo rispondere ed in base al quale viene acquisita l’immagine. Infine, non è detto che sia di interesse estrarre tutti i pattern costituenti l’immagine, anzi in generale il processo di segmentazione sarà mirato ad estrarre un numero limitato di strutture di interesse. Possiamo quindi definire il processo di segmentazione di una immagine biomedica come:\n\nDefiniamo segmentazione di una immagine biomedica un processo che consente di associare ad un certo numero di strutture anatomiche che ci interessa individuare una lista univoca di pixel/voxel afferenti a dette strutture.\n\nPer segmentazione si può intendere quindi una operazione che associ ad alcuni pixel/voxel dell’immagine una etichetta (label) che individui a quale oggetto appartiene quel determinato pixel/voxel. I pixel/voxel rimanenti possono essere assegnati ad una label “indeterminata” che raggruppa pixel/voxel afferenti a strutture anatomiche diverse ma che non ci interessa riconoscere.\nIn questo approccio il risultato della segmentazione sono delle maschere (mask), cioè delle immagini binarie di dimensioni uguali a quella su cui viene applicata la segmentazione, che assumono valore non nullo se un pixel appartiene ad un certo oggetto e zero altrove. Per quanto detto prima un processo di segmentazione produrrà K+1 maschere, dove K è il numero di oggetti segmentati. Un metodo alternativo è creare una immagine dove ogni pixel assume un valore V(k) con k=1…K+1. Nella segmentazione di immagini 3D le maschere diverranno anch’esse degli array 3D.\n\nAlternativamente, i K oggetti segmentati possono essere rappresentati da K contorni, dove ogni contorno è una lista ordinata e chiusa di punti definiti sullo spazio dell’immagine. Il numero di punti del contorno definirà la precisione con cui il contorno è definito. In 3D, il contorno diviene una nuvola di punti in uno spazio 3D che definisce una superficie chiusa. Essendo l’ordinamento di una nuvola di punti in 3D non banalmente definibile, la superficie viene tipicamente definita attraverso un particolare ordinamento dato da un processo di triangolarizzazione che produce una mesh, come quella utilizzata negli algoritmi di visualizzazione 3D.\n\nLe rappresentazioni a maschera e a contorni sono sostanzialmente equivalenti (Figura 3.2), nel senso che è possibile passare dall’una all’altra in modo semplice. Avendo una maschera, per ottenere il contorno corrispondente si può ad esempio calcolare la distance transform che sarà introdotta nel seguito e considerare solo i livelli di griglio pari ad uno. Oppure si può calcolare il gradiente che sarà diverso da zero solo sui bordi della maschera ed adottare l’algoritmo di Canny. Per passare da un contorno ad una maschera si può utilizzare il “fence algorithm” o “Even-odd rule”. In uno spazio discreto il contorno è definito come un poligono a N lati che approssima una curva. Per ogni pixel dell’immagine si definisce una semiretta che parte dal punto ed esce fuori dall’immagine stessa, la direzione della semiretta è irrilevante.\n\nFigura 3.2. Equivalenza tra maschera e contorni\n\nSe la semiretta interseca il contorno un numero dispari di volte il punto è all’interno del contorno, se la semiretta interseca il contorno un numero pari di volte (0 si considera pari) il punto è all’esterno.\nL’implementazione è abbastanza semplice, se si considera una semiretta lungo l’asse x orientata verso lo 0\n\nUn esempio di tale implementazione in MATLAB è la seguente:\n\nIl comando ((poly(i,1) > y) ~= (poly(j,1) > y)) controlla che la coordinata y sia tra la minima e massima coordinata y del segmento.  x < ((poly(j,2) - poly(i,2))*     (y-poly(i,1))/(poly(j,1) - poly(i,1)) + poly(i,2) )) controlla che coordinata x sia a destra del segmento. Usando la funzione MATLAB pointInsidePoly, il fence algoorithm può essere implementato come segue:\n\nIn MATLAB la funzione poly2mask implementa un algoritmo per la conversione di un poligono\nin una maschera binaria. In Python, una funzionalità analoga è disponibile nella libreria\nscikit-image, tramite la funzione skimage.draw.polygon2mask.\n\nIl numero di algoritmi di segmentazione sviluppati in letteratura ed utilizzati nella pratica clinica è virtualmente infinito. Nel seguito verrà introdotta una classificazione generale e verranno forniti alcuni esempi.","type":"content","url":"/c31-segmentazione","position":1},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Machine learning"},"type":"lvl2","url":"/c31-segmentazione#machine-learning","position":2},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Machine learning"},"content":"Dal punto di vista del “machine learning”, un algoritmo di segmentazione è un classificatore che assegna una classe a ciascun pixel dell’immagine. Gli algoritmi di machine learning si possono suddividere in unsupervised e supervised (Figura 3.3).\n\nFigura 3.3. Classificazione degli algoritmi di machine learning.\n\nSi parlerà di unsupervised learning se nel processo non viene utilizzato un set di dati validato, per cui l’algoritmo è basato su di un modello che deve includere una descrizione “accurata” dei dati da classificare. La base di conoscenza è quindi racchiusa nel modello, per cui l’approccio unsupervised è anche detto model-driven.\nInvece, nel supervised learning la base di conoscenza è data da un insieme di dati labellati, ad esempio coppie di immagini con la relativa segmentazione. Il modello qui è molto più “generale” rispetto la caso unsupervised ed “impara” la sua configurazione ottimale dall’esempio fornito attraverso i dati labellati. L’approccio supervised è quindi anche detto data-driven.","type":"content","url":"/c31-segmentazione#machine-learning","position":3},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Procedure di ottimizzazione","lvl2":"Machine learning"},"type":"lvl3","url":"/c31-segmentazione#procedure-di-ottimizzazione","position":4},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Procedure di ottimizzazione","lvl2":"Machine learning"},"content":"Prima di introdurre in modo formale il concetto di segmentazione di immagini, è opportuno definire il concetto di ottimizzazione, cioè della ricerca della combinazione di parametri che massimizza (o minimizza) una certa funzione. Utilizziamo come esempio il cosiddetto problema TSP o Problema del Commesso Viaggiatore (Travelling Salesman Problem, da cui la sigla TSP). La definizione del problema è abbastanza semplice: un commesso viaggiatore deve visitare un certo numero di clienti prima di tornare a casa. Conosce la posizione dei clienti e il tempo necessario a spostarsi dall’uno all’altro. Vuole ovviamente visitare tutti i clienti una sola volta nel tempo più breve possibile. In termini più formali, il problema consiste nel costruire un grafo i cui nodi rappresentano i clienti, mentre gli archi rappresentano i percorsi fra i nodi, e di trovare su di esso un ciclo che tocchi tutti i nodi una ed una sola volta e abbia la durata complessiva minima. Il problema, semplice da descrivere, è però complesso da risolvere. Il numero delle sue soluzioni, infatti, cresce molto rapidamente con il numero dei nodi. Consideriamo l’esempio in Figura 3.4.\n\nFigura 3.4. Esempio di problema TSP.\n\nNel grafo per il problema TSP il nodo 1 rappresenta il punto di partenza, i nodi 2:5 i clienti da visitare. I numeri sugli archi che connettono i nodi la lunghezza del percorso. Il tutto può essere illustrato anche in forma tabellare:\n\n\n\n1\n\n2\n\n3\n\n4\n\n5\n\n1\n\n0\n\n8\n\n15\n\n13\n\n10\n\n2\n\n8\n\n0\n\n3\n\n11\n\n6\n\n3\n\n15\n\n3\n\n0\n\n6\n\n7\n\n4\n\n13\n\n11\n\n6\n\n0\n\n9\n\n5\n\n10\n\n15\n\n7\n\n9\n\n0\n\nNotiamo che la tabella in questo caso è simmetrica, cioè la distanza tra due nodi non dipende dall’ordine. Questo può non essere vero in alcune applicazioni. Nel caso illustrato in figura le soluzioni possibili sono 4! = 24, infatti il primo nodo (1) e l’ultimo nodo (1) sono fissi ed abbiamo quindi tutte le possibili combinazioni di 4 elementi. Nel dettaglio i percorsi possibili sono: 1-2-3-4-5-1 (di durata 36), 1-2-3-5-4-1 (di durata 38), 1-2-4-3-5-1 (di durata 42), 1-2-4-5-3-1 (di durata 40), 1-2-5-3-4-1 (di durata 40), 1-2-5-4-3-1 (di durata 44), 1-3-2-4-5-1 (di durata 48), 1-3-2-5-4-1 (di durata 46), 1-3-4-2-5-1 (di durata 48), 1-3-4-5-2-1 (di durata 44), 1-3-5-2-4-1 (di durata 44), 1-3-5-4-2-1 (di durata 40), 1-4-3-2-5-1 (di durata 38), 1-4-3-5-2-1 (di durata 40), 1-4-2-3-5-1 (di durata 44), 1-4-2-5-3-1 (di durata 44), 1-4-5-3-2-1 (di durata 38), 1-4-5-2-3-1 (di durata 46), 1-5-3-4-2-1 (di durata 42), 1-5-3-2-4-1 (di durata 44), 1-5-4-3-2-1 (di durata 36), 1-5-4-2-3-1 (di durata 48), 1-5-2-3-4-1 (di durata 38), 1-5-2-4-3-1 (di durata 48).\nLe soluzioni migliori quindi sono 1-2-3-4-5-1 e 1-5-4-3-2-1, entrambe di durata 36, com’era ovvio, dato che si è considerato un grafo simmetrico.\nIn questo caso abbiamo risolto il problema calcolando tutte le possibili soluzioni e scegliendo la soluzione ottima. Questo approccio è detto a ricerca esaustiva, o brute force. Se il dominio di input è finito, tali algoritmi trovano sempre la soluzione corretta.\nIn generale, per n nodi il numero di soluzioni possibili sarà (n-1)!, che cresce molto rapidamente con n. Per comprendere la difficoltà di gestione di un problema di tale complessità, consideriamo un calcolatore capace di compiere 4*10^9 operazioni al secondo (circa 4 Gflops, dell’ordine della potenza di calcolo di un PC a 4 GHz di uso comune). Ammettiamo di riuscire a computare la lunghezza di un percorso con n operazioni (in realtà ce ne vorranno certamente di più, bisognerebbe contare anche gli accessi in memoria in lettura o scrittura). Allora il computo di tutte le soluzioni richiederà un numero di operazioni pari a:NO = n*(n-1)! = n!\n\nPer n=20 abbiamo N! = 2.4*10^{18} operazioni.\nIl tempo necessario con il calcolatore ipotizzato prima sarà (consideriamo 3 107 sec in un anno):T = 2.4*10^{18}/ 4*10^9 = 6 108 s \\sim 20 Anni\n\nNon avendo tanta pazienza usiamo il calcolatore più veloce esistente nel 2016 (Sunway TaihuLight, China, 93000 Tera Flops) composto da 10 milioni di processori (\n\nwww.top500.org). Per curiosità la potenza necessaria al funzionamento del calcolatore è 15 MW pari a quella erogata da un piccola centrale elettrica. Con questo calcolatore sarebbero necessari:T = 2.4*10^{18}/ 93*10^{15} = 26 sec\n\nPurtroppo già per n=25 la soluzione cinese non funzionaT = 1.55*10^{25}/ 93*10^{15} = 1.6*10^8 sec \\sim 5 Anni\n\nIl grafico in Figura 3.5 mostra in scala logaritmica il tempo di elaborazione stimato in anni per un computer a 93000 Teraflops. Come si osserva, problemi di tipo TSP con complessità superiore a 25 sono di fatto incomputabili.\n\nFigura 3.5. Tempo di elaborazione stimato al crescere di n.\n\nL’esempio dimostra come al crescere della dimensione del problema non sia possibile risolvere lo stesso in modo esaustivo. Il problema TSP e un esempio della famiglia  di problemi NP-completi, cioè problemi di complessità che cresce in modo non lineare con la dimensione del problema. Tali problemi non possono essere risolti in modo esaustivo quando le dimensioni dei dati di input crescono sopra una certa dimensione. Anche se esistono approcci che consentono in alcuni casi di risolvere in modo esatto un particolare problema NP-completo, in generale quello che si può fare è trovare degli algoritmi che ottengano delle soluzioni approssimate con una complessità accettabile. Tali algoritmi non potranno comunque fornire mai una soluzione sicuramente ottima, proprio perché le soluzioni possibili non sono note e quindi è impossibile determinare se una soluzione sia la migliore o meno.\nL’esempio più semplice di soluzione non esaustiva è la ricerca casuale o random. Invece di fare una ricerca esaustiva in modo sistematico utilizzando sempre lo stesso ordine di ricerca, possiamo scegliere un percorso di ricerca variabile in modo random volta per volta. Se viene esplorato tutto il dominino di input, tali algoritmi sono un caso particolare di un algoritmo esaustivo. Se viene esplorata solo una parte del dominio di input, gli algoritmi random hanno una certa probabilità di trovare la soluzione ottima pari al rapporto tra numero di percorsi esplorati e numero totale di percorsi. L’algoritmo di ricerca casuale non ha utilità pratica, ma serve come confronto per gli altri algoritmi di ottimizzazione, nel senso che qualsiasi algoritmo di ottimizzazione ragionevole deve funzionare meglio della ricerca casuale.\nUn esempio di soluzione approssimata del problema TSP si può ottenere con un approccio di tipo Greedy. Un esempio di programmazione Greedy sono gli algoritmi del tipo best-first-search, dove ci si muove in un grafo che descrive un problema scegliendo via via i nodi che sembrano migliori per risolvere il problema. Riconsideriamo il problema TSP visto in precedenza:\nPartendo dal nodo 1, il nodo alla minore distanza è il nodo 2 (distanza 8). Ci muoviamo quindi nel nodo 2. Dal nodo 2 il nodo più vicino è il nodo 3 (distanza 3). Dal nodo 3 andremo nel 4 (distanza 6) e dal nodo 4 dovremo andare nel 5 (distanza 9) e poi nell’1 (distanza 10). Il percorso (1-2-3-4-5-1) ha lunghezza totale 8+3+6+9+10=36, che come si era visto è la distanza minima. Con un algoritmo semplice abbiamo quindi trovato la soluzione ottima. In particolare il numero di passi dell’algoritmo Greedy è (N-1)+(N-2)+(N-3)+…..+1, infatti al primo passo devo fare N-1 confronti, al secondo N-2, etc. L’ordine di grandezza del numero di passi è N^2 molto minore del numero di passi dell’algoritmo esaustivo. In generale non è detto che in questo modo si trovi la soluzione migliore, consideriamo ad esempio il grafo di Figura 3.6.\n\nFigura 3.6. Altro esempio di problema TSP.\n\nL’algoritmo best-first-search ritrova il percorso ottimo visto prima 1-2-3-4-5-1=36, ma esiste un percorso migliore 1-5-2-3-4-1=35 che non viene “visto”. Questo conferma quanto si era detto a proposito del problema TSP, cioè che esistono algoritmi di limitata complessità computazionale che danno una soluzione approssimata, ma non necessariamente la migliore. La soluzione prodotta dall’algoritmo best-first-search dipende dal nodo iniziale che si sceglie per la partenza dell’algoritmo. La figura mostra i risultati di un algoritmo best-first-search applicato al problema TSP Berlin52 variando il nodo di partenza dell’algoritmo.\n\nFigura 3.7. Problema TSP Berlin52.\n\nSi osserva come la soluzione dipenda dal nodo, la soluzione migliore è 8182. Per confronto la migliore soluzione nota del problema Berlin52 è 7542. Per concludere, dato un certo problema NP-completo da risolvere per via algoritmica, esisterà una soluzione esaustiva che risolve il problema in maniere ottima, ma sarà applicabile solo per dimensioni del problema molto piccole e non interessanti da un punto di vista pratico. Per risolvere il problema sarà necessario utilizzare un algoritmo di ottimizzazione più veloce dell’algoritmo esaustivo. Tale algoritmo non potrà in generale assicurare la soluzione ottima, ma solo una soluzione “ragionevolmente” buona. In generale la soluzione trovata da un algoritmo non esaustivo dipenderà dalle condizioni iniziali, e quindi varierà in funzione delle condizioni iniziali stesse.\n\nIn generale un processo di ottimizzazione sarà definito da tre caratteristiche:\n\nSpazio di ricerca (Search-space). Lo spazio di ricerca è l’insieme dei valori che possono assumere le possibili soluzioni. Ad esempio, nel problema TSP tutte le possibili liste dei nodi senza ripetizioni.\n\nMetrica. La quantità da massimizzare o minimizzare, nel caso del TSP la lunghezza di un percorso.\n\nIl processo di ottimizzazione. L’algoritmo utilizzato per trovare all’interno del search-space la soluzione che massimizza o minimizza la metrica. Nel caso del TSP il metodo greedy.\n\nGli algoritmi di ottimizzazione possono essere classificati come:\n\nOttimizzatori locali. Un ottimizzatore locale trova una soluzione partendo da un punto del search-space (condizioni iniziali). La soluzione trovata dipenderà quindi dalle condizioni iniziali. Nel problema TSP, l’algoritmo greedy applicato ad un singolo nodo iniziale è un ottimizzatore locale e la soluzione dipende dal nodo selezionato.\n\nOttimizzatori globali. Un ottimizzatore globale trova la soluzione migliore all’interno del search-space indipendentemente dai parametri di ingresso. Come detto in precedenza, l’unico vero ottimizzatore globale è la ricerca esaustiva. Nel caso del TSP un algoritmo greedy iterato su tutti i nodi si può considerare una approssimazione di un ottimizzatore globale, in quanto il risultato non dipende dalle condizioni iniziali.","type":"content","url":"/c31-segmentazione#procedure-di-ottimizzazione","position":5},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Segmentazione a Soglia"},"type":"lvl2","url":"/c31-segmentazione#segmentazione-a-soglia","position":6},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Segmentazione a Soglia"},"content":"Nella segmentazione a soglia ogni pixel viene associato ad un tipo di tessuto mediante l’intensità di segnale. La segmentazione a soglia corrisponde quindi a scegliere una o più soglie nell’istogramma dell’immagine e a classificare i pixel mediante tale informazione. Consideriamo il fantoccio MR in Figura 3.8.\n\nFigura 3.8. Fantoccio MR e relativo istogramma.\n\nIl fantoccio è costituito da 6 pattern (in realtà c’è anche una parte esterna indistinguibile dallo sfondo data dallo zero padding) come in tabella:\n\n\n\nPattern\n\nTessuto\n\nSoglie\n\n1\n\nSfondo\n\nAria (sfondo)\n\n<90\n\n2\n\nCilindro acqua esterno\n\nAcqua\n\n>90 \\quad <500\n\n3\n\nParete cilindro ext\n\nVetro (sfondo)\n\n<90\n\n4\n\nCilindro olio\n\nOlio\n\n>500\n\n5\n\nParete cilindro int\n\nVetro (sfondo)\n\n<90\n\n6\n\nCilindro acqua interno\n\nAcqua\n\n>90 \\quad <500\n\nI sei pattern sono associati a quattro diversi “tessuti” (aria, vetro, olio, acqua). L’aria e il vetro in MR sono due oggetti del tutto equivalenti, in quanto non forniscono alcun segnale MR. Sono quindi impossibili da distinguere con una segmentazione a soglia e li possiamo raggruppare in una classe “sfondo” (background), ottenendo quindi tre classi. Analogamente le due regioni contenenti acqua hanno lo stesso segnale. Consideriamo l’istogramma dell’immagine. Appaiono i tre picchi che descrivono la distribuzione del segnale, con ampiezza proporzionale al numero di pixel. Per dividere le classi occorrono due soglie (il numero di soglie è N-1 dove N è il numero di classi). Fissiamo le soglie a T1=90 (sfondo-acqua) e T2=500 (acqua-olio). Estraiamo dall’immagine i pixel con valori s tali che s < T1, T1<=s<T2, s>=T2 e poniamo ad un valore diverso da 0 tali pixel e a zero gli altri. Otteniamo quindi tre maschere (mask) che descrivono la distribuzione delle tre classi sull’immagine (Figura 3.9).\n\nFigura 3.9. Maschere ottenute per diverse soglie.\n\nLe tre maschere rappresentano (colore bianco) la distribuzione dei pixel appartenenti alle classi sfondo, acqua e olio rispettivamente.\nNotiamo che alcuni pixel sono stati erroneamente assegnati alla maschera “sfondo” anche se sono acqua. Questo è dovuto alla imperfetta separazione dei picchi sfondo-acqua nell’istogramma.Dall’esempio individuiamo i limiti fondamentali della segmentazione a soglia:\n\nLa segmentazione a soglia non è in grado di distinguere oggetti topologicamente diversi ma ai quali è associato lo stesso livello di segnale. Nel nostro caso i pattern 1-3-5 e 2-6 vengono rappresentati in una sola maschera. Questo rappresenta un problema in molte applicazioni cliniche (es: ventricolo destro e sinistro nel cuore, grasso viscerale e grasso subcutaneo, etc).\n\nPossono apparire pixel spuri dovuti alle variazioni di segnale indotte dal rumore\n\nLe soglie vengono definite manualmente tramite esame visivo dell’istogramma.\n\nIl punto 1 è un limite intrinseco della segmentazione a soglia. Come si vedrà nel seguito può essere superato facendo seguire alla segmentazione a soglia l’applicazione di algoritmi di tipo topologico applicati alle maschere prodotte dalla segmentazione a soglia, come il labeling.\nIl punto 2 implica che la qualità della segmentazione ottenibile con un algoritmo a soglia dipende sostanzialmente dal CNR tra i due tessuti da separare. Anche in questo caso gli errori di segmentazione possono essere corretti, almeno in parte, applicando algoritmi di filtraggio al risultato della segmentazione.\nPer quanto riguarda il punto 3. esistono vari approcci automatici che consentono di trovare la soglia “ottima” che divide i pixel in classi attraverso soglie opportune. Tali algoritmi adottano un approccio “unsupervised learning”.","type":"content","url":"/c31-segmentazione#segmentazione-a-soglia","position":7},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Algoritmo di Otsu (adattato da Wikipedia Italia)","lvl2":"Segmentazione a Soglia"},"type":"lvl3","url":"/c31-segmentazione#algoritmo-di-otsu-adattato-da-wikipedia-italia","position":8},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Algoritmo di Otsu (adattato da Wikipedia Italia)","lvl2":"Segmentazione a Soglia"},"content":"L’algoritmo di Otsu nella sua versione originale presume che nell’immagine da segmentare siano presenti due sole classi e quindi calcola la soglia ottima per separare queste due classi minimizzando la varianza intra classe. L’algoritmo può essere esteso a più classi (multi Otsu method). L’algoritmo di Otsu standard minimizza la quantità (varianza intra classe):\\sigma_w^2(t) = \\omega_1(t)\\,\\sigma_1^2(t) + \\omega_2(t)\\,\\sigma_2^2(t)\n\ndove \\omega è la probabilità di una classe separata dall’altra dalla soglia t e \\sigma è la SD della classe. Quindi l’algoritmo cerca di trovare la soglia che separa l’immagine in due sottoclassi che siano il più possibile omogenee al loro interno. Intuitivamente, immaginando un istogramma a due picchi il metodo di Otsu consiste nel trovare la soglia t intermedia tra i due picchi che li divida in modo ottimale.\nSi dimostra che minimizzare la varianza intra-classe è equivalente a massimizzare la varianza inter-classe:\\sigma_b^2(t)\n= \\sigma^2 - \\sigma_w^2(t)\n= \\omega_1(t)\\,\\omega_2(t)\\,\\bigl[\\mu_1(t) - \\mu_2(t)\\bigr]^2\n\ndove \\mu è il valor medio di una classe. Il metodo consiste nel provare in modo esaustivo tutti i possibili t (che sono in numero uguale alla profondità dell’immagine) e prendere il t che minimizza la varianza inter classe. Da un punto di vista algoritmico:\n\nSi calcola l’istogramma h dell’immagine. La probabilità \\omega si ottiene normalizzando l’istogramma per il numero di pixel dell’immagine (da un punto di vista pratico la normalizzazione non è necessaria).\n\nSi calcola il valore di \\sigma_{b}^2(t) per ogni t calcolando\n\\omega_1(t) = \\sum_{i=0}^{t} p(i)\n\\qquad\n\\mu_1(t) = \\sum_{i=0}^{t} p(i)\\,x(i)\n\n\ne  \\omega_2(t) e \\mu_2(t) in modo analogo operando le somme da t+1 in avanti.\n\nSi sceglie il t che massimizza \\sigma_{b}^2(t)\n\nIn questo modo la soglia t viene definita in modo automatico. In MATLAB il metodo di Otsu è implementato tramite la funzione otsuthresh quando si\nopera sull’istogramma dell’immagine, oppure mediante la funzione graythresh\nquando il metodo viene applicato direttamente all’immagine. In Python, l’equivalente del metodo di Otsu è disponibile nella libreria\nscikit-image tramite la funzione skimage.filters.threshold_otsu,\nche può essere applicata direttamente all’immagine oppure ai valori di intensità\nper determinare automaticamente la soglia ottimale.\n\nIl metodo di Otsu opera secondo un processo di ottimizzazione basato sulla ricerca esaustiva su tutti i possibili valori del parametro t da ottimizzare, ed è quindi non particolarmente efficiente da un punto di vista computazionale. Tuttavia, essendo il calcolo della quantità \\sigma_{b}^2(t) da ottimizzare molto rapido il calcolo di t può essere effettuato in tempi ragionevoli. Notiamo che il tempo di calcolo è fortemente dipendente dal valore di profondità dell’immagine che determina il numero di prove da effettuare e la lunghezza delle somme da calcolare. Si noti infine che possono esistere più valori di t per cui \\sigma_{b}^2(t) è massima, si pensi al caso di un istogramma con due picchi ben separati. In questo caso l’algoritmo ritornerà il valor medio tra i valori di t che massimizzano \\sigma_{b}^2(t).","type":"content","url":"/c31-segmentazione#algoritmo-di-otsu-adattato-da-wikipedia-italia","position":9},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Clustering"},"type":"lvl2","url":"/c31-segmentazione#clustering","position":10},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Clustering"},"content":"Un altro metodo per la scelta dei valori di soglia corretti che minimizzino l’errore nella segmentazione è l’uso di algoritmi di clustering.  Il Clustering è una tecnica di analisi dei dati volta alla selezione e raggruppamento di elementi omogenei in un insieme di dati.\nL’approccio è del tipo unsupervised, nel senso che l’algoritmo di clustering riesce a dividere i dati in una serie di insiemi avendo come unico input il numero di insiemi da trovare. Gli algoritmi di Clustering si possono applicare a dati di diversa natura, e trovano applicazione in molteplici discipline come la bioinformatica, il marketing, la genetica, etc.\n\n\n\nOBS_1\n\nOBS_2\n\nOBS_3\n\n...\n\nOBS_K\n\nE_1\n\n\n\n\n\n\n\n\n\n\n\nE_2\n\n\n\n\n\n\n\n\n\n\n\nE_3\n\n\n\n\n\n\n\n\n\n\n\n.\n\n\n\n\n\n\n\n\n\n\n\n.\n\n\n\n\n\n\n\n\n\n\n\n.\n\n\n\n\n\n\n\n\n\n\n\nE_N\n\n\n\n\n\n\n\n\n\n\n\nIn generale i dati di ingresso di un algoritmo di clustering sono composti da una tabella con N righe, che corrispondono agli elementi da analizzare, e da K colonne che corrispondono alle osservazioni disponibili sugli elementi stessi. Ad esempio le righe della tabella potrebbero rappresentare dei pazienti e le colonne dei dati clinici sui pazienti (analisi del sangue, valori di pressione, etc). Lo scopo del clustering è raggruppare gli elementi simili tra loro in gruppi (cluster) sulla base delle osservazioni.\nIn Figura 3.10 è esemplificato un problema classico di clustering: abbiamo un insieme di dati (caratterizzati da una coppia di valori) intuitivamente raggruppabili in quattro classi e vogliamo ottenere la soluzione a destra in cui i dati sono opportunamente raggruppati. In questo caso l’osservazione intuitiva che i dati si raggruppano in quattro cluster corrisponde all’utilizzo come distanza tra i dati della distanza geometrica sul piano. Punti tra loro “vicini” secondo la distanza scelta sono assegnati allo stesso insieme.\n\nFigura 3.10. Esempio di tipico problema di clustering.\n\nNel campo dell’analisi delle immagini biomediche, le righe della tabella rappresentano le locazioni spaziali su cui vengono acquisite le immagini (quindi i pixel/voxel dell’immagine), mentre le colonne rappresentano i valori di segnale acquisiti in corrispondenza delle locazioni spaziali. In definitiva, l’input dell’algoritmo di clustering sarà una matrice N \\times M, dove N è il numero di locazioni spaziali considerate (numero di pixel o voxel) e M il numero di acquisizioni disponibili.\nQuindi per una immagine 2D che contiene N_x \\times N_y pixel, la matrice di clustering corrispondente sarà un matrice a N_x \\times N_y righe ed una singola colonna. Per una immagine 3D la matrice di clustering corrispondente sarà una matrice a N_x \\times N_y \\times N_z righe ed una singola colonna (Figura 3.11). Una immagine a colori RGB corrisponderà ad una matrice a tre colonne (K=3), in quanto per ogni pixel sono disponibili tre misure corrispondenti alla relativa tripletta RGB.\n\nFigura 3.11. Rappresentazione dei dati di input a un algoritmo di clustering.\n\nUna immagine 2D +T corrisponderà ad una matrice di clustering con N = N_x \\times N_y e K = nT, in quanto per ogni pixel sono disponibili nT osservazioni ad intervallo di tempo diversi. Analogamente una immagine 3D + nT corrisponderà a una matrice di clustering con N = N_x \\times N_y \\times N_z e K = nT (Figura 3.12). E’ importante notare che la corrispondenza ha senso se gli nT frame temporali sono allineati, cioè se i pixel nella serie temporale corrispondono alle stesse locazioni spaziali. Ad esempio una serie temporale in cui il paziente è fermo e viene iniettato un contrasto che cambia i livelli di grigio può essere interpretata in questo modo, mentre una serie temporale che segue il battito cardiaco no. Altri esempi in MRI sono immagini T1, T2, PD pesate dello stesso distretto anatomico. Oppure potremmo avere immagini multiecho acquisite a tempi di eco diversi.\n\nFigura 3.12. Rappresentazione dei dati di input a un algoritmo di clustering per il caso di serie temporali.\n\nGli algoritmi di clustering usati nella segmentazione di immagini si possono classificare in tre classi:\n\nClustering esclusivo (k-means)\n\nClustering non esclusivo (fuzzy c-means)\n\nClustering probabilistico\n\nNel clustering esclusivo, un dato deve appartenere ad uno ed un solo cluster L’implementazione più nota di questo approccio è l’algoritmo k-means.","type":"content","url":"/c31-segmentazione#clustering","position":11},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Algoritmo k-means","lvl2":"Clustering"},"type":"lvl3","url":"/c31-segmentazione#algoritmo-k-means","position":12},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Algoritmo k-means","lvl2":"Clustering"},"content":"L’algoritmo k-means (MacQueen, 1967) è l’algoritmo base ed uno dei primi ad essere stato realizzato per la classificazione di dati. Dato un insieme Y = (y_1,...,y_n) di dati da classificare in C gruppi (cluster) e una distanza ||.|| definita su Y, possiamo definire una funzione obiettivo da minimizzare. Questa funzione è:J = \\sum_{j \\in \\Omega} \\sum_{k=1}^{C}\n\\left\\lVert \\mathbf{y}_{jk} - \\mathbf{v}_k \\right\\rVert^2\n\ndove \\left\\lVert \\mathbf{y}_{jk} - \\mathbf{v}_k \\right\\rVert è la distanza tra un dato y_j e un centroide v_k. Per ogni dato vengono contate solo le distanze rispetto al centroide più vicino. Questa funzione obiettivo è un indicatore della distanza dei dati y dai centri dei rispettivi cluster. La distanza utilizzata può essere una qualsiasi metrica. Il problema di trovare l’assegnazione dei dati ai cluster che realizzi il minimo di J è NP-completo con complessità computazionale O(ndC+1 log(n)) dove d è la dimensionalità dei dati. Il problema per n non piccolissimo non è quindi risolubile in modo esaustivo e si utilizzano algoritmi in grado di trovare un minimo locale di J.\nLa procedura più nota è abbastanza semplice e richiede la definizione di un numero di cluster C in cui dividere i dati. Si definiscono C centroidi, uno per ogni cluster da trovare, in modo casuale ma in modo che siano abbastanza lontani tra loro, in modo da migliorare la convergenza dell’algoritmo. A questo punto per ogni dato calcoliamo la distanza dai centroidi ed associamo il dato al centroide più vicino. Calcoliamo ora C nuovi centroidi, come centro di massa dei cluster ottenuti al passo precedente. Possiamo ora ricomputare la distanza di tutti i dati dai nuovi centroidi e creare così dei nuovi cluster. Iterando il procedimento, arriveremo ad un punto in cui i centroidi si stabilizzano senza cambiare più di posizione. Saremo quindi in una situazione di convergenza dell’algoritmo che ci dà il clustering desiderato.\n\nLa procedura per trovare il valore minimo di J è la seguente:\n\nAssegnare un valore iniziale ai C centroidi v_k\n\nIterare i due passi seguenti fino a quando i valori di v_k si modificano:\n\nAssegnare ogni dato y ad uno ed un solo cluster sulla base della distanza di y dal centroide del cluster.\n\nAggiornare i valori dei centroidi v_k come media dei dati y appartenenti al centroide k.\n\nLa procedura descritta precedentemente minimizza la funzione obiettivo, ma non è ovviamente un procedimento esaustivo. È possibile quindi provare che l’algoritmo k-means converge sempre, ma non è detto che la configurazione di convergenza sia quella che trova un minimo assoluto della funzione obiettivo. L’algoritmo k-means è quindi un ottimizzatore locale. Poiché lo stato di convergenza dell’algoritmo dipende dalla definizione iniziale dei centroidi, è possibile ripetere il procedimento più volte assegnando in modo casuale il valore iniziale dei centroidi e tra le varie configurazioni stabili trovare quella dove la funzione obiettivo è minima.\nSi noti che dal punto di vista dell’algoritmo di clustering il numero di dimensioni che caratterizza i dati y e quindi i centroidi v è irrilevante. In generale y sarà un vettore a n dimensioni e v avrà n dimensioni come y. La distanza verrà calcolata nello spazio n-dimensionale dei dati y.In MATLAB l’algoritmo k-means è implementato nella funzione kmeans\n(disponibile nello Statistics Toolbox). In Python, l’algoritmo k-means è implementato nella libreria scikit-learn tramite la classe sklearn.cluster.KMeans, che fornisce un’implementazione efficiente e ampiamente utilizzata del metodo.\n\nL’algoritmo presenta alcune difficoltà:\n\nÈ necessario un algoritmo per inizializzare i centroidi. Un possibile metodo è far coincidere i C centroidi iniziali con C campioni scelti a caso dai dati. Oppure è possibile definire i valori iniziali dei centroidi sulla base della conoscenza a priori dei dati (opzione start di kmeans in MATLAB, parametro init di KMeans in Python).\n\nIl risultato finale dipende dai centroidi iniziali. Questo può essere sfruttato facendo girare più volte l’algoritmo in dipendenza da condizioni iniziali diverse e prendendo come risultato ottimo quello che minimizza J (opzione nreplicates di kmeans in MATLAB, parametro n_init di KMeans in Python).\n\nPuò accadere che un cluster si svuoti, per cui non può essere aggiornato. In questo caso l’algoritmo si blocca (opzione EmptyAction di kmeans in MATLAB; in Python la gestione dei cluster vuoti è interna all’implementazione di KMeans).\n\nIl risultato dipende dalla metrica (opzione Distance di kmeans in MATLAB). In Python, l’implementazione standard di KMeans utilizza la distanza euclidea. Per utilizzare altri tipi di distanza è possibile definire la matridice delle distanze tramite la funzione scipy.spatial.distance.pdist e poi applicarvi l’agoritmo k-means.\n\nIl risultato dipende dal numero C di cluster che è predefinito.\n\nRiguardo l’ultimo punto, in generale non esiste un metodo per la stima del numero ottimo di cluster. Si utilizza un modello (ad esempio nella segmentazione di immagini possiamo conoscere a priori quanti tessuti vogliamo segmentare), oppure si fanno diversi tentativi con C diversi e si cerca di stimare la soluzione migliore utilizzando il valore di J.\nUna variante del k-means è il metodo “Iterative Self-Organizing Data Analysis Technique” (ISODATA), che è sostanzialmente un algoritmo k-means in cui il numero di cluster non è un parametro di input non modificabile ma uno dei risultati dell’algoritmo che viene computato insieme ai cluster. Nell’approccio ISODATA due cluster possono essere combinati tra loro se la loro separazione è minore di una certa soglia mentre un singolo cluster può essere diviso in due se è troppo “disperso” nello spazio di ricerca.\nQuando applicato alla segmentazione di immagini, la metrica utilizzata è tipicamente la differenza assoluta o la media quadratica delle differenze dei livelli di grigio. Nel caso di immagini singole le due metriche sono chiaramente coincidenti. Nel caso di sequenze temporali, si possono usare metriche di tipo correlativo che descrivono ad esempio la correlazione temporale tra la variazione del livello di segnale dei pixel nel tempo.\nNel caso dell’immagine del fantoccio introdotta in precedenza, l’analisi visiva dell’istogramma ci suggerisce l’uso di C=3 cluster. Il problema è chiaramente monodimensionale (singola immagine). L’algoritmo kmeans con tre cluster ci fornisce tre centroidi (12.4, 174, 921) che rappresentano i valori tipici dei tre tessuti e le maschere dei tre tessuti sull’immagine (Figura 3.13).\n\nFigura 3.13. Segmentazione tramite algoritmo k-means.\n\nQuesta rappresentazione è alternativa rispetto alle tre maschere distinte che avevamo introdotto in precedenza ma ha l’identico significato. Il risultato è molto simile a quello ottenuto con la valutazione manuale delle soglie, con la differenza che qui le soglie stesse vengono trovate automaticamente. Questo tipo di maschere possono essere visualizzate a falsi colori dove ad ogni colore corrisponde un tessuto come nella rappresentazione di destra.\n\nSupponiamo ora di acquisire lo stesso fantoccio con due sequenze MR diverse, come in Figura 3.14. Il livello di grigio associato ai tre tessuti sarà diverso tra le due immagini. Ogni dato sarà quindi caratterizzato da una coppia di valori. Il funzionamento dell’algoritmo di clustering è esattamente lo stesso con la differenza che ora la distanza tra due pixel non è più la differenza assoluta tra due valori ma la distanza (euclidea o di Manhattan) tra i due pixel in uno spazio bi-dimensionale.\n\nFigura 3.14. Esempio di acquisizione con due sequenze MR diverse.\n\nIn questo caso i dati da fornire all’algoritmo k-means saranno un vettore Nx2, dove N è il numero di pixel dell’immagine. L’algoritmo k-means ci restituirà il valore dei centroidi, che sarà rappresentato come un vettore 3x2, perché ogni centroide è definito in uno spazio bidimensionale (Figura 3.15), ed una maschera come in precedenza.\n\nFigura 3.15. Spazio bidimensionale su cui applicare l’algoritmo di clustering.\n\nIl grafico rappresenta la distribuzione degli N pixel in dipendenza dal valore che il segnale dei pixel assume sulle due immagini. Il k-means fornisce come coordinate dei centroidi:\n\ncoord. 1\n\ncoord. 2\n\nClasse\n\n14.4741\n\n16.2826\n\nsfondo\n\n170.3552\n\n391.8894\n\nacqua\n\n923.4150\n\n758.6847\n\nolio\n\nche rappresentano i valori tipici di segnale dei tessuti nelle due immagini, e la maschera di Figura 3.16.\n\nFigura 3.16. Maschera in uscita dall’algoritmo k-means.\n\nIl metodo può essere esteso a qualunque numero di immagini.\n\nÈ importante notare come applicare un clustering multidimensionale abbia senso solo se si hanno più misure su una stessa locazione spaziale. Le immagini 3D (in cui ogni voxel corrisponde ad una diversa locazione spaziale) vengono elaborate con un algoritmo di clustering monodimensionale esattamente come le immagini 2D, quindi come una lista di pixels (o voxel).","type":"content","url":"/c31-segmentazione#algoritmo-k-means","position":13},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Clustering non esclusivo (FCM)","lvl2":"Clustering"},"type":"lvl3","url":"/c31-segmentazione#clustering-non-esclusivo-fcm","position":14},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Clustering non esclusivo (FCM)","lvl2":"Clustering"},"content":"Nel clustering non esclusivo, spesso definito fuzzy clustering, un dato può appartenere a più cluster con diversi livelli di appartenenza. La somma dei livelli di appartenenza su tutti i possibili cluster per un dato dovrà essere 1. L’algoritmo fuzzy c-means (FCM) quindi generalizza l’algoritmo k-means, consentendo una segmentazione più graduale basata sulla teoria del set di regole fuzzy.\nLa fuzzy logic o logica sfumata è un’estensione della logica booleana, basata su un grado di verità di ciascuna proposizione. È fortemente legata alla teoria degli insiemi sfocati e, dopo essere già stata intuita da pensatori precedenti, venne concretizzata da Lotfi Zadeh.\nLa teoria degli insiemi fuzzy costituisce un’estensione della teoria classica degli insiemi poiché per essa non valgono i principi aristotelici di non contraddizione e del terzo escluso (o del Tertium non datur). Il principio di non contraddizione stabilisce che, dati due insiemi A e !A (non-A), ogni elemento appartenente all’insieme A non può contemporaneamente appartenere anche a non-A; l’intersezione di A e !A è l’insieme vuoto. Secondo il principio del terzo escluso, se un qualunque elemento non appartiene all’insieme A, esso necessariamente deve appartenere al suo complemento non-A. L’unione di un insieme A e del suo complemento non-A costituisce il dominio completo di definizione degli elementi di A.\nLa modifica introdotta dalla logica fuzzy è di rifiutare questo assunto. Quando parliamo di grado di verità o valore di appartenenza intenderemo che una proprietà può assumere oltre che i valori vera (valore 1) o falsa (valore 0) come nella logica classica, anche valori intermedi. In logica fuzzy si può ad esempio dire che un bambino appena nato è giovane di valore 1, un diciottenne è giovane 0,8, ed un sessantacinquenne è giovane di valore 0,15 (tale valore dipende dall’età del docente che tiene il corso). Solitamente il valore di appartenenza si indica con u. È importante notare che il concetto di appartenenza fuzzy non ha nulla a che vedere con il concetto di probabilità. Nella probabilità una affermazione o è vera o è falsa con una certa probabilità, mentre nella logica fuzzy è insieme vera e falsa.\nL’algoritmo fuzzy c-means$* ha quindi la particolarità di consentire ad un dato di appartenere a più cluster contemporaneamente. Il metodo è stato ideato da Dunn nel 1973 e migliorato da Bezdek nel 1981. Il metodo è basato sulla minimizzazione della funzione obiettivo:J_{\\mathrm{FCM}} =\n\\sum_{j \\in \\Omega}\n\\sum_{k=1}^{C}\nu_{jk}^{\\,m}\n\\left\\lVert \\mathbf{y}_j - \\mathbf{v}_k \\right\\rVert^2\n\ndove m è un numero strettamente maggiore di 1, u_{jk} è il grado di appartenenza di y_j rispetto al cluster k, y_j è il j-esimo di N dati d-dimensionali appartenenti all’insieme \\Omega, v_k è il centro d-dimensionale  del cluster k, e ||.|| è una distanza. C è il numero di cluster.  m è un parametro detto fuzzyness dell’algoritmo. Di solito si pone m=2. Si noti che se u è una matrice binaria, cioè può assumere solo i valori 0 e 1, la funzione J diviene uguale a quella definita per il k-means, che si può quindi considerare un caso particolare dell’algoritmo FCM. In particolare, come si vede dal grafico in Figura 3.17 al crescere del valore di m il peso dei valori di appartenenza “piccoli” nel computo di J diviene sempre minore, fino ad annullarsi per valori di m molto grandi. L’algoritmo FCM va quindi a coincidere con il k-means per m che tende ad infinito.\n\nFigura 3.17. Valori di appartenenza al variare di m.\n\nLa complessità computazionale del problema è equivalente a quella del k-means, per cui anche il partizionamento fuzzy è ottenuto attraverso una ottimizzazione iterativa della funzione obiettivo, in modo simile a quanto visto per il k-means, attraverso l’aggiornamento della funzione di appartenenza u_{ik} e dei centri dei cluster v_k:u_{jk} =\n\\frac{1}{\n\\displaystyle\n\\sum_{q=1}^{C}\n\\left(\n\\frac{\\lVert \\mathbf{y}_j - \\mathbf{v}_k \\rVert}\n     {\\lVert \\mathbf{y}_j - \\mathbf{v}_q \\rVert}\n\\right)^{\\frac{2}{m-1}}\n}\\mathbf{v}_k =\n\\frac{\n\\displaystyle \\sum_{j=1}^{N} u_{jk}^{\\,m}\\, \\mathbf{y}_j\n}{\n\\displaystyle \\sum_{j=1}^{N} u_{jk}^{\\,m}\n}\n\nIl processo iterativo si ferma quando la differenza tra il valore corrente di u ed il valore precedente è più piccola di una soglia. La procedura descritta converge ad un minimo locale della funzione obiettivo, J_{FCM}. Si noti che a differenza del k-means dove la condizione di convergenza è univoca nel FCM la condizione di convergenza dipende dalla soglia utilizzata.\nL’algoritmo FCM presenta gli stessi problemi associati all’algoritmo k-means, con la possibile eccezione dello svuotamento dei cluster che è meno probabile per la natura continua (e non discreta come nel k-means) del processo di ottimizzazione.\n\nIn MATLAB l’algoritmo di fuzzy clustering è implementato tramite la funzione fcm (disponibile nel Fuzzy Logic Toolbox). In Python, l’algoritmo di Fuzzy C-Means è disponibile in diverse librerie. Una\nimplementazione è fornita dalla libreria scikit-fuzzy tramite la funzione\nskfuzzy.cluster.cmeans. Un’ulteriore implementazione è disponibile nella libreria\npyclustering, tramite la funziona pyclustering.cluster.fcm.\n\nSe applichiamo l’algoritmo allo stesso fantoccio utilizzato in precedenza otteniamo i centroidi:\n\ncoord. 1\n\ncoord. 2\n\n13.53\n\n15.86\n\n170.76\n\n392.26\n\n925.27\n\n773.48\n\nchiaramente simili a quelli precedenti. Otterremo invece tre maschere, una per ogni tessuto, dove i valori delle maschere non sono più binari ma sono distribuiti tra 0 e 1. Un valore 1 (bianco) indica la massima appartenenza del tessuto al cluster.\n\nFigura 3.18. Maschere in uscita dall’algoritmo FCM.\n\nL’approccio FCM è di interesse nella segmentazione delle immagini mediche in quanto consente di tener conto del PVE. Infatti in una immagine biomedica il segnale di alcuni pixel/voxel sarà dato da due o più tessuti che si compenetrano nella stessa regione elementare di spazio nella quale viene misurato il segnale. L’approccio FCM consente di assegnare a tali pixel/voxel un valore di appartenenza distribuito tra i tessuti presenti, interpretando nel modo corretto il PVE.\n\nA titolo di esempio, consideriamo l’immagine in Figura 3.19 (imageNS) formata da sei pattern omogenei, di valore [20, 70, 150, 300, 550, 750], con l’aggiunta di rumore gaussiano e l’applicazione di un filtro a media mobile per simulare il PVE.\n\nFigura 3.19. imageNS.\n\nAbbiamo evidentemente C = 6. Applichiamo all’immagini il FCM attraverso la funzione MATLAB fcm.[center,U,J] = fcm(imageNS(:),6)\n\nIn Python, l’algoritmo Fuzzy C-Means può essere applicato utilizzando la libreria\nscikit-fuzzy, tramite la funzione cmeans, oppure la libreria pyclustering.from pyclustering.cluster.fcm import fcm\n\nfcm_instance = fcm(imageNS.reshape(-1, 1), 6)\nfcm_instance.process()\n\ncenter = fcm_instance.get_centers()\nU = fcm_instance.get_membership()\n\nSi noti che abbiamo trasformato l’immagine in un vettore monodimensionale. La funzione fcm restituisce il valore dei centroidi (center), la funzione di appartenenza (U) e il valore J della funzione obiettivo durante le iterazioni. Il grafico della funzione J (Figura 3.20) mostra come l’algoritmo minimizzi il valore di J e si fermi quando J non varia più in modo significativo.\n\nFigura 3.20. Valori di J al crescere del numero di iterazioni.\n\nI valori dei centroidi risultanti sono [19.95,   69.78,  153.42,  300.54,  548.46,  748.53]  e rappresentano una stima del valore di segnale dei 6 pattern omogenei.\nU è un array 6xN dove N è il numero di pixel dell’immagine. Ognuna delle 6 componenti di U rappresenta la mappa di appartenenza per un certo cluster. La mappa di appartenenza del cluster di valore massimo (750) risulta come in Figura 3.21 (a sinistra), con un valore massimo (1) in corrispondenza del pattern.\n\nFigura 3.21. Mappe di appartenenza.\n\nA destra viene riportata la mappa di appartenenza del cluster con valore 550. Come si vede i pixel di bordo del pattern che assumono un valore intermedio tra 550 e lo sfondo hanno un grado di appartenenza minore di 1 (intorno a 0.5), tranne nell’interfaccia tra il pattern “550” e il pattern “300” dove l’appartenenza assume un valore più alto in quanto il segnale risultante dal PVE è più vicino al valore di segnale del pattern. Notiamo infine che ai pixel di bordo del pattern “750” viene assegnato un grado di appartenenza elevato al cluster “550”. Tali pixel infatti risultano dal PVE tra il pattern “750” e il pattern “70” e quindi possono assumere valori simili al pattern “550”. Questa errata attribuzione è tipica degli algoritmi di segmentazione a soglia quali l’algoritmo FCM, che non avendo connotazioni topologiche non è in grado di distinguere strutture topologicamente connesse o meno.","type":"content","url":"/c31-segmentazione#clustering-non-esclusivo-fcm","position":15},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Algoritmo EM (Expectation Maximization) e Gaussian Mixture","lvl2":"Clustering"},"type":"lvl3","url":"/c31-segmentazione#algoritmo-em-expectation-maximization-e-gaussian-mixture","position":16},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Algoritmo EM (Expectation Maximization) e Gaussian Mixture","lvl2":"Clustering"},"content":"Gli algoritmi di segmentazione prima visti non assumono alcuna ipotesi sulla distribuzione probabilistica dei dati. Questo in generale può non essere corretto.\nConsideriamo ad esempio la separazione tra aria e acqua in un fantoccio MR esaminandone l’istogramma di Figura 3.22.\n\nFigura 3.22. Istogramma del fantoccio MR.\n\nUn algoritmo di clustering (o il metodo di Otsu) troverà una soglia che massimizza la separazione  tra i due picchi. Questa soglia però non terra conto del fatto che a causa dell’effetto volume parziale i pixel dell’acqua possono assumere comunque livelli di grigio inferiori alla soglia sui bordi. Inoltre la distribuzione dell’aria è non gaussiana (è di tipo Riciano) e quindi i pixel dell’aria hanno maggior probabilità di superare la soglia rispetto a quelli dell’acqua. L’approccio di tipo EM consente di tener conto di tali peculiarità introducendo informazioni sulla distribuzione di probabilità dei vari cluster.L’approccio di tipo EM introduce quindi il clustering basato su modelli (model-based approach), dove la distribuzione dei dati sui singoli cluster viene modellata attraverso funzioni probabilistiche note, tra le quali la distribuzione di tipo gaussiano è la più utilizzata. L’approccio modellistico consente di tenere in conto eventuali ipotesi sulla generazione dei dati e sul rumore che li accompagna. In pratica, ogni cluster è rappresentato matematicamente da una distribuzione di tipo parametrico, come le distribuzioni Gaussiana (Continua) o di Poisson (Discreta). L’insieme dei dati è modellato come una combinazione di queste distribuzioni. Le singole distribuzioni sono chiamate distribuzioni componenti.\n\nIl caso più semplice nel clustering probabilistico è quello della combinazione di gaussiane (Gaussian Mixtures). I cluster sono modellati come gaussiane centrate sui centroidi dei cluster. In Figura 3.23 i cerchi in grigio rappresentano la varianza delle distribuzioni. Possiamo pensare che i due assi rappresentino il livello di grigio dei pixel di due immagini ed i punti nel piano il livello di grigio del singolo pixel. Vogliamo raggruppare gli spot in due classi, supponiamo quindi di avere due cluster (k=2). Dato un punto del piano, il punto avrà una certa probabilità di essere stato generato da ognuno dei due cluster, probabilità che dipende dai parametri della gaussiana che descrive il cluster stesso. In particolare, una gaussiana con un centro “lontano” dal punto o con una varianza piccola avrà basse probabilità di aver generato il punto, e viceversa. Nel caso delle immagini, una immagine con basso SNR avrà più possibilità di generare un pixel lontano dal suo valor medio e viceversa.\n\nFigura 3.23. Esempio di Gaussian Mixtures.\n\nSe i parametri che descrivono le gaussiane sono noti, il problema si riduce ad utilizzare come distanza nell’algoritmo di clustering la probabilità che un certo dato sia stato generato da una certa gaussiana. Avremo quindi:d(x_i, c_j) = P(x_i \\mid G_j) = P(x_i \\mid m_j, \\sigma_j)\n\nIl problema diviene più complesso quando i parametri caratterizzanti le gaussiane non sono noti. L’algoritmo di clustering dovrà quindi stimare oltre ai cluster anche i parametri delle distribuzioni componenti. Quello che vogliamo trovare sono quindi i parametri delle gaussiane che hanno la maggiore probabilità di aver generato i dati osservati.\n\nIl procedimento da utilizzare, detto EM (Expectation Maximization), è di tipo iterativo. L’algoritmo EM è molto generale e può essere utilizzato per le più diverse distribuzioni di probabilità. Facciamo un esempio semplice nell’ipotesi che le distribuzioni di probabilità siano Gaussiane.\n\nIl problema si può formalizzare come segue:\n\nSiano date K classi; nel caso della segmentazione le classi corrispondono ai pattern dell’immagine.\n\nOgni classe abbia una probabilità  “a priori” P_k; nel caso della segmentazione la probabilità a priori corrisponde al numero normalizzato dei pixel appartenenti alla classe (altezza del picco nell’istogramma).\n\nOgni classe abbia media M_k e SD \\sigma _k; nel caso della segmentazione media e SD descrivono la posizione e larghezza del picco dell’istogramma relativo alla classe.\n\nK (numero delle classi) è noto mentre gli altri parametri devono essere determinati.\n\nSe ciascuna classe obbedisce ad una legge di probabilità gaussiana si può definire per la distribuzione dei livelli di grigio dell’immagine (cioè l’istogramma dell’immagine):h(s) = \\sum_{k=1}^{K} P_k \\, G(s \\mid M_k, \\sigma_k)\n\nSi tratta di scegliere il vettore delle probabilità a priori P, delle medie M e delle SD \\sigma in modo da minimizzare la differenza tra istogramma e somma di gaussiane. Il problema può essere affrontato tramite l’algoritmo EM che è composto da due passi che vengono iterati. Nel caso della Gaussian Mixture, l’algoritmo EM è abbastanza semplice ed è composto dai due passi seguenti:\n\nSi definisce un valore iniziale dei valori P, M e \\sigma. Essendo l’algoritmo EM un ottimizzatore locale il risultato finale dipenderà dai valori iniziali. Dati P, M e \\sigma si calcolano le probabilità che un certo valore di segnale sia stato generato da un certa gaussiana.\n\nPasso E-step (Expectation) calcolo della probabilità che il dato x_j sia stato generato dalla gaussiana i.p_{ij} = P_i \\, G(x_j \\mid M_i, \\sigma_i)\n\nche è facilmente ricavabile dalla formula della gaussiana, essendo il valore della gaussiana stessa nel punto per la probabilità a priori che un pixel appartenga a quella gaussiana.\n\nIl secondo passo è detto M-step (maximization) e consiste nell’aggiornare i parametri delle gaussiane in base al passo precedente. Definiamo:\np_i = \\sum_{j} p_{ij}\n\n\nIl passo M consiste nelle operazioni:\n\\mu_i \\leftarrow \\frac{1}{p_i} \\sum_{j} p_{ij}\\, x_j\n\n\n\\sigma_i \\leftarrow \\frac{1}{p_i} \\sum_{j} p_{ij}\\, x_i x_j\n\n\nP_i \\leftarrow p_i\n\nIn questo modo abbiamo definito delle nuove gaussiane e torniamo al passo E. L’algoritmo al solito si ferma quando i valori della media, della varianza e dei pesi si stabilizzano. A questo punto i parametri trovati descrivono i cluster.\nE’ possibile dimostrare che l’algoritmo EM aumenta il grado di verosimiglianza della combinazione di gaussiane ad ogni iterazione e converge sotto certe condizioni ad un massimo locale della verosimiglianza della distribuzione di gaussiane. La dimostrazione è estremamente complessa e non viene qui riportata. L’algoritmo EM può essere utilizzato non sono nella risoluzione del problema della Gaussian Mixture, ma in molte altre applicazioni.\nI problemi fondamentali in cui può incorrere l’algoritmo EM applicato alle gaussian mixture sono il fatto che due gaussiane convergano alla stessa gaussiana, con la scomparsa di un cluster, e che una gaussiana assuma varianza infinita e quindi si trasformi in un valore costante. Al solito variare le condizioni iniziali partendo da una stima ragionevole dei parametri può ottimizzare il funzionamento dell’algoritmo EM.\n\nL’algoritmo EM-GM è implementato in MATLAB dalla funzione fitgmdist\n(nello Statistics and Machine Learning Toolbox). In Python, l’algoritmo EM per modelli di tipo Gaussian Mixture è implementato nella libreria scikit-learn tramite la classe\nsklearn.mixture.GaussianMixture.\n\nFigura 3.24. Output dell’algoritmo EM-GM su fantoccio MR.\n\nApplicato all’immagine del fantoccio MRI, il metodo fornisce tre gaussiane, con valori medi   [11.7349, 170.7140, 905.0376] simili ai centroidi dei cluster prima trovati, e  varianze [137, 858, 12000], probabilmente per l’effetto del volume parziale tra acqua e olio che costringe a “spalmare” le due gaussiane.\n\nLa somma delle gaussiane trovate rappresenta poi un approssimazione dell’istogramma dell’immagine, per cui l’algoritmo EM può anche essere considerato un metodo per modellare l’istogramma.","type":"content","url":"/c31-segmentazione#algoritmo-em-expectation-maximization-e-gaussian-mixture","position":17},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Clustering Gerarchico","lvl2":"Clustering"},"type":"lvl3","url":"/c31-segmentazione#clustering-gerarchico","position":18},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Clustering Gerarchico","lvl2":"Clustering"},"content":"In generale, è possibile che un cluster di dati possa essere diviso a sua volta in sub-cluster più piccoli, che a loro volta possono essere divisi in sub-cluster ancora più piccoli e così via. Un esempio tipico è la classificazione degli organismi vegetali o animali, che vengono divisi in categorie sempre più specifiche (ordini, specie). Nell’imaging il clustering gerarchico può essere utilizzato per raggruppare immagini appartenenti ad esempio ad una serie temporale, come si vedrà nel capitolo dedicato agli algoritmi di registrazione. Il clustering gerarchico è una tecnica che permette di creare un albero gerarchico, in cui gli elementi dell’insieme su cui si opera il clustering sono le foglie dell’albero. Una riga orizzontale nell’albero individua una serie di cluster analoghi a quelli ottenuti nel clustering tradizionale.\nDato un insieme di N oggetti da raggruppare, ed una matrice N \\times N di distanze tra gli oggetti, il processo di clustering gerarchico può essere definito come (S.C. Johnson 1967):\n\nSi definiscono N cluster, uno per ogni oggetto. Abbiamo quindi N cluster che contengono ognuno un solo oggetto. Le distanze tra i cluster saranno uguali alle distanze tra gli oggetti che in questo primo passo si identificano con i cluster stessi.\n\nSi identificano i due cluster più vicini nel senso della distanza adottata, cioè più simili. Questi due cluster vengono raggruppati in un cluster unico, abbiamo così N-1 cluster, uno con due oggetti e gli altri con un solo oggetto.\n\nRicomputiamo la matrice delle distanze, che sarà ora una matrice N-1 \\times N-1.\n\nSi ripetono i passi 2 e 3 fino a quando non rimane un solo cluster che contiene N oggetti.\n\nIl passo 3 può essere eseguito in modi diversi, in base ai diversi approcci possibili riconosciamo tre categorie di clustering gerarchico: single-linkage (singolo collegamento), complete-linkage (collegamento completo)  e average-linkage (collegamento mediato).\nNel single-linkage clustering, la distanza tra due cluster è definita come la minima distanza tra tutti gli elementi di un cluster e tutti quelli dell’altro cluster. In pratica si calcola la matrice delle distanze tra gli elementi dei due cluster e si prende come distanza tra i due cluster il minimo sulla matrice. Se invece di una funzione distanza si utilizza una funzione di similarità, cioè una funzione che è grande quando i due oggetti sono simili, si considererà il massimo della matrice di similarità.\nNel complete-linkage clustering (chiamato anche metodo del diametro o del massimo), la distanza tra due cluster sarà definita come il massimo sulla matrice delle distanze computata tra I dati sui due cluster. Nell’average-linkage clustering, la distanza tra due cluster sarà la media della matrice delle distanze computata sui dati appartenenti ai due cluster. Una variazione abbastanza usata del metodo precedente è il metodo che usa la mediana della matrice delle distanze invece che la media, che è meno sensibile a dati spuri.\nL’approccio descritto finora è di tipo agglomerativo, perche si basa sul ragruppamento progressivo di cluster sempre più grandi. Esiste anche un approccio inverso, dove si parte da un cluster che comprende tutti i dati e si procede per divisioni successive (divisive hierarchical clustering). Questo approccio è comunque molto meno diffuso.","type":"content","url":"/c31-segmentazione#clustering-gerarchico","position":19},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl4":"Esempio di Single-Linkage Clustering","lvl3":"Clustering Gerarchico","lvl2":"Clustering"},"type":"lvl4","url":"/c31-segmentazione#esempio-di-single-linkage-clustering","position":20},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl4":"Esempio di Single-Linkage Clustering","lvl3":"Clustering Gerarchico","lvl2":"Clustering"},"content":"Consideriamo un approccio single-linkage e descriviamo come è impostato un algoritmo che lo implementi. L’idea di base è cancellare in modo progressivo righe e colonne della matrice delle distanze dei dati coinvolti nell’operazione conglobando coppie di righe e di colonne in una sola riga o colonna.\nSia D = [d(i,j)] la matrice N \\times N delle distanze. Vogliamo ottenere N cluster 0,1,..., (N-1) dove L(k) è il livello gerarchico del singolo cluster. Il cluster m sarà indicato con (m) mentre la distanza tra due cluster (r) e (s) sara indicata da d[(r),(s)].\nL’algoritmo procederà nel modo seguente:\n\nInizia con il cluster di livello L(0) = 0 e di posizione nella sequenza dei cluster m = 0.\n\nTrova i due cluster più simili, siano essi (r) e (s), tali che: d[(r),(s)] = min d[(i),(j)] dove l’operazione di minimo è estesa a tutte le possibili coppie di cluster.\n\nIncrementa l’indice della sequenza dei cluster: m = m +1. Combina i cluster (r) e (s) in un singolo cluster di livello L(m) = d[(r),(s)]\n\nAggiorna la matrice delle distanze D, cancellando le righe e le colonne corrispondenti ai cluster (r) e (s) e aggiungendo una nuova riga ed una nuova colonna corrispondenti al nuovo cluster ottenuto al passo precedente. La distanza tra il nuovo cluster e un vecchi cluster k è definita come: d[(k), (r,s)] = min d[(k),(r)], d[(k),(s)]\n\nSe esiste un solo cluster, la procedura si ferma. Altrimenti vai al passo 2.\n\nUn esempio grafico del clustering gerarchico è riportato in Figura 3.25.\n\nFigura 3.25. Esempio di clustering gerarchico.\n\nE’ importante notare che il processo è dipendente dalla funzione distanza scelta. Scelte diverse della funzione distanza producono risultati anche completamente diversi.","type":"content","url":"/c31-segmentazione#esempio-di-single-linkage-clustering","position":21},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Metriche negli algoritmi di clustering","lvl2":"Clustering"},"type":"lvl3","url":"/c31-segmentazione#metriche-negli-algoritmi-di-clustering","position":22},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Metriche negli algoritmi di clustering","lvl2":"Clustering"},"content":"Negli algoritmi d clustering è importante la scelta della metrica, cioè della grandezza che misura la differenza (o distanza) tra due punti nello spazio da classificare.\nLa scelta più immediata è quella di distanze di tipo geometrico, scelte cioè nella famiglia delle metriche di Minkowski:d(x_i, x_j) =\n\\left(\n\\sum_{q=1}^{K}\n\\lvert x_{iq} - x_{jq} \\rvert^{\\,p}\n\\right)^{\\frac{1}{p}}\n\nche comprendono la classica distanza euclidea come caso particolare per p=2. Per p=1 abbiamo la cosiddetta distanza Manhattan.\n\nIn Figura 3.26 vengono riportate le principali metriche utilizzate negli algoritmi di clustering come misura di distanza tra gli elementi.\n\nFigura 3.26. Metriche utilizzate negli algoritmi di clustering.\n\nCome si vede la maggior parte delle distanze sono derivate dalla distanza di Minkowski. Un esempio di applicazione della correlazione come distanza può essere la segmentazione di immagini 2D+T con iniezione di mezzo di contrasto, nella quale ci interessa raggruppare tra loro i pixel che rispondono in modo simile all’all’arrivo del contrasto.\nInfine è opportuno notare come il funzionamento di un algoritmo di clustering multidimensionale dipenda dalla scala su cui sono misurati i valori dei dati su cui fare il clustering. Se i dati non sono omogenei, i dati con valori maggiori peseranno di più nel computo della distanza falsando il comportamento dell’algoritmo. Si pensi ad esempio ad un clustering in cui si utilizzano immagini su 8 bit ed immagini a 16 bit. In questi casi è opportuno normalizzare i dati su distribuzioni di tipo standard (tipicamente distribuzioni gaussiane con media zero e deviazione standard 1), un processo detto standardizzazione delle variabili.\n\nL’operazione di standardizzazione in MATLAB è implementata tramite le funzioni zscore\no normalize (a partire dalla versione R2018). In Python, la standardizzazione dei dati può essere effettuata utilizzando la libreria\nscikit-learn, tramite la classe sklearn.preprocessing.StandardScaler, oppure\nmediante funzioni equivalenti disponibili in NumPy e SciPy.","type":"content","url":"/c31-segmentazione#metriche-negli-algoritmi-di-clustering","position":23},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Algoritmi di Labeling"},"type":"lvl2","url":"/c31-segmentazione#algoritmi-di-labeling","position":24},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Algoritmi di Labeling"},"content":"Come detto in precedenza, la segmentazione a soglia non è in grado di distinguere oggetti topologicamente diversi ma ai quali è associato lo stesso livello di segnale. Nell’immagine del fantoccio ad esempio abbiamo pattern topologicamente distinti che vengono estratti come un oggetto unico. Questo è un limite intrinseco della segmentazione a soglia, che può essere superato elaborando i dati della segmentazione stessa.\nIl metodo più diretto è scomporre la segmentazione usando un algoritmo detto “label region”.  L’algoritmo lavora su una immagine binaria (quindi una maschera) e funziona nel seguente modo:\n\nPer ogni pixel dell’immagine:\n\nSe è il primo pixel, crea un gruppo (blob) e aggiungi il pixel al blob\n\nPer tutti i pixel dell’intorno: a. Controlla se sono già stati assegnati a un blob. Se si, vai a 2 b. Controlla se appartengono allo stesso blob del pixel di partenza. Se si, aggiungili al blob e vai a 2.\n\nCompletato il primo blob, si prende un pixel non appartenente al blob e si ritorna a 1, creando un nuovo blob. Si itera fino a quando tutti i pixel sono stati assegnati.\nI blob ottenuti possono essere classificati per grandezza e i blob più piccoli eventualmente eliminati (come nel filtro mediano). Per pixel dell’intorno si intendono i pixel contenuti in un kernel centrato sul pixel stesso di dimensioni 3x3 e può includere o meno le diagonali.\n\nIn MATLAB le funzioni utilizzate per il labeling delle regioni connesse sono bwlabel\no bwconncomp. In Python, funzionalità equivalenti sono fornite dalla libreria scikit-image tramite\nle funzioni skimage.measure.label e skimage.measure.regionprops, oppure dalla libreria\nSciPy mediante la funzione scipy.ndimage.label.\n\nSe consideriamo il fantoccio MR di Figura 3.27 (sinistra) e applichiamo una segmentazione a soglia,\nad esempio mediante l’algoritmo k-means, si ottiene una maschera dell’acqua come\nmostrato al centro della figura. L’algoritmo di labeling restituisce quindi una mappa\nin cui le regioni non appartenenti alla maschera dell’acqua sono impostate a 0,\nmentre la maschera viene suddivisa in una serie di regioni topologicamente connesse,\nciascuna identificata da un indice univoco.\n\nFigura 3.27. Fantoccio MR, segmentazione e labeling.","type":"content","url":"/c31-segmentazione#algoritmi-di-labeling","position":25},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Region Growing"},"type":"lvl2","url":"/c31-segmentazione#region-growing","position":26},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Region Growing"},"content":"Una estensione della segmentazione a soglia è l’algoritmo region growing”*, dove partendo da un pixel all’interno dell’oggetto da segmentare si estende la segmentazione a tutta la regione di interesse. In questo caso la soglia è quindi definita in modo locale come la differenza di segnale tra un pixel ed i pixel vicini. Questo approccio sfrutta le informazioni spaziali e garantisce la formazione di regioni tra loro collegate. Di fatto è una implementazione ricorsiva di una segmentazione effettuata per pixel adiacenti.\nLa routine prevede la definizione di un punto di partenza all’interno del pattern da riconoscere. Il valore di livello di grigio così individuato costituisce il punto di partenza (detto ‘seed pixel’, ‘pixel seme’) per la successiva elaborazione: si vanno  ad analizzare ricorsivamente i pixel adiacenti a quello selezionato inizialmente. Quelli che hanno una differenza di livello di grigio appartenente ad un intervallo prefissato vengono selezionati, gli altri scartati.\n\nL’approccio region growing prevede quindi i seguenti passi:\n\nSi sceglie un arbitrario ‘seed pixel’ che viene confrontato con quelli adiacenti.\n\nDal seme si passa ad una regione che ‘cresce’ aggiungendo progressivamente quei pixel adiacenti che sono ‘simili’ a quello iniziale secondo criteri di somiglianza fissati.\n\nQuando la crescita della regione si ferma il processo termina\n\nIl seed pixel può essere scelto in modo manuale o con algoritmi automatici.\nLa funzione “paint” definita in tutti i programmi di image processing utilizza questo metodo. Questo tipo di segmentazione consente di individuare in modo ottimale i confini degli oggetti individuati per osservazione.\nIl punto cruciale della tecnica region growing è il criterio di similarità dei pixel, che determina se un pixel debba o meno essere aggiunto alla regione. Nel caso di immagini binarie, il criterio di simiglianza si identifica con l’uguaglianza del valore dei pixel. In questo caso l’algoritmo si comporta come l’algoritmo di labeling.\nNel caso di immagini a livello di grigio, la condizione di inclusione diviene |s0-s1| < T, dove T è un valore di soglia. Si include quindi un pixel se il suo valore differisce da quello di un pixel adiacente appartenente alla regione per meno di T. Per il buon funzionamento dell’algoritmo, T deve essere abbastanza grande da includere tutti i pixel nella regione omogenea che vogliamo segmentare e abbastanza piccolo da impedire che la segmentazione “debordi” nei tessuti vicini.\n\nIn Figura 3.28, da sinistra a destra, un esempio di segmentazione region growing con T ottimale, con T troppo basso (alcuni pixel vengono persi) e con T troppo alto.\n\nFigura 3.28. Esempio di region growing per diversi valori della soglia T.\n\nT può essere definito esplicitamente come numero di livelli di grigio (es. T=50). E’ anche possibile definire T come percentuale rispetto al valore dei pixel, la condizione di inclusione diviene \\frac{|s0-s1|}{mean(s0,s1)} < T . In questo caso T sarà compreso tra 0 e 1 e un pixel verrà incluso se la sua differenza rispetto al pixel vicino è inferiore al (100*T)\\%.\nUn approccio ragionevole può essere quello di definire T come proporzionale al rumore sull’immagine. Infatti, se consideriamo una zona omogenea dell’immagine, come ad esempio l’acqua o l’olio nel fantoccio, il segnale nella regione omogenea sarà S_0 + \\mathcal{N}(0, \\sigma)\nnel caso di rumore gaussiano a media nulla. Se ad esempio poniamo T=1.96\\sigma, otterremo di includere il 95\\% dei pixel della regione. Questo approccio è simile a quello visto nel design dei filtri adattivi.\n\nL’algoritmo di region growing è implementato in MATLAB dalla funzione grayconnected. In Python, un’implementazione analoga del region growing (a partire da un seed e con un\ncriterio di similarità/threshold) è disponibile nella libreria scikit-image tramite\nla funzione skimage.segmentation.flood (o skimage.segmentation.flood_fill).","type":"content","url":"/c31-segmentazione#region-growing","position":27},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Segmentazione a contorni"},"type":"lvl2","url":"/c31-segmentazione#segmentazione-a-contorni","position":28},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Segmentazione a contorni"},"content":"Un approccio diverso al problema della segmentazione consiste nell’identificare sull’immagine i contorni dei pattern, che corrispondono alle zone di transizione tra due tessuti diversi.\nConsideriamo l’immagine del fantoccio in Figura 3.29 ed il relativo profilo dei livelli di grigio estratto dall’immagine stessa.\n\nFigura 3.29. Fantoccio MR e relativo profilo.\n\nI contorni visti sul profilo saranno caratterizzati da una transizione netta del segnale. Se calcoliamo la derivata del profilo, valori alti della derivata corrisponderanno ai contorni, come si vede dalla figura. Per trovare i contorni, possiamo definite una soglia sul valore assoluto della derivata, valori della derivata superiori alla soglia definiscono una transizione e quindi un contorno.\n\nFigura 3.30. Fantoccio MR e derivata del profilo.\n\nEstendendo il concetto in 2D o 3D, un mappa di gradiente dell’immagine consente di identificare i contorni. La mappa di gradiente di una immagine può essere ottenuta in vari modi, tipicamente attraverso un filtro convolutivo. Ad esempio il filtro di Sobel calcola la derivate del gradiente per righe e per colonne utilizzando i due kernel convolutivi:G_x= \n\\begin{bmatrix}\n-1 & 0 & 1 \\\\\n-2 & 0 & 2 \\\\\n-1 & 0 & 1 \n\\end{bmatrix}G_y = \n\\begin{bmatrix}\n1 & 2 & 1 \\\\\n0 & 0 & 0 \\\\\n-1 & -2 & -1 \n\\end{bmatrix}\n\ne il gradiente viene poi calcolato come G=\\sqrt{(G_x^2+G_y^2)}\nUn’altra possibilità è usare il filtro Laplaciano:L = \n\\begin{bmatrix}\n0 & -1 & 0 \\\\\n-1 & 4 & -1 \\\\\n0 & -1 & 0 \n\\end{bmatrix}\n\nla dimensione del kernel può essere aumentata per ottimizzare il calcolo.\n\nLe operazioni di filtraggio convolutivo possono essere implementate in MATLAB tramite la\nfunzione conv2; tuttavia, è in genere preferibile utilizzare funzioni specifiche\nfornite dall’Image Processing Toolbox.\n\nIn particolare, la funzione edge consente di individuare i contorni dell’immagine\nmediante l’applicazione di diversi filtri, selezionabili tra Sobel, Roberts, Laplaciano,\necc. Sull’immagine di gradiente viene quindi applicata una soglia al fine di eliminare\ni valori di gradiente più bassi e mettere in evidenza i contorni. In Python, operazioni equivalenti possono essere effettuate utilizzando le librerie\nscikit-image e SciPy. In particolare, la libreria scikit-image fornisce la\nfunzione skimage.filters.sobel e la funzione skimage.filters.roberts per il calcolo\ndel gradiente, nonché la funzione skimage.filters.laplace per l’operatore laplaciano.\nIl rilevamento dei contorni può inoltre essere effettuato tramite la funzione\nskimage.feature.canny, che include internamente il calcolo del gradiente e\nl’applicazione di una soglia.\n\nUn esempio di rilevamento dei contorni è riportato in Figura 3.31.\n\nFigura 3.31. Segmentazione dei contorni.\n\nUna volta ottenuta la mappa dei contorni (edge map) con questo tipo di algoritmi, è necessario costruire un contorno continuo che unisca le aree ad alto valore di gradiente. Uno degli approcci più semplici per collegare i punti di discontinuità in un’immagine di gradiente è quello di analizzare le caratteristiche dei pixel in un intorno ristretto (3 \\times 3 o 5 \\times 5) del punto in esame. In altre parole si cercano delle proprietà comuni ai punti dell’intorno e successivamente si collegano i punti simili con qualche criterio. La creazione di un contorno continuo da una mappa di gradiente utilizza tipicamente due parametri (algoritmo di Canny):\n\nIntensità della mappa di gradiente. Un punto nell’intorno è simile al punto considerato se la differenza del valore di modulo del gradiente nel punto è minore di una soglia T.\n\\left\\lVert G(x,y) - G(x',y') \\right\\rVert \\le T\n\nDirezione del gradiente.  Un punto nell’intorno è simile al punto considerato se la direzione del gradiente nel punto (data dal rapporto tra le componenti x e y del gradiente) è minor e di una soglia A.\n\\lvert \\alpha(x,y) - \\alpha(x',y') \\rvert \\le A\n\n\n\\alpha(x,y) = \\angle G(x,y)\n= \\tan^{-1}\\!\\left(\n\\frac{\\partial f(x,y)/\\partial y}\n  {\\partial f(x,y)/\\partial x}\n\\right)\n\nIl processo può essere completato eliminando piccoli tratti di segmenti isolati e colmando piccoli intervalli tra i segmenti.","type":"content","url":"/c31-segmentazione#segmentazione-a-contorni","position":29},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Active Contours (Snakes)","lvl2":"Segmentazione a contorni"},"type":"lvl3","url":"/c31-segmentazione#active-contours-snakes","position":30},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Active Contours (Snakes)","lvl2":"Segmentazione a contorni"},"content":"I metodi prima visti hanno il limite fondamentale di non includere al loro interno un modello dell’oggetto da segmentare. Nell’imaging biomedico, ad esempio, possiamo senz’altro supporre che gli organi siano oggetti con una superficie che non presenta spigoli o punti ad elevata curvatura. Per molti organi abbiamo inoltre una informazione a priori sulla forma, ad esempio un vaso sarà simile ad un cilindro, un ventricolo visto in asse lungo ad un ellissoide, etc.\nEsistono vari algoritmi che introducono queste informazioni ottimizzando la segmentazione. L’esempio più noto sono gli algoritmi a contorni attivi o snakes.\nL’algoritmo tradizionale, introdotto per la prima volta da Kass, consiste nell’inizializzare una certa curva (snake) e nel deformarla poi in modo da farla convergere in corrispondenza di un minimo locale di energia, cioè di un contorno dell’immagine.\nIl primo problema da affrontare è quindi quello della definizione del contorno iniziale: essendo un metodo di analisi locale infatti i risultati dipenderanno dal modo in cui è stato inizializzato lo snake. Pertanto in dipendenza dell’applicazione si dovrà decidere se inizializzare automaticamente la curva oppure usare una procedura semiautomatica in cui le condizioni iniziali sono gestite direttamente da un utente esperto.\nLa curva iniziale C è definita da un insieme di N punti ordinati caratterizzati dalle loro coordinate  (x_i,y_i) per i=1,..,N. La curva è chiusa quindi il punto N è connesso con il punto 1 (Figura 3.32).\n\nFigura 3.32. Esempio di curva chiusa.\n\nPossiamo anche definire C come una curva parametrica in s, dove $s indica la posizione del punto sulla curva:C(s) = {(x(s), y(s)) : 0 ≤ s ≤ 1}\n\nAd uno snake può essere associata una energia, vista come somma di una componente interna e di una esterna o associata all’immagine:E(C) = \\int_{0}^{1} \\left( E_{\\mathrm{int}} + E_{\\mathrm{imm}} \\right)\n\nFacendo evolvere lo snake in modo che l’energia dello snake venga minimizzata, si ottiene una curva che evolvendosi nel tempo va ad operare una segmentazione sulle immagini. Le forze interne tendono a conservare la forma dello snake e quindi determinano il modo in cui lo snake si evolve, quelle esterne sono correlate all’immagine e determinano le caratteristiche che vengono rilevate.\nL’energia interna può essere divisa in due componenti:E_{\\mathrm{int}}(C(s)) =\n\\alpha \\, \\lvert C'(s) \\rvert^{2}\n+ \\beta \\, \\lvert C''(s) \\rvert^{2}\n\nLa prima componente è legata alla derivata prima e rappresenta la tendenza dello snake a mantenere la sua forma opponendosi alla trazione. L’energia elastica è legata alla distanza dei punti dello snake, aumentando la distanza tra due punti aumenta anche l’energia elastica.  La seconda componente (rigidità) è legata alla derivata seconda e rappresenta la tendenza dello snake ad opporsi alle modifiche della sua curvatura. Impedisce allo snake di “ingarbugliarsi”.\nManipolando la tensione o elasticità e la rigidità dello snake è possibile modificare l’importanza relativa delle due componenti dell’energia interna. Intuitivamente si può pensare allo snake come ad un elastico, che se lasciato libero riassume la sua forma originale grazie alla sua elasticità.Il secondo contributo viene di solito chiamato energia dell’immagine o energia esterna in quanto viene ricavata dall’immagine stessa in modo da assumere valore minimo nelle zone di maggiore interesse come ad esempio i bordi. Tipiche e semplici funzioni di energia dell’immagine sono legate all’edge map dell’immagine:E_{\\mathrm{imm}}(X(s)) = - \\lvert \\nabla I(x,y) \\rvert^{2}E_{\\mathrm{imm}}(X(s)) =\n- \\left\\lvert\n\\nabla \\bigl( G_{\\sigma}(x,y) * I(x,y) \\bigr)\n\\right\\rvert^{2}\n\ndove G_{\\sigma}(x,y) è una maschera gaussiana bidimensionale con una certa deviazione standard (filtro gaussiano) aumentando la quale si rendono i contorni più sfocati, ma si aumenta il range di cattura di tali contorni. Introducendo dei pesi per le forze prima definite abbiamo per l’energia dello snake:E_{\\mathrm{Snake}} =\n\\alpha \\int \\lvert C'(s) \\rvert^{2} \\, ds\n+ \\beta \\int \\lvert C''(s) \\rvert^{2} \\, ds\n+ k \\, E_{\\mathrm{imm}}(s)\n\nDove \\alpha è il peso dell’energia elastica, \\beta il peso dell’energia di curvatura, k il peso delle forze esterne relative all’immagine.\n\nL’idea è di lasciare evolvere lo snake sull’immagine fino quando non raggiunge un minimo dell’energia. Questo stato di minimo corrisponderà ad una situazione stabile in cui lo snake corrisponderà ai contorni definiti sull’immagine stessa a meno dei limiti di rigidità ed elasticità imposti. L’algoritmo per minimizzare l’energia è un processo iterativo che parte da un contorno iniziale e converge (se la differenza della posizione dello snake tra due iterazioni è abbastanza piccola) ad un minimo locale. Il processo iterativo è efficiente dal punto di vista computazionale (ogni passo è rappresentato dalla moltiplicazione di matrici numeriche) ed il suo risultato dipende dai valori \\alpha, \\beta e k. Da un punto di vista pratico l’utilizzo del metodo AC (active contours) richiede il tuning accurato dei parametri \\alpha, \\beta e k, e la possibilità di definire un contorno iniziale abbastanza vicino al risultato desiderato in modo da ottimizzare la convergenza dell’operatore locale.\nCome si osserva dall’equazione dello snake, le forze esterne sono computate solo sullo snake stesso. Questo implica che lo snake nella sua evoluzione “vede” i contorni dell’immagine solo quando gli attraversa. Una modifica dell’algoritmo implica l’uso di informazioni interne ed esterne allo snake (region-based models) in modo da favorire la convergenza.\n\nL’implementazione più nota è l’algoritmo di Chan–Vese che troviamo implementato in MATLAB\nnella funzione activecontour. In Python, un’implementazione equivalente dell’algoritmo di Chan–Vese è disponibile\nnella libreria scikit-image tramite la funzione skimage.segmentation.chan_vese,\noltre alla variante morfologica skimage.segmentation.morphological_chan_vese.\n\nConsideriamo una immagine con due pattern con livello di grigio I_1 e I_2, a meno del rumore e di altri fattori quali il PVE e l’attenuazione. Nell’approccio di Chan-Vese l’energia da minimizzare è valutata come:F(C) = F_1(C) + F_2(C)\n= \\int_{\\text{inside}(C)} \\lvert I - C_1 \\rvert^{2}\n+ \\int_{\\text{outside}(C)} \\lvert I - C_2 \\rvert^{2}\n\nDove I è l’immagine, C_1 è la media dei pixel dell’immagine dentro il contorno C, C_2 è la media dell’immagine fuori dal contorno C. Chiaramente F(C) si minimizza quando C corrisponde al bordo tra i due pattern e divide l’immagine in due regioni omogenee in cui F_1 e F_2 sono uguali alla potenza del rumore associato all’immagine. L’approccio realmente implementato è più complesso in quando si estende ad una immagine ad N pattern omogenei.\n\nIn MATLAB la funzione activecontour implementa le versioni proposte da Chan (Chan–Vese) e Caselles (edge).La funzione dei parametri alfa e beta dello snake è svolta dai parametri ContractionBias (determina se il contorno si espande o si restringe, alfa) e SmoothFactor (determina la regolarità del contorno, beta). In Python, le controparti più dirette sono disponibili nella libreria scikit-image:\n\nl’implementazione di Chan–Vese è skimage.segmentation.chan_vese (e la variante morfologica morphological_chan_vese);\n\nl’implementazione basata su edge (snake/Caselles) è skimage.segmentation.active_contour, che accetta i parametri alpha (termine di elasticità) e beta (termine di rigidità/smooting), corrispondenti concettualmente a ContractionBias e SmoothFactor in MATLAB.\n\nGli algoritmi a contorni attivi funzionano su immagini bidimensionali. In teoria è possibile estendere l’algoritmo ad immagini 3D, in questo caso il contorno diviene una superficie chiusa (balloon) definita da una mesh geometrica come negli algoritmi di visualizzazione 3D. Nella pratica nell’elaborazione di immagini 3D si utilizza un approccio diverso detto level set.","type":"content","url":"/c31-segmentazione#active-contours-snakes","position":31},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Level set","lvl2":"Segmentazione a contorni"},"type":"lvl3","url":"/c31-segmentazione#level-set","position":32},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Level set","lvl2":"Segmentazione a contorni"},"content":"L’algoritmo level set (o implicit Active Contour) definisce il contorno C come l’intersezione di una funzione \\Phi(x,y) con il piano dell’immagine che corrisponde a \\Phi(x,y)=0.\n\nLa Figura 3.33 mostra come il processo iterativo che modifica \\Phi nel tempo causi l’evoluzione del contorno C sul piano.\n\nFigura 3.33. Level-set. Hoang Ngan Le T. et al. (2020) Active Contour Model in Deep Learning Era: A Revise and Review. In: Oliva D., Hinojosa S. (eds) Applications of Hybrid Metaheuristic Algorithms for Image Processing. Studies in Computational Intelligence, vol 890. Springer, Cham.\n\nDa un punto di vista formale si definisce:C = {(x, y): \\phi(x, y) = 0},\n\nl’evoluzione della curva è governata dall’equazione:\\frac{\\partial \\phi}{\\partial t}\n+ F \\, \\lvert \\nabla \\phi \\rvert = 0\n\nDove F è la “speed function” che caratterizza l’algoritmo. F deve essere zero sui contorni dell’immagine in modo da assicurare la convergenza. Ad esempio potremmo definire:F(x,y) =\n\\frac{1}{1 + \\lambda \\, \\lvert \\nabla I(x,y) \\rvert}\n\nQuindi per un valore del gradiente dell’immagine alto (transizione tra tessuti)F tenderà a 0, per un valore del gradiente molto basso (regione omogenea) F tenderà a 1 ed il contorno avanzerà velocemente. Il parametro \\lambda modula il peso del gradiente sul valore di F.\n\nPartendo da un valore iniziale di \\Phi, e quindi di C, viene applicato un algoritmo iterativo che simula nel discreto l’evoluzione temporale ed il contorno si modifica seguendo l’evoluzione di \\Phi fino alla convergenza. Una potenzialità interessante dell’algoritmo level set è che il contorno definito da \\Phi si può dividere in due o più contorni durante la progressione dell’algoritmo (e viceversa) permettendo una maggiore flessibilità rispetto all’algoritmo a contorni attivi.\n\nUn esempio del funzionamento dell’approcio level-set è riportato in Figura 3.34.\n\nFigura 3.34. Level-set.\nDa Wikipedia (Eng).","type":"content","url":"/c31-segmentazione#level-set","position":33},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Watershed transform"},"type":"lvl2","url":"/c31-segmentazione#watershed-transform","position":34},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Watershed transform"},"content":"Una importante famiglia di tecniche per la segmentazione di immagini si basa su concetto della ricerca delle componenti connesse (Connected component, CC). Il concetto alla base di queste tecniche è la cosiddetta watershed transform, (spartiacque) nella quale il valore di livello di grigio dell’immagine rappresenta la profondità di un pixel rispetto ad un valore di riferimento. Una immagine viene quindi vista come una struttura (ad esempio un lago) che viene via via riempita con acqua in una certa posizione. Man mano che si versa l’acqua, il pelo dell’acqua stessa si alza disegnando delle strutture (maschere) che individuano le regioni sotto una certa altezza. Quando si arriva ad un watershed, l’acqua trabocca in una regione vicina. L’algoritmo, concettualmente simile al region growing, individua le configurazioni “borderline” che precedono il travaso dell’acqua da una regione dove la superficie dell’acqua è “stabile” ad un’altra.\n\nConsideriamo il fantoccio MR in Figura 3.35 e il relativo profilo.\n\nFigura 3.35. Fantoccio MR e relativo profilo.\n\nL’asse Y del profilo esprime in questo caso la distanza dal fondo che ha riferimento 0. Immettiamo l’acqua da un seed posto sul fondo. L’acqua riempirà i pixel connessi al seed e più “bassi” (quindi con livello di grigio minore) del seed.  Questa funzione è implementata in Matlab da grayconnected.\n\nMan mano che sale l’acqua i pixel del fondo vengono sommersi e la maschera risultante cresce.\n\nFigura 3.36. Maschere al crescere della soglia.\n\nA Th=1 viene coperto lo zero-padding, a Th=20 una parte del fondo. Ad un certo punto la maschera si stabilizza fino a quando il livello non supera lo spartiacque della parete del fantoccio, dopodiché ricomincia a crescere lentamente fino a stabilizzarsi di nuovo fino al riempimento dell’olio.\nSe tracciamo il grafico dell’area della maschera in funzione del livello otteniamo la situazione in Figura 3.37, dove come si è detto periodi di area stabile indicano che sono state riconosciute le strutture principali del fantoccio. L’algoritmo watershed attraverso soglie opportune riconosce le transizioni tra i periodi di riempimento e produce le relative maschere. Un metodo semplice è calcolare il gradiente della curva (in rosso in figura) e scegliere le soglie tra due picchi del gradiente.\n\nFigura 3.37. Dimensione delle maschere al crescere della soglia.\n\nUn esempio delle relative maschere che possono essere ottenute è riportato in Figura 3.38.\n\nFigura 3.38. Mappe per th=200, th=500, th=650, th=1900.\n\nIn Figura 3.39 riportiamo invece le mappe ottenute per sottrazione progressiva delle maschere.\n\nFigura 3.39. Mappe per sottrazione progressiva delle maschere.\n\nNotiamo che pur utilizzando un approccio a soglia, l’algoritmo riesce a distinguere regioni topologicamente non connesse.","type":"content","url":"/c31-segmentazione#watershed-transform","position":35},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Algoritmo MSERs","lvl2":"Watershed transform"},"type":"lvl3","url":"/c31-segmentazione#algoritmo-msers","position":36},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Algoritmo MSERs","lvl2":"Watershed transform"},"content":"L’algoritmo prima descritto può essere generalizzato ottenendo l’algoritmo Maximally Stable Extremal Regions (MSERs). L’algoritmo MSERs ricerca le regioni estreme massimamente stabili all’interno dell’immagine. Le regioni estreme sono definite come le regioni connesse i cui livelli di grigio sono tutti al di sopra o al di sotto dei valori dei pixel che circondano la regione. Le connessioni tra un pixel ed i pixel circostanti come al solito possono essere definiti dai 4 (6) pixel vicini o dagli 8 (24) pixel vicini in 2D o 3D, rispettivamente. Come visto in precedenza, tra le regioni estreme quelle massimamente stabili sono quelle dove, definita una soglia \\Delta, la variazione del numero di pixel della regione variando il livello di grigio g da g-\\Delta a g+\\Delta è un minimo locale. Il valore di \\Delta determina il numero di regioni che vengono trovate, che è inversamente proporzionale a \\Delta.L’algoritmo MSERs nella sua forma originaria prevede i seguenti passi:\n\nI pixel dell’immagine vengono ordinati secondo il valore del livello di grigio\n\nPartendo dal valore più basso di livello di grigio, i pixel vengono inseriti nell’immagine (si considera di partire da una immagine vuota) e si individuano le componenti connesse con un opportuno algoritmo di labeling.\n\nViene memorizzata per ogni livello di grigio la lista delle componenti connesse e la loro area. Se due componenti si uniscono, la più piccola viene inglobata nella più grande.\n\nSi ottiene quindi una lista che per ogni livello di grigio contiene l’area delle componenti connesse.\nPer ogni componente, si considera la curva area vs livello di grigio ed in corrispondenza dei minimi locali della curva si individuano le componenti massimamente stabili.\n\nDal punto di vista computazionale, il punto critico è il terzo. Bisogna per ogni passo ordinare le componenti per grandezza e riconoscere quali provengono da componenti esistenti al passo precedente. Per ottimizzare la procedura si memorizzano i dati con un approccio Component Tree, nel quale le componenti vengono memorizzate in un albero che consente di realizzare l’algoritmo in modo efficiente dal punto di vista computazionale.\nIn MATLAB l’algoritmo è implementato dalla funzione detectMSERFeatures. In Python un’implementazione pratica è fornita da OpenCV tramite l’API MSER (cv2.MSER_create()).\n\nIn Figura 3.40 è illustrato il risultato dell’applicazione dell’algoritmo al fantoccio prima considerato.\n\nFigura 3.40. Risultato dell’algoritmo MSER su fantoccio MR.","type":"content","url":"/c31-segmentazione#algoritmo-msers","position":37},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Component Tree","lvl2":"Watershed transform"},"type":"lvl3","url":"/c31-segmentazione#component-tree","position":38},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Component Tree","lvl2":"Watershed transform"},"content":"L’approccio Component Tree come detto in precedenza permette di memorizzate il contenuto di una immagine in un albero, permettendo di realizzare algoritmi di elaborazione dell’immagine in modo efficiente. Si tratta di un approccio molto usato e vale quindi la pena di esaminarlo in maggior dettaglio.\nConsideriamo una immagine  a quattro livelli di grigio come quella in Figura 3.41 e 3.42. L’immagine contiene 8 o 9 pattern a seconda del tipo di connessione considerata, per un kernel di connessione a 8 elementi contiamo 8 pattern distinti. Al passo uno dell’algoritmo viene selezionata la componente a minima intensità (1) c1, che va a costituire la radice dell’albero e rappresenta il background. Al passo due, alla componente uno vengono aggiunte due componenti topologicamente distinte c2 e c3, con livello di grigio 2, che diventano due rami dell’albero. Al passo tre vengono aggiunte tre componenti (c4, c5, c6), con livello di grigio 3, delle quali c6 è contigua a c3 e quindi va nel ramo corrispondente dell’albero, mentre le altre due nell’altro essendo contigue a c2. Al passo quattro l’albero si completa con le componenti c7 e c8 relative al livello gi grigio 4. In generale i livelli dell’albero saranno pari al numero di livelli di grigio presenti nell’immagine ed il numero di nodi sarà uguale al numero di pattern presenti nell’immagine.\n\nFigura 3.41. Immagine a 4 livelli di grigio.\n\nFigura 3.42. Rappresentazione ad albero dell’immagine a 4 livelli di grigio.\n\nIn ogni nodo sono memorizzati i pixel appartenenti al nodo. Il peso delle connessioni (edge) tra nodi è dato dalla differenza di livello di grigio tra i livelli (in questo caso uno per tutte le connessioni).\n\nUna volta effettuato il processo di conversione da immagine ad albero connesso, le operazioni di filtraggio e segmentazione possono essere effettuate sull’albero invece che sull’immagine. Ad esempio se selezioniamo i due rami principali dell’albero attraverso una funzione di ricerca depth-first search, otteniamo i due “macro-pattern” in cui è divisa l’immagine, come illustrato in Figura 3.43.\nUna segmentazione a soglia si ottiene immediatamente selezionando tutti i nodi sopra un certo livello. Il region-growing è equivalente a partire da un nodo e muoversi sull’albero con la condizione che il peso di una connessione deve essere minore uguale alla soglia di tolleranza dell’algoritmo. In generale molti algoritmi di filtraggio e segmentazione possono essere implementati utilizzando in modo efficace la rappresentazione ad albero connesso.\n\nFigura 3.43. Segmentazione da albero.","type":"content","url":"/c31-segmentazione#component-tree","position":39},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Skeletonization"},"type":"lvl2","url":"/c31-segmentazione#skeletonization","position":40},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Skeletonization"},"content":"(Adattato da HIPR - \n\nhttp://​homepages​.inf​.ed​.ac​.uk​/rbf​/HIPR2​/hipr​_top​.htm)\n\nNell’elaborazione di immagini biomediche, una volta definita la maschera che definisce una certa struttura anatomica, può essere utile caratterizzare la struttura da un punto di vista geometrico, estraendone i caratteri fondamentali, quali la connettività, la topologia, la lunghezza, la direzione e l’ampiezza. Questo può essere ottenuto attraverso l’estrazione del cosiddetto scheletro topologico (skeleton). Intuitivamente, lo scheletro topologico di una maschera si ottiene “assottigliando” la maschera stessa fino ad ottenerne una versione essenziale, tipicamente composta da linee.\n\nIn Figura 3.44 è mostrato lo scheletro topologico dell’albero bronchiale estratto da una maschera dell’albero stesso ottenuta da immagini TAC.\n\nFigura 3.44. Esempio di scheletro topologico applicato all’albero bronchiale.\n\nIn questo caso lo scheletro estratto può essere utile in varie applicazioni. Ad esempio lo scheletro, rappresentando la linea di equidistanza dalle pareti del bronco (center line) può essere usata come guida per applicazioni di endoscopia virtuale. Dallo scheletro è possibile individuare facilmente le biforcazioni dell’albero bronchiale, permettendo la costruzione di un albero binario che permette di confrontare il paziente in esami eseguiti a tempi diversi o pazienti diversi tra loro.\n\nL’operazione di skeletonization può essere eseguita in vari modi. I due approcci fondamentali sono il thinning (assottigliamento) morfologico e il calcolo della distance transform dell’immagine.\nL’operazione di thinning si basa sull’idea di eliminare i pixel di confine della maschera in modo progressivo (erosione) riducendone le dimensioni fino a quando non è più possibile assottigliare ulteriormente la maschera e si ottiene lo scheletro topologico.","type":"content","url":"/c31-segmentazione#skeletonization","position":41},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Hit-and-miss (HAM) transform","lvl2":"Skeletonization"},"type":"lvl3","url":"/c31-segmentazione#hit-and-miss-ham-transform","position":42},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Hit-and-miss (HAM) transform","lvl2":"Skeletonization"},"content":"Per formulare in termini algoritmici la funzione di thinning è utile introdurre la hit-and-miss transform (HAM) che, come avviene nei filtri spaziali, è basata sulla convoluzione della maschera con un kernel (structuring element). Il kernel contiene la descrizione della struttura geometrica che si vuole individuare. La struttura del kernel (supponendo di voler individuare un angolo della maschera) sarà del tipo in figura, dove 0 indica un pixel appartenente allo sfondo della maschera, 1 un pixel appartenente alla maschera e le caselle bianche indicano locazioni non di interesse. In altri termini le caselle vuote vengono ignorate ed il loro scopo è ottenere un kernel quadrato più comodo da usare a fini computazionali.\nAnalogamente alla convoluzione spaziale, il kernel viene fatto scorrere sulla maschera e se esiste una totale corrispondenza tra il kernel e la maschera sottostante il pixel della maschera risultante viene posto ad uno, altrimenti viene lasciato a zero.\nTipicamente vengono generate una serie di versioni del kernel che corrispondono alle possibili orientazioni della struttura che interessa determinare, ad esempio nel caso in oggetto potremmo avere i quattro kernel di Figura 3.45.\n\nFigura 3.45. Kernel HAM.\n\nChe corrispondono alla ricerca di un angolo nella maschera nelle quattro orientazioni possibili. L’ OR logico delle quattro maschere risultanti permette di identificare gli angoli sulla maschera (Figura 3.46).\n\nFigura 3.46. Identificazione degli angoli.\n\nUn esempio di uso della hit-and-miss transform è il rilevamento dei punti isolati di una maschera, che può essere utile nella correzione degli errori di segmentazione. In questo caso il kernel da utilizzare risulta:\\begin{bmatrix}\n0 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 0 \n\\end{bmatrix}\n\nE si ottiene una maschera che definisce i punti isolati della maschera di input.\nLa trasformazione HAM è implementata in MATLAB dalla funzione bwhitmiss.\nNel caso del thinning si può usare una coppia di kernel del tipo:\\begin{bmatrix}\n0 & 0 & 0 \\\\\n & 1 &  \\\\\n1 & 1 & 1 \n\\end{bmatrix}\\begin{bmatrix}\n & 0 & 0 \\\\\n1 & 1 & 0 \\\\\n & 1 &  \n\\end{bmatrix}\n\nL’applicazione di questi due kernel equivale a conservare i pixel che sono al centro di un ottagono che è totalmente incluso nella maschera. I kernel vengono ruotati nelle quattro direzioni possibili ottenendo quindi 8 trasformazioni hit-and-miss. L’operazione di thinning si ottiene calcolando:M_{thin} = M – M_{HAM}\n\nIn pratica il risultato della trasformazione HAM viene sottratto alla maschera originale, che quindi si ”assottiglia”. Per ottenere lo skeleton l’operazione di thinning viene iterata fino a quando la maschera originale non si modifica ulteriormente.Se consideriamo l’esempio di Figura 3.47, la maschera originale a sinistra l’applicazione dell’operazione di thinning porta alla maschera al centro, l’effetto del thinning è visibile nell’immagine a destra che rappresenta la differenza tra le due maschere.\n\nFigura 3.47. Applicazione della trasformazione HAM e differenza tra maschere.\n\nCome si osserva la parte più esterna della maschera originale viene asportata. Applicando 10 volte il thinning si ottiene il caso di Figura 3.38:\n\nFigura 3.48. Applicazione della trasformazione HAM e differenza tra maschere applicato 10 volte.\n\nEd infine per N grande (ad esempio N=100) la procedura iterativa conduce allo skeleton di Figura 3.49.\n\nFigura 3.49. Applicazione della trasformazione HAM e differenza tra maschere applicato 100 volte.\n\nIn MATLAB l’operazione di skeletonization è implementata direttamente dalla funzione\nbwmorph tramite l’opzione 'skel'. In Python, un’operazione equivalente di skeletonization è disponibile nella libreria\nscikit-image tramite la funzione skimage.morphology.skeletonize\n(per immagini binarie), oppure skimage.morphology.medial_axis, che fornisce anche\ninformazioni aggiuntive sulla distanza dal contorno.\n\nUn esempio della sua applicazione è riportato in Figura 3.50.\n\nFigura 3.50. Applicazione della funzione bwmorph con opzione ‘skel’.","type":"content","url":"/c31-segmentazione#hit-and-miss-ham-transform","position":43},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Distance transform","lvl2":"Skeletonization"},"type":"lvl3","url":"/c31-segmentazione#distance-transform","position":44},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl3":"Distance transform","lvl2":"Skeletonization"},"content":"Il calcolo della distance transform dell’immagine prevede invece che per ogni punto della maschera di input venga calcolata la distanza minima dal contorno della maschera stessa (o dallo sfondo della maschera). Di solito si utilizza la distanza City-block o `chessboard’ che vale il numero di pixel che bisogna attraversare per andare da un pixel all’altro dell’immagine (nella City-block non si possono usare le diagonali). Si ottiene quindi una immagine di dimensioni uguali all’originale e a cui a ciascun pixel corrisponde un livello di grigio pari alla minima distanza dai bordi (Figura 3.51). I pixel con valore più alto rappresentano lo skeleton in quando sono i più “centrali”. Utilizzando una soglia si può estrarre lo skeleton vero e proprio.\n\nIn MATLAB la distance transform è computata tramite la funzione bwdist, che consente\ndi definire il tipo di distanza da utilizzare. La funzione calcola la distanza a partire\ndai pixel non nulli; pertanto, per ottenere lo skeleton è necessario invertire la maschera. In Python, un’operazione equivalente è disponibile nella libreria SciPy tramite la\nfunzione scipy.ndimage.distance_transform_edt, che calcola la Euclidean Distance Transform.\nAnche in questo caso la distanza viene calcolata a partire dai pixel non nulli, per cui\nè necessario invertire la maschera binaria prima di applicare la trasformata di distanza.\n\nFigura 3.50. Esempio di distance transform.","type":"content","url":"/c31-segmentazione#distance-transform","position":45},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Bibliografia"},"type":"lvl2","url":"/c31-segmentazione#bibliografia","position":46},{"hierarchy":{"lvl1":"Capitolo 3: Segmentazione dell’immagine Biomedica","lvl2":"Bibliografia"},"content":"R Rangayyan, Biomedical Image Analysis, CRC Press 2004\n\nHoang Ngan Le T. et al. (2020) Active Contour Model in Deep Learning Era: A Revise and Review. In: Oliva D., Hinojosa S. (eds) Applications of Hybrid Metaheuristic Algorithms for Image Processing. Studies in Computational Intelligence, vol 890. Springer, Cham.\n\nhttp://​homepages​.inf​.ed​.ac​.uk​/rbf​/HIPR2​/hipr​_top​.htm","type":"content","url":"/c31-segmentazione#bibliografia","position":47},{"hierarchy":{"lvl1":"Esempio algoritmo di thinning - Codice 3.10"},"type":"lvl1","url":"/n-3-10-thinning","position":0},{"hierarchy":{"lvl1":"Esempio algoritmo di thinning - Codice 3.10"},"content":"Nel seguente codice è simulato il comportamento di un algoritmo di thinning.# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Fri Jun  7 16:23:24 2024\n\n@author: alejandro\n\"\"\"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.ndimage import binary_hit_or_miss\n\nimage = plt.imread('./data/hippo.jpg')\nimage = (1-image/255)>0.5\nimage_or= np.copy(image)\n\nplt.figure()\nplt.imshow(image, cmap=\"gray\")\nplt.show()\n\n# kernels for thinning\nk2 = np.array([[0,0,0],[-1,1,-1],[1,1,1]])\nprint(k2)\n\nk3 = np.array([[-1,0,0],[1,1,0],[-1,1,-1]])\nprint(k3)\n\n# iterations\nn = 100\n\nfig, ax = plt.subplots(1,3)\nax[0].imshow(image, cmap=\"gray\")\nax[0].axis(\"image\")\n\nplt.ion()\nplt.show(block=False)\nfor k in range(0,70):\n    # apply kernels\n    tmp = binary_hit_or_miss(image, structure1=k2)\n    image = (image.astype(np.float32) - tmp.astype(np.float32)).astype(bool)\n    tmp = binary_hit_or_miss(image, structure1=k3)\n    image = (image.astype(np.float32) - tmp.astype(np.float32)).astype(bool)\n    \n    # apply rotated kernels\n    tmp = binary_hit_or_miss(image, structure1=np.rot90(k2))\n    image = (image.astype(np.float32) - tmp.astype(np.float32)).astype(bool)\n    tmp = binary_hit_or_miss(image, structure1=np.rot90(k3))\n    image = (image.astype(np.float32) - tmp.astype(np.float32)).astype(bool)\n    \n    \n    tmp = binary_hit_or_miss(image, structure1=np.rot90(k2,2))\n    image = (image.astype(np.float32) - tmp.astype(np.float32)).astype(bool)\n    tmp = binary_hit_or_miss(image, structure1=np.rot90(k3,2))\n    image = (image.astype(np.float32) - tmp.astype(np.float32)).astype(bool)\n    \n    \n    tmp = binary_hit_or_miss(image, structure1=np.rot90(k2,3))\n    image = (image.astype(np.float32) - tmp.astype(np.float32)).astype(bool)\n    tmp = binary_hit_or_miss(image, structure1=np.rot90(k3,3))\n    image = (image.astype(np.float32) - tmp.astype(np.float32)).astype(bool)\n    \n    ax[1].cla(); ax[1].imshow(image, cmap=\"gray\"); \n    ax[1].axis(\"image\")\n    ax[2].cla(); ax[2].imshow(image_or & ~image, cmap=\"gray\"); \n    ax[2].axis(\"image\")\n\n    fig.canvas.draw()\n    fig.canvas.flush_events()\n    plt.pause(0.1)\nplt.show()","type":"content","url":"/n-3-10-thinning","position":1},{"hierarchy":{"lvl1":"Esempio di segmentazione a soglia - Notebook 3.1"},"type":"lvl1","url":"/n-3-1-seg-soglia","position":0},{"hierarchy":{"lvl1":"Esempio di segmentazione a soglia - Notebook 3.1"},"content":"Import delle librerie necessarie per la simulazione\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage.filters import threshold_otsu\nfrom sklearn.cluster import k_means\nimport pydicom\nimport skfuzzy as fuzz\n\n\n\nimport dei dati e stampa a video dei metadati del phantom\n\n# load image\nds = pydicom.dcmread('../data/phantom.dcm')\nimage = ds.pixel_array\nprint(ds)\n\n\n\n\nSegmentazione a soglia basata sull’istogramma dell’immagine\n\n# get histogram\np = np.max(image, None)\nprint(p)\ncounts, bins = np.histogram(image, bins = p)\n\n# display phantom image and image histogram\nfig, ax = plt.subplots(1,2)\nax[0].imshow(image, cmap = \"gray\")\nax[1].plot(counts[1:]/np.sum(counts, None))\nax[0].set_title('image')\nax[1].set_title('histogram')\nplt.show()\n\n# manual segmentation of water signal (dall'isto sopra 200 e sotto 600)\nmask = np.zeros((ds.Rows, ds.Columns), None)\n\nid = np.logical_and(image > 200, image<600)\nmask[id] = 255\nplt.figure()\nplt.imshow(mask, cmap = \"gray\")\nplt.title('manual')\nplt.show()\n\n\n\n\n\n\n\nSegmentazione tramite il metodo di Otsu\n\n# otsu algorithm\notsu_thresh = threshold_otsu(image)\nimage_otsu = image > otsu_thresh\nplt.figure()\nplt.imshow(image_otsu, cmap = 'gray')\nplt.title('otsu')\nplt.show()\n\n\n\nSegmentazione tramite clustering k-means\n\n#### kmeans ####\nx = image.reshape((-1,1))\ncentroid, label, inertia = k_means(x, n_clusters = 3)\nimage_classes = label.reshape(image.shape)\nplt.figure()\nplt.imshow(image_classes, cmap = 'gray')\nplt.title('kmeans')\nplt.show()\nc = np.sort(centroid, axis = 0)\nT = np.array([np.mean(c[0:2]), np.mean(c[1:3])])\nprint(\"soglie = \", T)\n\n\n\n\n\nSegmentazione tramite fuzzy c-means\n\n# x: vettore dei pixel (N,) o (N,1). Qui lo porto a (1, N) come richiesto da cmeans\nx1 = x.astype(np.float64).reshape(1, -1)\n\n# FCM\ncntr, u, _, _, _, _, _ = fuzz.cluster.cmeans(\n    x1,              # data: (features, samples)\n    c=3,             # numero cluster\n    m=2.0,           # fuzziness\n    error=1e-5,\n    maxiter=1000,\n    init=None        \n)\n\n# hard label (cluster più probabile)\nlabels = np.argmax(u, axis=0)  # (N,)\n\nmask1 = (labels == 0).reshape(image.shape)\nmask2 = (labels == 1).reshape(image.shape)\nmask3 = (labels == 2).reshape(image.shape)\n\nfig, ax = plt.subplots(1, 3, figsize=(12, 4), constrained_layout=True)\nax[0].imshow(mask1, cmap='gray'); ax[0].set_title('fcm - cluster 1')\nax[1].imshow(mask2, cmap='gray'); ax[1].set_title('fcm - cluster 2')\nax[2].imshow(mask3, cmap='gray'); ax[2].set_title('fcm - cluster 3')\nplt.show()\n\n\n","type":"content","url":"/n-3-1-seg-soglia","position":1},{"hierarchy":{"lvl1":"Esempio di segmentazione con FCM - Notebook 3.2"},"type":"lvl1","url":"/n-3-2-esempio-fcm","position":0},{"hierarchy":{"lvl1":"Esempio di segmentazione con FCM - Notebook 3.2"},"content":"Import delle librerie necessarie per la simulazione\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport skfuzzy as fuzz\n\n\n\nImmagine simulata\n\n# --- synthetic image ---\nimage = np.zeros((512, 512), dtype=np.float32)\nimage[:, :] = 20\nimage[49:99, 49:99] = 150\nimage[100:179, 100:449] = 300\nimage[199:499, 199:349] = 70\nimage[229:269, 229:269] = 750\nimage[4:399, 449:499] = 550\n\nsigma = 10\nrng = np.random.default_rng(0)  # riproducibilità\nimageN = image + sigma * rng.standard_normal(image.shape)\n\nfig, ax = plt.subplots(figsize=(5, 5), constrained_layout=True)\nax.imshow(imageN, cmap=\"gray\")\nax.set_title(\"Immagine rumorosa\")\nax.axis(\"off\")\nplt.show()\n\n\n\nSegmentazione con FCM\n\n# --- fuzzy c-means ---\n# cmeans vuole data shape = (features, samples)\nx = imageN.astype(np.float64).ravel()\ndata = x.reshape(1, -1)\n\ncntr, U, _, _, _, _, _ = fuzz.cluster.cmeans(\n    data,\n    c=6,\n    m=2.0,\n    error=1e-5,\n    maxiter=500,\n    init=None\n)\n\n# assegno a ogni pixel il cluster con membership massima (hard label)\nlabels = np.argmax(U, axis=0)  # (N,)\n\n# ordino i cluster per centroide crescente (solo per avere plot ordinati)\norder = np.argsort(cntr.ravel())\nlabels_ord = order[labels]  # rimappa 0..5 secondo l'ordine dei centroidi\n\n# --- masks + plot ---\nmasks = [(labels_ord == k).reshape(image.shape) for k in range(6)]\n\nfig, ax = plt.subplots(1, 6, figsize=(18, 3), constrained_layout=True)\nfor k in range(6):\n    ax[k].imshow(masks[k], cmap=\"gray\")\n    ax[k].set_title(f\"fcm - cluster {k+1}\")\n    ax[k].axis(\"off\")\nplt.show()\n\nprint(\"Centroidi (ordinati):\", np.sort(cntr.ravel()))\n\n\n\n","type":"content","url":"/n-3-2-esempio-fcm","position":1},{"hierarchy":{"lvl1":"Esempio di segmentazione con Gaussian Mixtures - Notebook 3.3"},"type":"lvl1","url":"/n-3-3-esempio-em-gmm","position":0},{"hierarchy":{"lvl1":"Esempio di segmentazione con Gaussian Mixtures - Notebook 3.3"},"content":"Import delle librerie necessarie per la simulazione\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import k_means\nfrom sklearn.mixture import GaussianMixture\n\n\n\nImmagine simulata\n\ndim = 512\ntissueMask = np.zeros((dim, dim), dtype = np.float32)\ntissueMask[:,:] = 1\ntissueMask[49:189,49:189] = 2\ntissueMask[199:499,199:349] = 3\n\nfig, ax = plt.subplots()\nax.imshow(tissueMask, cmap = \"gray\")\nax.set_title('image')\n\n# create 3 guassian distributions\nm1, m2, m3 = 50, 400, 200\nsd1, sd2, sd3 = 60, 40, 20\n\ndata1 = m1+sd1*np.random.randn(dim, dim) # signal distribution\ndata2 = m2+sd2*np.random.randn(dim, dim)\ndata3 = m3+sd3*np.random.randn(dim, dim)\nid1 = np.where(tissueMask == 1)\nid2 = np.where(tissueMask == 2)\nid3 = np.where(tissueMask == 3)\n\nimage = np.zeros_like(tissueMask)\nimage[id1] = data1[id1]\nimage[id2] = data2[id2]\nimage[id3] = data3[id3]\n\nfig, ax = plt.subplots()\nax.imshow(image, cmap = \"gray\")\nax.set_title(\"image\")\n\n\n\n\n\nFitting di Gaussian Mixtures tramite algoritmo EM\n\n# probability of a tissue in the image\np1 = len(id1)/(dim*dim)\np2 = len(id2)/(dim*dim)\np3 = len(id3)/(dim*dim)\n\n# calculate histogram\nfig, ax = plt.subplots(1,2)\ncounts, bins = np.histogram(image, bins = 256)\nxcounts = bins[:-1] + np.diff(bins) / 2\nax[0].plot(xcounts,counts)\n\n# apply gaussian mixtures using kmeans as initial condition\nimage = image.reshape((-1,1))\ngmm = GaussianMixture(\n    n_components = 3, \n    max_iter = 100, \n    init_params = 'kmeans', \n    verbose = 1)\ngmm.fit(image)\n\nn_samples = len(image)\n\ndataGM1 = gmm.means_[0]+np.sqrt(gmm.covariances_[0])*np.random.randn(int(gmm.weights_[0] * n_samples)).flatten()\ndataGM2 = gmm.means_[1]+np.sqrt(gmm.covariances_[1])*np.random.randn(int(gmm.weights_[1] * n_samples)).flatten()\ndataGM3 = gmm.means_[2]+np.sqrt(gmm.covariances_[2])*np.random.randn(int(gmm.weights_[2] * n_samples)).flatten()\ndataGM = np.concatenate([dataGM1.T, dataGM2.T, dataGM3.T])\n\nhGM, edgesGM = np.histogram(dataGM, bins = 256)\nxGM = edgesGM[:-1] + np.diff(edgesGM) / 2\nax[1].plot(xcounts,counts)\nax[1].plot(xGM,hGM, color='r')\n\n\n\n\n","type":"content","url":"/n-3-3-esempio-em-gmm","position":1},{"hierarchy":{"lvl1":"Esempio di clustering gerarchico - Notebook 3.4"},"type":"lvl1","url":"/n-3-4-hierarchical-clustering-example","position":0},{"hierarchy":{"lvl1":"Esempio di clustering gerarchico - Notebook 3.4"},"content":"Import delle librerie necessarie per la simulazione\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy.cluster.hierarchy import linkage, dendrogram\n\n\n\nSegmentazione tramite hierarchical clustering\n\n# create images with different noise levels\ndim = 256\nimage = np.zeros((dim, dim), dtype =np.float32)\nimage[:,:]=20\nimage[99:199,99:199] = 200\nN = 10\nx = np.zeros((N, dim, dim), dtype = np.float32)\nnoise = np.random.permutation(N)**2\n\nfig, ax = plt.subplots(1, 10)\nfor i in range(N):\n    x[i,:,:] = image + noise[i]*np.random.randn(dim,dim)\n    ax[i].imshow(x[i,:,:], cmap=\"gray\")\n    \nxd = x.reshape(N, dim*dim)\nd = pdist(xd)\nz = squareform(d)\n\n# single linkage\nt = linkage(z, method = \"single\")\nfig, ax = plt.subplots()\ndendrogram(t)\nplt.show()\n\n# average linkage\ntav = linkage(z, method = \"average\")\nfig, ax = plt.subplots()\ndendrogram(tav)\nplt.show()\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/n-3-4-hierarchical-clustering-example","position":1},{"hierarchy":{"lvl1":"Esempio labeling/region growing - Codice 3.5"},"type":"lvl1","url":"/n-3-5-esempio-labeling-region-growing","position":0},{"hierarchy":{"lvl1":"Esempio labeling/region growing - Codice 3.5"},"content":"Nel seguente codice è simulato il comportamento degli algoritmi di labeling e region growing.# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Tue May 28 11:42:56 2024\n\n@author: alejandro\n\"\"\"\n\nimport pydicom\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import k_means\nfrom skimage import measure\nfrom skimage.segmentation import flood\n\nds = pydicom.dcmread(\"./data/phantom.dcm\")\nimage = ds.pixel_array\nprint(ds)\n\n# get histogram\np = np.max(image, None)\nprint(p)\ncounts, bins = np.histogram(image, bins = p)\n\n# display phantom image and image histogram\nfig, ax = plt.subplots(1,2)\nax[0].imshow(image, cmap = \"gray\")\nax[1].plot(counts[1:]/np.sum(counts, None))\nplt.show()\n\n# kmeans\nx = image.reshape((-1,1))\ncentroid, label, inertia = k_means(x, n_clusters = 3)\nimage_classes = label.reshape(image.shape)\nidx_c = np.argsort(centroid, axis = 0)\n\n# get water mask\nmask = np.zeros_like(label)\nidx = np.where(label == idx_c[1])\nmask[idx]=255\nmask = np.reshape(mask, image.shape)\n\n# do labeling on water mask\nlabeled_image = measure.label(mask, connectivity=2) \nfig, ax = plt.subplots(1, 3)\nax[0].imshow(image, cmap = 'gray')\nax[1].imshow(mask, cmap = \"gray\")\nax[2].imshow(labeled_image)\nplt.show()\n\n# region growing\nseed = (49, 107)\ntmp = image.copy()\nfig, ax = plt.subplots(1, 4)\nax[0].imshow(image, cmap = \"gray\")\nmask = flood(tmp, seed, tolerance=50)\nax[1].imshow(mask, cmap = \"gray\")\nmask = flood(tmp, seed, tolerance=100)\nax[2].imshow(mask, cmap = \"gray\")\nmask = flood(tmp, seed, tolerance=400)\nax[3].imshow(mask, cmap = \"gray\")\nplt.show()\n\nplt.ion()\nfig, ax = plt.subplots(1, 2)\nnpoints = np.zeros([500])\nx = np.arange(0,500)\nfor T in range(0,500,5):\n    mask = flood(tmp, seed, tolerance = T)\n    ax[0].imshow(mask, cmap = \"gray\")\n    npoints[T] = np.sum(mask, axis = None)\n    ax[1].plot(x, npoints)\n    plt.pause(0.001)\n    plt.gcf().canvas.flush_events()\n    ax[1].clear()\n    \nplt.ioff()\nplt.show()","type":"content","url":"/n-3-5-esempio-labeling-region-growing","position":1},{"hierarchy":{"lvl1":"Esempio di segmentazione ai contorni - Notebook 3.6"},"type":"lvl1","url":"/n-3-6-seg-contorni","position":0},{"hierarchy":{"lvl1":"Esempio di segmentazione ai contorni - Notebook 3.6"},"content":"Import delle librerie necessarie per la simulazione\n\nimport pydicom\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom skimage import filters, measure\n\n\n\nImport dei dati e visualizzazione di dati e metadati\n\n# load image\nds = pydicom.dcmread(\"../data/phantom.dcm\")\nimage = ds.pixel_array\nprint(ds)\n\n# display phantom\ntmp = image.copy()\ntmp[127,:] = 1000\nfig, axes = plt.subplots(1,2)\naxes[0].imshow(tmp, cmap = \"gray\")\n\n# display profile and profile derivative\nprofile = image[127,:]\nderiv = np.diff(profile)\naxes[1].plot(profile)\naxes[1].plot(deriv)\nplt.show()\n\n\n\n\n\nSegmentazione ai contorni\n\n# find tissue changes\ntmp = image.copy()\nT = 100 # derivative threshold\nidx = np.argwhere(np.abs(deriv) > T) \ncont = 255*np.ones(len(profile))\nx = np.arange(0,len(profile))\ncont = cont[idx]\nx = x[idx]\ntmp[127,idx]=2000\nfig, axes = plt.subplots(1,2)\naxes[0].imshow(tmp, cmap = \"gray\")\naxes[1].plot(profile)\naxes[1].plot(np.abs(deriv))\naxes[1].scatter(x, cont, marker = \"*\", color = \"red\")\nplt.show()\n\n# working in 2d space by edge function\nfig, axes = plt.subplots()\nedge_map = filters.sobel(image)>0.005\nplt.imshow(edge_map, cmap = \"gray\")\nplt.show()\n\n# create contours from edge map\nfig, axes = plt.subplots()\nlabeled_image = measure.label(edge_map, connectivity=2) \nplt.imshow(labeled_image)\nplt.show()\n\nfig, axes = plt.subplots()\nidx = np.where(labeled_image == 1)\nmask = np.zeros(image.shape)\nmask[idx]=1\nplt.imshow(mask, cmap = \"gray\")\nplt.show()\n\n\n\n\n\n\n\n","type":"content","url":"/n-3-6-seg-contorni","position":1},{"hierarchy":{"lvl1":"Esempio di algoritmo mser - Codice 3.8"},"type":"lvl1","url":"/n-3-8-mser-example","position":0},{"hierarchy":{"lvl1":"Esempio di algoritmo mser - Codice 3.8"},"content":"Nel seguente codice è simulato il comportamento dell’algoritmo mser.# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Mon Sep 16 15:11:20 2024\n\n@author: alejandro\n\"\"\"\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport pydicom\nfrom scipy.ndimage import label\nimport time\n\n# read file\ndb = pydicom.dcmread('./data/phantom.dcm')\nimage = db.pixel_array\n\nfig, ax = plt.subplots(1, 2)\nax[0].imshow(image, cmap = 'gray')\nax[1].plot(image[np.round(db.Rows/2).astype(int),:])\n\n# simplified MSER algorithm\npixel_list= np.sort(image.reshape(-1,1))\ntmp = np.zeros_like(image)\n\nG = np.unique(pixel_list)\n\nn_labels = np.zeros_like(G)\nlabels = np.zeros((len(G), *image.shape))\nfig, ax = plt.subplots(1,2)\nfor g in range(len(G)):\n    ax[0].cla()\n    ax[1].cla()\n    idx = np.where(image == G[g])\n    tmp[idx] = 1\n    l, num = label(tmp, structure=np.ones((3, 3))) \n    labels[g] = l\n    n_labels[g] = num\n    \n    ax[0].imshow(l)\n    ax[1].plot(np.arange(len(G)), np.log(n_labels +1))\n    # drawing updated values\n    fig.canvas.draw()\n    \n    # This will run the GUI event\n    # loop until all UI events\n    # currently waiting have been processed\n    fig.canvas.flush_events()\n\n    time.sleep(0.001)\n    \n    # show the figure\n    fig.show()\n    \n# follow background area\n# fig, ax = plt.subplots()\nmaps = np.zeros((len(G), *image.shape))\nareaBK = np.zeros(len(G))\n\nfor g in range(len(G)):\n    current_lab = np.squeeze(labels[g,:,:])\n    area = np.zeros(n_labels[g])\n    for n in range(n_labels[g]):\n        idx = np.where(current_lab == (n+1))\n        area[n] = len(idx[0])\n    idmax = np.argmax(area)\n    id1 = np.where(current_lab == (idmax+1))\n    areaBK[g] = len(id1[0])\n    maps[g][idx] = 255\n    \n    # ax.imshow(np.squeeze(maps[g,:,:]))\n    # # drawing updated values\n    # fig.canvas.draw()\n    # fig.canvas.flush_events()\n    # time.sleep(0.001)\n    \n    # fig.show()\n    \nfig, ax = plt.subplots(1, 2)\nax[0].plot(areaBK)\nvariazione = 50*np.diff(areaBK)\nax[1].plot(variazione)\n\nidx = np.where(variazione == 0)\n\n\nmser = cv2.MSER_create()\nregionsc= mser.detectRegions(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY))","type":"content","url":"/n-3-8-mser-example","position":1},{"hierarchy":{"lvl1":"Esempio di component tree - Notebook 3.9"},"type":"lvl1","url":"/n-3-9-connected-tree-example","position":0},{"hierarchy":{"lvl1":"Esempio di component tree - Notebook 3.9"},"content":"Import delle librerie necessarie per la simulazione\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport networkx as nx\n\n\n\n\n# ----------------------------\n# 1) Build the 7x7 image + map\n# ----------------------------\nimage = np.ones((7, 7), dtype=int)  # background = 1\nmp = np.zeros((7, 7), dtype=int)    # pattern map\n\n# MATLAB is 1-based; Python is 0-based -> subtract 1 from indices\n\n# level 2 patterns\nimage[1:3, 3] = 2\nmp[1:3, 3] = 2\n\nimage[5, 3:6] = 2\nimage[4, 3] = 2\nmp[5, 3:6] = 3\nmp[4, 3] = 3\n\n# level 3 patterns\nimage[1:3, 1:3] = 3\nmp[1:3, 1:3] = 4\n\nimage[1:3, 4] = 3\nimage[3, 5] = 3\nmp[1:3, 4] = 5\nmp[3, 5] = 5\n\nimage[4, 1:3] = 3\nimage[5, 2] = 3\nmp[4, 1:3] = 6\nmp[5, 2] = 6\n\n# level 4 patterns\nimage[1:3, 5] = 4\nmp[1:3, 5] = 7\n\nimage[5, 1] = 4\nmp[5, 1] = 8\n\n# background pixels -> map = 1 (like MATLAB)\nmp[image == 1] = 1\n\n# visualize image + map\nfig, ax = plt.subplots(1, 2, figsize=(10, 4))\nax[0].imshow(image, cmap=\"gray\", interpolation=\"nearest\")\nax[0].set_title(\"image\")\nax[0].axis(\"image\")\n\nax[1].imshow(mp, cmap=\"jet\", interpolation=\"nearest\")\nax[1].set_title(\"map\")\nax[1].axis(\"equal\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nadattamento di component tree da codice matlab\n\n# -----------------------------------------\n# 2) Build NodeTable (names, levels, pixels)\n# -----------------------------------------\nnames = [f\"c{i}\" for i in range(1, 9)]\nlevels = [1, 2, 2, 3, 3, 3, 4, 4]\n\n# MATLAB 'find' returns linear indices (column-major).\n# We'll store BOTH:\n# - MATLAB-style linear indices (1..49, col-major)\n# - Python linear indices (0..48, row-major), if you ever need them\ndef matlab_linear_indices(mask_2d: np.ndarray) -> np.ndarray:\n    # mask_2d is boolean shape (7,7)\n    r, c = np.where(mask_2d)\n    # MATLAB linear index (1-based), column-major:\n    return (r + 1) + (c) * mask_2d.shape[0]\n\npixels_matlab = []\nfor i in range(1, 9):\n    idx = matlab_linear_indices(mp == i).tolist()\n    pixels_matlab.append(idx)\n\nnode_table = pd.DataFrame({\n    \"Name\": names,\n    \"Level\": levels,\n    \"Pixels\": pixels_matlab\n})\nprint(\"\\nNodeTable:\")\nprint(node_table)\n\n# ----------------------------\n# 3) Build directed graph (A)\n# ----------------------------\nA = np.zeros((8, 8), dtype=int)\nA[0, 1] = 1  # c1->c2\nA[0, 2] = 1  # c1->c3\nA[1, 3] = 1  # c2->c4\nA[1, 4] = 1  # c2->c5\nA[4, 6] = 1  # c5->c7\nA[2, 5] = 1  # c3->c6\nA[5, 7] = 1  # c6->c8\n\nG = nx.DiGraph()\n\n# add nodes with attributes (like NodeTable)\nfor i in range(8):\n    G.add_node(i+1, Name=names[i], Level=levels[i], Pixels=pixels_matlab[i])  # node ids 1..8\n\n# add edges from adjacency matrix\nfor i in range(8):\n    for j in range(8):\n        if A[i, j] == 1:\n            G.add_edge(i+1, j+1)\n\n# plot graph\nplt.figure(figsize=(6, 4))\npos = nx.spring_layout(G, seed=1)\nnx.draw(G, pos, with_labels=True, node_size=1200, arrows=True)\nnx.draw_networkx_labels(G, pos, labels={n: G.nodes[n][\"Name\"] for n in G.nodes})\nplt.title(\"Connected tree (digraph)\")\nplt.tight_layout()\nplt.show()\n\n# ----------------------------\n# 4) DFS order + split branches\n# ----------------------------\nv = list(nx.dfs_preorder_nodes(G, source=1))  # like dfsearch(G,1)\nlev = np.array([G.nodes[n][\"Level\"] for n in v])\n\ndiff_lev = np.diff(lev)\nprint(\"\\n[v, level, diff(level)]\")\nprint(np.column_stack([v, lev, np.r_[diff_lev, 0]]))\n\n# find where the level drops (diff < 0), like MATLAB\ndrop_idx = np.where(diff_lev < 0)[0]\nprint(\"\\nIndices where diff(level) < 0:\", drop_idx)\n\n# In your MATLAB, you used v(2:id) and v(id+1:end).\n# Here we mirror that using the first drop.\nif len(drop_idx) == 0:\n    raise RuntimeError(\"No level drop found; cannot split into two main branches.\")\nid0 = drop_idx[0]  # index in diff -> split point in v\n\n# note: MATLAB starts from v(2), skipping root\nMpattern1 = v[1:id0+1]      # inclusive up to id0\nMpattern2 = v[id0+1:]       # rest\n\nprint(\"\\nMpattern1:\", Mpattern1, [G.nodes[n][\"Name\"] for n in Mpattern1])\nprint(\"Mpattern2:\", Mpattern2, [G.nodes[n][\"Name\"] for n in Mpattern2])\n\n# collect pixel lists (MATLAB-style linear indices)\nMp1_pixels = [p for n in Mpattern1 for p in G.nodes[n][\"Pixels\"]]\nMp2_pixels = [p for n in Mpattern2 for p in G.nodes[n][\"Pixels\"]]\n\n# ---------------------------------------\n# 5) Build map1: 1 for branch1, 2 for branch2\n# ---------------------------------------\nmap1 = np.zeros((7, 7), dtype=int)\n\ndef matlab_lin_to_rc(lin: int, nrows: int) -> tuple[int, int]:\n    # lin is 1-based column-major\n    lin0 = lin - 1\n    r = lin0 % nrows\n    c = lin0 // nrows\n    return r, c\n\nfor lin in Mp1_pixels:\n    r, c = matlab_lin_to_rc(lin, 7)\n    map1[r, c] = 1\n\nfor lin in Mp2_pixels:\n    r, c = matlab_lin_to_rc(lin, 7)\n    map1[r, c] = 2\n\nplt.figure(figsize=(4, 4))\nplt.imshow(map1, cmap=\"jet\", interpolation=\"nearest\")\nplt.title(\"map1 (branch1=1, branch2=2)\")\nplt.axis(\"equal\")\nplt.tight_layout()\nplt.show()\n\n# ----------------------------\n# 6) Shortest path 2 -> 7\n# ----------------------------\nTR_nodes = nx.shortest_path(G, source=2, target=7)\nprint(\"\\nShortest path 2 -> 7:\", TR_nodes, [G.nodes[n][\"Name\"] for n in TR_nodes])\n\nTR = G.subgraph(TR_nodes).copy()\n\nplt.figure(figsize=(6, 4))\npos2 = nx.spring_layout(TR, seed=1)\nnx.draw(TR, pos2, with_labels=True, node_size=1200, arrows=True)\nnx.draw_networkx_labels(TR, pos2, labels={n: G.nodes[n][\"Name\"] for n in TR.nodes})\nplt.title(\"TR (shortest path subgraph)\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/n-3-9-connected-tree-example","position":1},{"hierarchy":{"lvl1":"Esercitazione 3: segmentazione su immagini di perfusione cardiaca"},"type":"lvl1","url":"/z-3-1-esercitazione","position":0},{"hierarchy":{"lvl1":"Esercitazione 3: segmentazione su immagini di perfusione cardiaca"},"content":"","type":"content","url":"/z-3-1-esercitazione","position":1},{"hierarchy":{"lvl1":"Esercitazione 3: segmentazione su immagini di perfusione cardiaca","lvl2":"Contesto clinico"},"type":"lvl2","url":"/z-3-1-esercitazione#contesto-clinico","position":2},{"hierarchy":{"lvl1":"Esercitazione 3: segmentazione su immagini di perfusione cardiaca","lvl2":"Contesto clinico"},"content":"L’oggetto dell’esercitazione è una serie di immagini 2D+T che descrive la diffusione del mezzo di contrasto (Gadolinio) a livello cardiaco dopo la somministrazione dello stesso (first-pass perfusion). Lo scopo di questa tecnica di imaging MR è verificare la corretta perfusione del miocardio da parte delle tre arterie coronariche. Se le coronarie non presentano stenosi, il mezzo di contrasto iniettato al paziente come bolo compatto passerà nelle tre coronarie e perfonderà il miocardio in modo omogeneo. Se una coronaria presenta una stenosi, il passaggio del mezzo di contrasto sarà rallentato e si noterà un difetto di perfusione nella regione di miocardio corrispondente alla coronaria interessata. In Figura 3.52 è riportata un’immagine di perfusione cardiaca in cui si osserva un difetto nella perfusione dei segmenti infero-settale e inferiore, corrispondenti alla coronaria destra (RCA) come definite dal modello AHA. Il modello AHA è usato come riferimento per la refertazione del miocardio, i tra anelli, dall’interno verso l’esterno, corrispondono alla regione apicale, media e basale del ventricolo sinistro. I settori ai segmenti cardiaci.\n\nFigura 3.52. Immagine di perfusione cardiaca MRI e modello AHA.\n\nNon essendo predicibile la dinamica del contrasto nel singolo paziente, si inietta un bolo di contrasto e si iniziano ad acquisire le immagini fino al passaggio completo del bolo. Lo schema di acquisizione è illustrato in Figura 3.53 (Gerber et al JCMR 2008). L’acquisizione è sincronizzata con l’ECG del paziente. Per\nogni fetta in asse corto acquisita la sequenza prevede un impulso di saturazione (PREP) ed una procedura veloce di imaging (GRE) in modo da ottenere una immagine T1 pesata che massimizzi il contrasto tra il gadolinio ed il miocardio. Nella configurazione in figura vengono acquisite sei fette, tre per ogni intervallo RR. In questo caso l’intervallo di campionamento è 2RR.\n\nFigura 3.53. Schema di acquisizione per immagini di perfusione cardiaca.\n\nNei dati dell’esercitazione si è scelto di acquisire tre fette, basale, media ed apicale, per ottenere una refertazione secondo il modello AHA, accorciando l’intervallo di campionamento a RR. Le immagini sono acquisite in fase diastolica, una per ogni battito cardiaco, quindi l’intervallo di campionamento temporale è pari alla durata di un ciclo cardiaco. Il valore del tempo di acquisizione è contenuto nel campo TriggerTime del DICOM.\n\nCome si osserva dalla Figura 3.54 (che mostra un crop dell’immagine originale centrato sul cuore) il contrasto appare nel ventricolo destro, poi nel sinistro ed infine nel miocardio. Il segnale sugli altri tessuti non viene sostanzialmente influenzato in quanto osserviamo solo il primo passaggio del contrasto. L’acquisizione nel caso di figura dura 80 cicli cardiaci, quindi 1 minuto/1 minuto e mezzo in dipendenza dalla frequenza cardiaca del paziente. L’acquisizione viene effettuata a “respiro trattenuto” per evitare i movimenti respiratori che disallineerebbero le immagini. Spesso il paziente non può trattenere il respiro per un tempo così lungo per cui in fase di elaborazione si utilizzano algoritmi per la registrazione di immagini per correggere il problema. In questo caso possiamo considerare le immagini registrate.\n\nFigura 3.54. Serie di immagini da perfusione cardiaca.\n\nEssendo le immagini di tipo 2D+T, ogni pixel dell’immagine corrisponderà ad una locazione spaziale per la quale sono disponibili N (N=80 nel nostro esempio) misure del valore di segnale. Nell’analisi quantitativa delle immagini di perfusione si procede nel modo seguente:\n\nSulla prima delle tre fette, si definisce il miocardio attraverso il suo contorno interno (endocardio) ed esterno (epicardio).\n\nFigura 3.55. Definizione del contorno del miocardio.\n\nSi propaga il contorno su tutti i frame temporali, identificando il miocardio su tutti i frame visto che tutte le immagini sono in fase diastolica (Figura 3.56). Se è presente il movimento respiratorio del paziente le immagini dovranno essere riallineate attraverso un algoritmo di registrazione.\n\nFigura 3.56. Estensione del contorno del miocardio a tutto il volume di acquisizione.\n\nSi divide il miocardio in sei segmenti (o quattro nella fetta apicale) seguendo il modello AHA (Figura 3.57).\n\nFigura 3.57. Divisione del miocardio in 6 segmenti.\n\nIl programma calcola la media del segnale su un segmento per tutti i frame temporali, ottenendo sei curve di perfusione, una per ogni segmento. Inoltre da una regione posta al centro dell’endocardio viene estratta la curva di segnale del sangue nel ventricolo sinistro.\n\nFigura 3.58. Media del segnale per i diversi frame temporali.\n\nDalle curve viene estratta la cosiddetta up-slope, cioè la massima pendenza della curva. Si ottiene facendo scorrere sulla curva una finestra ( ad esempio di 5 punti) e calcolando la pendenza della retta che fitta la curva sulla finestra (Figura 3.59).\n\nFigura 3.59. Stima della up-slope.\n\nIl massimo valore della up-slope rappresenta la massima crescita del segnale sul miocardio e caratterizza la curva di perfusione. Per diminuire la dipendenza dalla modalità di iniezione del contrasto si utilizza in realtà la up-slope normalizzata pari a 100*upslope_{miocardio}/upslope_{sangue}.\n\nSi ripete l’analisi sulle altre due fette e si ottiene il modello AHA a 16 segmenti della perfusione miocardica (Figura 3.60).\n\nFigura 3.60. Modello AHA a 16 segmenti.\n\nUn punto debole della procedura prima descritta è il tracciamento manuale del miocardio che implica una variabilità indotta dall’operatore e una perdita di tempo. Lo scopo dell’esercitazione è sviluppare una procedura automatica di segmentazione del miocardio, del ventricolo sinistro e del ventricolo destro attraverso l’algoritmo di clustering esclusivo k-means.","type":"content","url":"/z-3-1-esercitazione#contesto-clinico","position":3},{"hierarchy":{"lvl1":"Esercitazione 3: segmentazione su immagini di perfusione cardiaca","lvl2":"Esercitazione"},"type":"lvl2","url":"/z-3-1-esercitazione#esercitazione","position":4},{"hierarchy":{"lvl1":"Esercitazione 3: segmentazione su immagini di perfusione cardiaca","lvl2":"Esercitazione"},"content":"Lo scopo dell’esercitazione è segmentare una sequenza 2D+T di perfusione miocardica attraverso l’algoritmo k-means. L’algoritmo k-means può incontrare il problema dello svuotamento di un cluster, soprattutto se un cluster è “piccolo” rispetto alle dimensioni complessive dell’immagine. Per questo motivo e per velocizzare l’algoritmo può essere quindi opportuno effettuare un “crop” dell’immagine in modo da isolare la zona di interesse.\n\nCome detto in precedenza, ogni pixel dell’immagine è caratterizzato dalla propria curva intensità/tempo, come in figura (RV, blu; LV, rosso; miocardio, verde). Le altre regioni avranno un segnale costante a meno del rumore non essendo interessate dal contrasto. L’algoritmo kmeans classificherà i pixel dell’immagine in base alla “distanza” tra le curve relative ai pixel.\n\nSe si usa una distanza classica di tipo geometrico (‘sqEuclidean’ o ‘cityblock’) il valore della distanza tra due curve sarà la somma delle differenze su tutti i campioni temporali (frame). Essendo gli ultimi frame molto simili tra loro, per esaltare la differenza tra le curve può essere preferibile usare un numero di frame temporali minore di quello a disposizione, cioè definire una finestra temporale in cui effettuare la segmentazione.\n\nUna alternativa è utilizzare una distanza di tipo correlativo. In python questo è possibile andando a definire le distanze correlative tramite il comandocorrelation_distances = squareform(pdist(data, metric='correlation'))\n\ne usarlo come input alla classe Kmeans.\n\nOpzioni di interesse della classe Kmeans sono ‘n_init’ (ripete il clustering N volte e ritorna il risultato migliore). Notiamo infine che l’algoritmo k-means in generale restituisce i cluster ordinati in modo casuale, quindi sarà necessario identificare a posteriori i quattro tessuti di interesse.\n\nL’esercitazione richiede:\n\nUtilizzare l’algoritmo k-means per effettuare la segmentazione del ventricolo destro, ventricolo sinistro e miocardio (numero di cluster = 4). Si determini il numero di frames da utilizzare ed il tipo di distanza che ottimizza la segmentazione (geometrica o correlativa).\n\nPer valutare la qualità della segmentazione si utilizzi lo Jaccard index definito tra la maschera ottenuta dalla segmentazione e la maschera presa come “ground truth”:J(A,B) = \\frac{|A \\cap B|}{|A \\cup B|}\n\nLo jaccard index misura il livello di sovrapposizione percentuale tra due maschere, un valore uno indica la perfetta sovrapposizione. Le maschere di riferimento sono nel file “GroundTruth.mat”. Se è stato effettuato un cropping dell’immagine originale, lo stesso cropping dovrà essere applicato alla maschera GT. Per ottimizzare il risultato della segmentazione è possibile utilizzare un algoritmo di tipo labeling per eliminare le regioni spurie.","type":"content","url":"/z-3-1-esercitazione#esercitazione","position":5},{"hierarchy":{"lvl1":"Esercitazione 4: stima della rigidità arteriosa"},"type":"lvl1","url":"/z-3-2-esercitazione","position":0},{"hierarchy":{"lvl1":"Esercitazione 4: stima della rigidità arteriosa"},"content":"","type":"content","url":"/z-3-2-esercitazione","position":1},{"hierarchy":{"lvl1":"Esercitazione 4: stima della rigidità arteriosa","lvl2":"Contesto clinico"},"type":"lvl2","url":"/z-3-2-esercitazione#contesto-clinico","position":2},{"hierarchy":{"lvl1":"Esercitazione 4: stima della rigidità arteriosa","lvl2":"Contesto clinico"},"content":"Negli ultimi anni, è stata posta grande enfasi sul ruolo della rigidità arteriosa (stiffnes) nello sviluppo della malattia cardiovascolare (\n\nhttp://​www​.sisa​.it​/upload​/GIA​_2010​_n0​_3​.pdf). Le grandi arterie elastiche, come l’aorta e la carotide, rivestono due fondamentali funzioni fisiologiche. Da un lato consentono la conduzione del sangue dal cuore alle arterie periferiche, dall’altro permettono la trasformazione di un flusso pulsatile, generato dall’azione cardiaca, in uno continuo, proprio dei tessuti periferici. L’incremento della rigidità dei grandi vasi comporta una serie di conseguenze fisiologiche, tanto che numerosi studi clinici hanno documentato il significato prognostico indipendente della rigidità aortica nei confronti della morbilità e mortalità cardiovascolare. L’età rappresenta il principale determinante della stiffness arteriosa. Con l’età e con il ripetersi di cicli di stress, le fibre elastiche vanno incontro a fratturazione e frammentazione, con conseguente dilatazione del vaso ed irrigidimento della parete. Il progressivo irrigidimento dell’aorta comporta almeno due conseguenze emodinamiche sfavorevoli:\n\nl’aumento della velocità sia dell’onda incidente che di quella riflessa fa sì che l’onda riflessa si fonda con quella incidente più precocemente, ossia già nella prima della parte della sistole, invece che alla fine della sistole. Ciò aumenta la pressione sistolica aortica (con conseguente aumento del post-carico cardiaco) e riduce la pressione diastolica (che riduce il flusso ematico miocardico, che avviene quasi esclusivamente in diastole), favorendo l’evoluzione verso l’ischemia miocardica e l’insufficienza cardiaca;\n\nl’aumento della stiffness è molto più marcato nelle arterie elastiche (aorta e carotidi) che nelle arterie muscolari (più periferiche). Succede così che con l’età la stiffness aortica raggiunga e superi quella periferica, riducendo così o addirittura invertendo il normale gradiente di stiffness centro-periferia che è il principale responsabile della riflessione dell’onda sfigmica. Ciò fa sì che il sito di riflessione dell’onda si sposti con l’età più distalmente e che l’entità dell’onda riflessa si riduca. Tutto ciò aumenta la trasmissione in periferia di un’ampia onda incidente, che espone arterie ed arteriole periferiche a livelli dannosi di pulsatilità pressoria e può contribuire all’ampio spettro di alterazioni microvascolari che si osservano comunemente nell’anziano specialmente negli organi ad alto flusso e bassa resistenza come l’encefalo ed i reni.\n\nPer questo motivo, la valutazione della rigidità arteriosa è stata sempre più introdotta nella pratica clinica. La rigidità arteriosa locale si definisce come la pressione necessaria per ottenere una determinata dilatazione in un segmento arterioso. Questa definizione fa già intuire le limitazioni insite nella sua misurazione: per una misurazione diretta della rigidità è infatti necessario misurare contemporaneamente, in maniera accurata e nello stesso segmento arterioso, l’andamento istantaneo della pressione e del calibro. Ciò è ottenibile solo con metodiche invasive, attraverso un accesso intra-arterioso e l’uso contemporaneo di trasduttori di pressione e di sensori di flusso.\n\nÈ possibile però stimare la rigidità di un vaso o dell’intero sistema arterioso in maniera non invasiva, senza quindi conoscere i valori istantanei di pressione e di volume di un dato segmento arterioso. Sono stati proposti vari metodi che implicano diversi metodi di misurazione. La maggior parte dei metodi di misura si basa sulla misura della velocità dell’onda sfigmica o di polso (pulse wave velocity, PWV). Con l’aumento della rigidità del vaso aumenta la velocità di trasmissione dell’onda sfigmica. Questa relazione è stata teorizzata per la prima volta nel 1878 da due scienziati olandesi, Moens e Korteweg: velocità dell’onda sfigmica.PWV = \\sqrt{\\frac{Eh}{\\rho D}}\n\nnella quale E è il modulo elastico di una data arteria ovvero la sua rigidità, h lo spessore della parete arteriosa, \\rho la densità del sangue e D il diametro dell’arteria in diastole, ed è stata quindi sviluppata nel 1922 da Bramwell e dal premio Nobel Hill. Da questa relazione si evince che la PWV ha una relazione diretta quadratica con il modulo elastico, ossia con la rigidità intrinseca di un vaso arterioso. \\rho è nota o può essere misurata con un prelievo sanguigno, D e h possono essere misurati con una metodica di imaging, come nel caso dell’ecografia in figura (h=IMT+EMT) dove IMT (Intima-media thickness) è lo spessore della parte interna (indice diagnostico legato alla presenza di arteriosclerosi) e EMT (external-media thickness) è lo spessore della parete esterna. Nota la PWV è quindi possibile stimare E.\n\nFigura 3.61.  EMT e IMT.\n\nLa PWV viene valutata misurando il tempo che l’onda sfigmica impiega a percorrere una data distanza. I fattori critici in questo calcolo sono la misurazione precisa del tempo di transito e della distanza percorsa.\n\nL’onda sfigmica può essere registrata con metodiche differenti: tonometrica, piezoelettrica, Doppler, impedenziometrica. La tecnica più frequentemente usata è quella tonometrica. La tonometria ad applanazione è una tecnica semplice e riproducibile di analisi dell’onda sfigmica, che consiste nella lieve compressione (applanazione) di un’arteria superficiale contro il piano osseo sottostante per mezzo di un sensore pressorio (tonometro). La forma d’onda ottenuta con un tonometro ad alta fedeltà è virtualmente identica a quella registrata con un trasduttore intraarterioso. Qualunque sia la metodica utilizzata per ottenere il segnale, ai fini della misurazione della PWV viene registrata l’onda di polso in due punti differenti dell’albero arterioso e viene calcolato l’intervallo temporale che intercorre tra il piede dell’onda nel punto prossimale e nel punto distale (utilizzando come comune repere l’onda R dell’ECG).","type":"content","url":"/z-3-2-esercitazione#contesto-clinico","position":3},{"hierarchy":{"lvl1":"Esercitazione 4: stima della rigidità arteriosa","lvl2":"Esercitazione"},"type":"lvl2","url":"/z-3-2-esercitazione#esercitazione","position":4},{"hierarchy":{"lvl1":"Esercitazione 4: stima della rigidità arteriosa","lvl2":"Esercitazione"},"content":"Nell’esercitazione ci proponiamo di stimare i valori di D (diametro aorta in diastole) ed h (spessore della parete del vaso) da immagini ecografiche della carotide.\n\nLe immagini sono state ottenute dal sito del software CAROLAB (\n\nhttps://​www​.creatis​.insa​-lyon​.fr​/carolab/). Si tratta di immagini 2D+T acquisite durante il ciclo cardiaco.\n\nLe immagini sono fornite in formato DICOM (CAROLAB_SAMPLE_DICOM.dcm) (Figura 3.62). Si tratta di un file DICOM in formato multi-slice, quindi un file unico che contiene più frame temporali. I dati essendo del tipo 2D+T possono essere visualizzati con il viewer napari.\n\nFigura 3.62.  Frame del dataset a disposizione.\n\nCome avviene comunemente nei dati US, il file DICOM contiene sostanzialmente un salvataggio della schermata che appare sull’ecografo. Oltre all’immagine sono quindi presenti annotazioni e regioni di zero-padding. L’immagine è sostanzialmente un mosaico di regioni, che sono elencate nel campo DICOM SequenceOfUltrasoundRegions. Nel nostro caso ci sono due regioni, Item_1 e Item_2. L’immagine che ci interessa è Item_1 che può essere estratta dall’immagine globale con una operazione di crop attraverso i campi RegionLocationMin/Max. Essendo l’immagine un salvataggio dello schermo, non è presente nel DICOM il campo pixel_spacing. Per conoscere la dimensione del pixel dobbiamo utilizzare i campi all’interno di Item_1. In particolare, i campi PhysicalDeltaX e PhysicalDeltaY contengono la dimensione del pixel in cm. Trattandosi di un formato multi-slice, il tempo di acquisizione è contenuto nel campo DICOM FrameTimeVector, che è un vettore contenente l’intervallo temporale tra un frame temporale ed il precedente. Per convenzione il primo frame ha valore zero. La sequenza temporale non è sincronizzata con l’ECG e contiene 4-5 cicli cardiaci.\n\nPer la segmentazione del vaso si può utilizzare un approccio a Contorni Attivi (snakes),\nimplementato in MATLAB dalla funzione activecontour.\nLa funzione activecontour implementa due differenti approcci, quello Chan–Vese\n(opzione di default) e quello edge, che realizza il classico approccio di tipo Snake.\nL’approccio Chan–Vese rappresenta una versione ottimizzata dal punto di vista del tempo\ndi convergenza e dell’accuratezza. In Python, approcci equivalenti sono disponibili nella libreria scikit-image.\nIn particolare, l’algoritmo di Chan–Vese è implementato tramite la funzione\nskimage.segmentation.chan_vese (e la relativa variante\nskimage.segmentation.morphological_chan_vese), mentre l’approccio basato sui bordi\n(edge-based snake) è implementato tramite la funzione\nskimage.segmentation.active_contour.\n\nLa segmentazione a contorni utilizza contorni chiusi, mentre la carotide nella visualizzazione a disposizione è “aperta” sui due lati confinando con due regioni di zero-padding. Essendo noto il valore di tali regioni è possibile imporre in esse un valore arbitrario per ottimizzare la segmentazione.\n\nI parametri della di una funziona a contorni attivi sono generalmente:\n\nL’immagine da segmentare A\n\nIl contorno di partenza (seed), che viene definito come la maschera interna al contorno. Le due rappresentazioni sono infatti equivalenti.\n\nIl numero n di iterazioni dello snake.\n\nL’uscita della funzione è la maschera del contorno risultante che definirà l’area della carotide in pixel. L’area reale andrà calcolata tenendo conto della dimensione del pixel stesso estratta dai campi DICOM.\n\nI parametri che definiscono il comportamento dello snake sono:\n\nContractionBias (\\alpha), che definisce la tendenza dello snake ad espandersi o contrarsi. In questa implementazione, valori negativi favoriscono l’espansione dello snake, mentre valori positivi ne favoriscono la contrazione.\n\nSmoothFactor (\\beta), che definisce la regolarità dello snake; valori più elevati producono contorni più lisci e regolari.\n\nIn Python, nel caso dell’algoritmo di Chan–Vese implementato dalla funzione\nskimage.segmentation.morphological_chan_vese, i parametri lambda1 e lambda2\ncontrollano il peso dei termini di omogeneità delle regioni interna ed esterna al contorno.\nSebbene tali parametri non siano direttamente equivalenti a ContractionBias e\nSmoothFactor, essi consentono di ottenere comportamenti qualitativamente simili\nin termini di evoluzione e regolarità della segmentazione.\n\nDovendo elaborare una serie di immagini, per ogni immagine bisognerà identificare un seed. Per minimizzare l’interazione con l’utente è opportuno definire il seed su un singolo frame temporale ed utilizzare lo stesso seed su tutte le fasi, oppure utilizzare come seed di una fase il contorno calcolato alla fase precedente. È anche possibile adattare i parametri dello snake in base alla fase da segmentare.\n\nLa ROI iniziale può essere definita in MATLAB con funzioni dedicate come getpts, seguita da\npoly2mask per trasformare la ROI in una maschera, oppure mediante le funzioni più evolute\ndel toolbox ROI-Based Processing. In Python, una procedura analoga può essere realizzata definendo interattivamente i punti\ndella ROI tramite strumenti di visualizzazione. In particolare, la funzione\nplt.ginput di matplotlib consente di selezionare manualmente con il mouse i vertici\ndi una ROI poligonale, che può poi essere convertita in una maschera binaria tramite la\nfunzione skimage.draw.polygon2mask della libreria scikit-image. In alternativa,\nlibrerie come napari offrono strumenti più avanzati per la definizione interattiva\ndelle ROI.\n\nPer controllare la bontà della segmentazione, in MATLAB si può utilizzare la funzione\nimshowpair, che consente di visualizzare la maschera sovrapposta all’immagine a falsi colori\n(dopo normalizzazione a 8 bit). In Python, una visualizzazione equivalente può essere ottenuta\nsovrapponendo la maschera all’immagine tramite matplotlib, ad esempio utilizzando\nax.imshow per l’immagine e ax.contour per visualizzare il contorno della maschera.\nQuesta modalità riproduce il comportamento della funzione visboundaries di MATLAB.\n\nUna volta noto il contorno del lume della carotide è possibile stimare il diametro della stessa modellando il vaso (o un tratto del vaso) come un cilindro che abbia come sezione longitudinale l’area definita dal contorno. Comprendendo l’acquisizione più cicli cardiaci, per avere la variazione di diametro durante il ciclo sarà necessario isolare i cicli cardiaci completi presenti nell’acquisizione e computare la variazione del diametro per i detti cicli e quindi il valore minimo del diametro che è relativo alla fase diastolica.\n\nPer ottenere lo spessore del vaso si possono utilizzare come seed i contorni del lume ed implementare una nuovo algoritmo a contorni che trovi la parete, ottenendo lo spessore come differenza tra le due serie di contorni. La parete è sostanzialmente incomprimibile, quindi il suo spessore dovrebbe risultare costante lungo il ciclo.\n\nIn definitiva il programma da sviluppare deve implementare i seguenti passi:\n\nCaricare l’immagine DICOM multi-frame estraendo la sotto-immagine di interesse ed i dati di risoluzione geometrica e temporale.\n\nPermettere la definizione da parte dell’utente di un seed, cioè il contorno di partenza, solo sul primo frame.\n\nAttraverso la un approccio di segmentazione ai contorni calcolare la maschera del lume della carotide e la sua area in mm^2.\n\nRipetere la procedura 3 per tutti i frame temporali.\n\nStimare dall’area il diametro D del lume della carotide e graficare il diametro rispetto al tempo del ciclo in ms. Ci si attende che il diametro della carotide vari in modo congruente al ciclo cardiaco ripetendo lo stesso andamento su tutti i cicli compresi nell’acquisizione. Calcolare quindi il diametro della carotide in fase diastolica (diametro minimo).\n\nRipetere la procedura per lo spessore di parete stimando lo spessore come media durante il ciclo.\n\nPer i più volenterosi stimare la frequenza cardiaca del paziente.","type":"content","url":"/z-3-2-esercitazione#esercitazione","position":5},{"hierarchy":{"lvl1":"Esercitazione 5: segmentazione del grasso sottocutaneo e viscerale"},"type":"lvl1","url":"/z-3-3-esercitazione","position":0},{"hierarchy":{"lvl1":"Esercitazione 5: segmentazione del grasso sottocutaneo e viscerale"},"content":"","type":"content","url":"/z-3-3-esercitazione","position":1},{"hierarchy":{"lvl1":"Esercitazione 5: segmentazione del grasso sottocutaneo e viscerale","lvl2":"Contesto clinico"},"type":"lvl2","url":"/z-3-3-esercitazione#contesto-clinico","position":2},{"hierarchy":{"lvl1":"Esercitazione 5: segmentazione del grasso sottocutaneo e viscerale","lvl2":"Contesto clinico"},"content":"Obesità e sovrappeso sono universalmente riconosciuti come fattori di rischio per le principali malattie croniche: malattie cardiovascolari, ictus, diabete di tipo 2, alcuni tumori (endometriale, colon-rettale, renale, della colecisti e della mammella in post-menopausa), malattie della colecisti, osteoartriti. Altri problemi associati a un eccesso di grasso corporeo sono: ipertensione, ipercolesterolemia, apnea notturna e problemi respiratori, asma, aumento del rischio chirurgico, complicanze in gravidanza, irsutismo e irregolarità mestruali. L’obesità è il principale fattore di rischio nei paesi occidentali, la più importante causa indiretta di morte e di invalidità non accidentale. L’insieme dei disturbi associati all’obesità o al sovrappeso prende in nome di sindrome metabolica.\nL’eccesso di tessuto adiposo in particolari sedi anatomiche rappresenta un indice di rischio rilevante per diverse patologie. In particolare, l’accumulo di grasso a livello intra-addominale è correlato con un rischio elevato di sviluppare malattie cardiovascolari e disturbi metabolici. È quindi importante sviluppare tecniche di imaging che siano in grado di misurare con precisione la massa grassa e di valutarne in modo quantitativo la distribuzione.\nIl tessuto adiposo può essere individuato e misurato con varie tecniche, che possono essere suddivise in tecniche dirette e tecniche indirette. Le tecniche indirette di più comune utilizzo sono:\n\nl’indice di massa corporea (BMI) che si ottiene dividendo il peso corporeo per l’altezza al quadrato e fornisce indicazioni sulla presenza di eventuali alterazioni del peso corporeo, dovute tipicamente ad un eccesso di grasso. L’OMS fornisce una tabella (per dire la verità abbastanza conservativa) che consente sulla base del BMI di valutare la presenza di uno stato patologico associato al peso corporeo. Il computo del BMI richiede l’uso solo di una bilancia e di un metro lineare.\n\nla misura della circonferenza della vita e il rapporto tra la circonferenza della vita e quella dei fianchi (specifica per valutare la quantità di grasso addominale).\n\nLa plicometria, che stima la percentuale di grasso corporeo derivandola dallo spessore delle pliche cutanee a livello del bicipite, del tricipite, della zona sottoscapolare e soprailiaca; la plicometria richiede l’uso di un apposito strumento detto plicometro.\n\nl’analisi dell’impedenza bioelettrica attraverso bilance impedenziometriche, ormai disponibili anche a costi molto contenuti per quanto la precisione della misura in strumenti economici può essere limitata. La bioimpedenziometria si basa sostanzialmente sulla misura della resistenza elettrica del corpo, che viene ridotta dalla presenza di grasso. Le tecniche bioimpedenziometriche includono evidentemente un modello semplificato del corpo umano che rende la misura intrinsecamente imprecisa.\n\nLe tecniche indirette risultano adeguate per la maggior parte delle esigenze cliniche, tuttavia non consentono di verificare la localizzazione dei depositi di grasso. Per raggiungere questo obiettivo sono necessarie tecniche dirette che sono tipicamente tecniche di imaging. Le principali tecniche dirette sono:\n\nLa DEXA (Dual Energy X-ray Absorbitiometry) è una modalità di imaging che permette la valutazione del contenuto minerale osseo (BMC), della massa di tessuto magro (LTM) e della massa di tessuto adiposo (FM). La tecnica è basata sul principio fisico secondo il quale raggi X a differenti energie sono attenuati in modo diverso quando attraversano il corpo umano. Ha un ruolo dominante in diagnosi di osteopenia e osteoporosi ed è usata in un largo spettro di studi e ricerche mediche, anche grazie al limitato rischio radiologico legato alle basse energie utilizzate. La DEXA è una modalità di tipo radiologico non tomografico (proiettiva), e quindi non permette di valutare la distribuzione volumetrica dei depositi di grasso.\n\nLa TAC o CT (Computed Tomography) è una modalità di imaging molto veloce con un’accuratissima discriminazione tra i vari tessuti, che in base alla loro natura, sono individuati da un numero di Hounsfield. Tuttavia l’esposizione a radiazioni ionizzanti rende questa tecnica poco utilizzabile in studi a lungo termine, studi su bambini e adolescenti. Nella CT il grasso avendo un assorbimento inferiore appare più scuro del tessuto muscolare.\n\nLa Risonanza Magnetica è il metodo di elezione per la misura diretta della distribuzione del grasso corporeo, soprattutto per il fatto di non presentare un rischio radiologico. Il grasso e l’acqua si diversificano nelle immagini di risonanza, perché, pur contenendo entrambi molti nuclei di idrogeno, acqua e grasso hanno tempi di rilassamento abbastanza diversi. In particolare, il grasso ha un T1 più lungo rispetto a quello degli altri tessuti e dei liquidi corporei e produce quindi un segnale di alta intensità rispetto agli altri tessuti utilizzando un tempo di eco elevato. Le immagini MR per la valutazione del grasso sono quindi tipicamente pesate T1. Sono poi disponibili sequenze MR specializzate (DIXON, IDEAL) basate sul fenomeno del chemical shift che permettono di separare il grasso dagli altri tessuti.\n\nUna immagine addominale MR T1w è del tipo in Figura 3.63. Sono evidenziati il Tessuto Adiposo Sottocutaneo (SAT) e il Tessuto Adiposo Viscerale (VAT). Poiché il metabolismo dei due depositi differisce è importante la loro quantizzazione separata.\n\nFigura 3.63. Immagine addominale MR T1w con evidenziati il tessuto adiposo sottocutaneo (SAT) ed il tessuto adiposo viscerale (VAT).\n\nTipicamente viene acquisito un volume 3D composto da una sequenza di fette assiali che coprono tutto l’addome del paziente. Le immagini acquisite vengono elaborate in modo da identificare il volume complessivo di SAT e VAT ed il loro rapporto rispetto al volume addominale complessivo.\nIl problema della segmentazione del grasso addominale costituisce un esempio tipico dei problemi che si incontrano nello sviluppo di algoritmi utili al supporto della pratica clinica. Infatti, abbiamo due tessuti (SAT e VAT) che hanno lo stesso livello di segnale ma sono topologicamente distinti. La struttura dei due tessuti è profondamente diversa, in quanto il SAT è un tessuto compatto con una forma ben definita (approssimabile ad un anello o una struttura toroidale in 3D) mentre il VAT è una struttura ramificata non necessariamente connessa. SAT e VAT possono essere connessi tra loro, almeno al livello di risoluzione delle immagini MR. Esistono vari metodi in uso nella pratica clinica e scientifica (Bliton 2017). La procedura di segmentazione classica può essere schematizzata con la flow-chart seguente (Positano V, JMRI 2004) (Figura 3.64).\n\nFigura 3.64. Flowchart per la segmentazione di SAT e VAT.\n\nIl primo passo nella segmentazione è separare i tre componenti principali dell’immagine, cioè lo sfondo (aria), il grasso e gli altri tessuti che presentano il tipico segnale dell’acqua. In dipendenza dal FOV usato nel campo di vista possono comparire le braccia del paziente che devono essere eliminate dall’analisi. Questo passo può essere effettuato attraverso un algoritmo di clustering k-means o FCM. Imponendo K=3 (numero di cluster) si ottengono le mappe di distribuzione dei tessuti in figura, dove (a) è l’immagine originale, (b) la mappa dell’aria, (c) la mappa dell’acqua e (d) la mappa del grasso.\n\nPer eliminare la regione delle braccia di può applicare un algoritmo di labeling alla maschera dello sfondo, dopo averla binarizzata con un filtro a soglia se ottenuta con l’algoritmo FCM che produce mappe di appartenenza non binarie ed invertita. Con un algoritmo di labeling si estrae la parte del tronco che è quella di maggiori dimensioni. Otteniamo così la maschera della regione addominale.\n\nEliminate le braccia, si procede quindi alla segmentazione del grasso subcutaneo (SAT). Si definisce un contorno circolare che sia esterno al tronco e si utilizza la mappa dell’aria invertita o quella del grasso come campo di forze esterne. Il contorno settato con parametri opportuni verrà attirato sul bordo esterno del SAT. A questo punto si crea un nuovo contorno, duplicando il contorno ottenuto nel passo precedente eventualmente rimpicciolendolo per facilitare la convergenza. In questo secondo passaggio si utilizza come mappa delle forze esterne la mappa del grasso. Il contorno convergerà al bordo interno del SAT. Nel caso di utilizzo di un algoritmo di tipo Snake, dovrà essere usato un parametro alfa alto per favorire la convergenza all’interno e un parametro beta alto per evitare che lo snake debordi nel VAT. In questo modo è facile ottenere la misura del grasso subcutaneo (SAT). E’ sufficiente calcolare il numero di pixel contenuti tra il contorno esterno e interno del SAT (tipicamente sottraendo le due maschere) e moltiplicarlo per il volume di un voxel ottenuto dai campi DICOM.\n\nA questo punto sottraendo la maschera del SAT dalla maschera del grasso addominale potremmo ottenere la maschera del VAT e quindi la misura del VAT stesso. Tuttavia, questa soluzione non è opportuna a causa dell’effetto volume parziale, che è molto rilevante nel VAT a causa della sua struttura complessa e molto variabile da paziente a paziente a causa della struttura paziente-specifica del VAT. Inoltre all’interno della maschera del VAT possono comparire strutture con segnale molto simile ma non assegnabili al VAT. E quindi opportuno utilizzare un algoritmo che comprenda un modello del segnale atteso dal VAT. Si estraggono quindi i pixel all’interno del secondo contorno definito nella valutazione del SAT e si costruisce l’istogramma (Figura 3.65).\n\nFigura 3.65. Istogramma per la stima del VAT.\n\nL’istogramma avrà due picchi, uno a sinistra che rappresenta il segnale dell’acqua e uno a destra che rappresenta il segnale del grasso che è quello che ci interessa. L’istogramma viene fittato con due gaussiane (in Figura 3.65 è visualizzata solo quella del grasso) attraverso un algoritmo EM-GMM e il numero di pixel del VAT viene valutato come area della seconda gaussiana.\n\nNel caso di immagini CT la procedura è analoga, con la differenza che essendo in CT il segnale del grasso inferiore a quello dell’acqua il picco da individuare nell’analisi del VAT sarà quello di sinistra.","type":"content","url":"/z-3-3-esercitazione#contesto-clinico","position":3},{"hierarchy":{"lvl1":"Esercitazione 5: segmentazione del grasso sottocutaneo e viscerale","lvl2":"Esercitazione"},"type":"lvl2","url":"/z-3-3-esercitazione#esercitazione","position":4},{"hierarchy":{"lvl1":"Esercitazione 5: segmentazione del grasso sottocutaneo e viscerale","lvl2":"Esercitazione"},"content":"I dati dell’esercitazione sono rappresentati da una fetta assiale CT che descrive l’addome di un paziente affetto da obesità (Figura 3.66). I risultati attesi dell’elaborazione sono la superfice del SAT, la superfice del VAT, il loro rapporto percentuale 100*VAT/SAT che è un indicatore del rischio clinico e la percentuale totale di grasso P=100*(VAT+SAT)/VOL dove VOL è la superfice totale dell’addome. l’analisi di una singola fetta invece che il volume addominale è accettabile perché esiste una forte correlazione tra la misura effettuata su una singola slice a livello ombelicale ed un’analisi volumetrica, anche se sarebbe sempre preferibile effettuare un’analisi in 3D.\n\nFigura 3.66. Fetta assiale CT.\n\nNell’immagine CT si riconosce il lettino CT, che va evidentemente escluso dall’analisi, e la regione della colonna con un segnale alto rispetto al grasso ed al tessuto addominale (segnale dell’acqua).\n\nLe immagini CT sono tipicamente codificate come valori di Hounsfield (HU). Nelle immagini CT il segnale del grasso ha valore H intorno a -100 mentre i segnale dell’acqua è circa 50. Il fondo ha il segnale tipico dell’aria -1000 mentre le regioni di zero padding assumono il valore convenzionale HU=-3024.\n\nAnche se sarebbe possibile memorizzare valori negativi nel DICOM utilizzando un formato “signed 16-bit integer, tipicamente per le immagini CT il valore del livello di grigio è memorizzato come “unsigned 16 bit integer” (interi positivi) e vengono utilizzati due campi del DICOM per codificare i valori HU. Tali campi sono:\n\ntag\n\ndescription\n\n0028,1052\n\nRescale Intercept\n\n0028,1053\n\nRescale Slope\n\nSe questi campi sono presenti, una libreria DICOM correttamente configurata leggerà i valori interi positivi dell’immagine I e calcolerà i valori corretti Ic come:Ic = RescaleIntercept + RescaleSlope*I\n\nIn pratica viene definita una funzione lineare che implementa una hash table che converte valori interi positivi a 16 bit (quelli del DICOM) in un qualsiasi range di uscita. Il campo RescaleType se presente ci fornisce il tipo di rescaling utilizzato, se assente si presume che venga applicata la scala HU.\n\nLa funzione dicomread di MATLAB ignora erroneamente questi campi (almeno fino alla versione\n2021b); pertanto, è preliminarmente necessario implementare la conversione corretta\nutilizzando i campi indicati in precedenza, in modo da ottenere un array contenente i valori\nHU corretti. È inoltre opportuno convertire il tipo di dato da int16 a float (double)\nper le elaborazioni successive. In Python, la lettura dei file DICOM può essere effettuata con la libreria pydicom.\nLa conversione in unità Hounsfield (HU) può essere ottenuta utilizzando i campi\nRescaleSlope e RescaleIntercept, e successivamente convertendo l’array in float64\nper le elaborazioni successive.\n\nAnalogamente a quanto visto per immagini MR, la procedura di analisi delle immagini CT dovrà tipicamente includere i seguenti passi:\n\nImport dell’immagine CT e conversione in valori HU. Essendo presente una regione di zero padding (la regione non ricostruita, valore -3024) a valore noto, è opportuno escluderla per evitare di dover inserire un altro cluster nell’analisi FCM. Ad esempio possiamo porre i pixel di zero padding uguali al valore dell’aria (-1000) o escluderli dal clustering\n\nApplicazione dell’algoritmo FCM all’immagine per identificare i pixel appartenenti all’aria, acqua e grasso (3 cluster). Ci aspettiamo un risultato come quello di Figura 3.67, in cui i pixel della colonna hanno un valore di appartenenza “ibrido”.\n\nFigura 3.67. Risultato atteso da algoritmo FCM.\n\nEscludere dall’immagine il lettino CT, analogamente a quanto previsto per le braccia in MR, con un algoritmo di labeling che operi sull’inverso della mappa dell’aria.\n\nDefinire automaticamente un contorno seed esterno all’addome (ad esempio un cerchio iscritto nell’immagine). Con un algoritmo a contorni attivi segmentare il bordo esterno del SAT utilizzando la mappa del grasso.\n\nPartendo dal contorno esterno del SAT usato come seed, con un algoritmo a contorni attivi segmentare il bordo interno del SAT. Come guida per l’algoritmo si può usare la mappa del grasso e quella dell’acqua. Potrebbe essere utile “rimpicciolire” il seed iniziale con imerode o funzioni simili (ad esempio un filtro di thinning). Dovremmo ottenere dei contorni simili a queli di Figura 3.68.\n\nFigura 3.68. Risultato atteso contorni attivi.\n\nCalcolare la superfice del SAT in cm^2 come sottrazione del maschere del contorno SAT esterno e interno.\n\nCalcolare il volume del VAT applicando un algoritmo GMM alla porzione di immagine interna al contorno interno del SAT. All’interno della regione avremo due picchi principali di segnale (grasso e acqua) ed un picco secondario (tessuto osseo della colonna). E’ da valutare se per la convergenza del GMM sia opportuno usare due gaussiane o tre.\n\nCalcolare il volume del VAT in cm^2 dai parametri del GMM e quindi il rapporto VAT/SAT e grasso/addome. I risultati di massima sono:\n\nSAT circa 290 cm^2, VAT circa 140 cm^2, VAT/SAT circa 50\\%, grasso/addome circa 60\\%.","type":"content","url":"/z-3-3-esercitazione#esercitazione","position":5},{"hierarchy":{"lvl1":"Esercitazione 6: segmentazione dell’albero bronchiale"},"type":"lvl1","url":"/z-3-4-esercitazione","position":0},{"hierarchy":{"lvl1":"Esercitazione 6: segmentazione dell’albero bronchiale"},"content":"","type":"content","url":"/z-3-4-esercitazione","position":1},{"hierarchy":{"lvl1":"Esercitazione 6: segmentazione dell’albero bronchiale","lvl2":"Contesto clinico"},"type":"lvl2","url":"/z-3-4-esercitazione#contesto-clinico","position":2},{"hierarchy":{"lvl1":"Esercitazione 6: segmentazione dell’albero bronchiale","lvl2":"Contesto clinico"},"content":"L’analisi dell’albero polmonare è importante in pazienti con insufficienza respiratoria, nei quali per varie patologie si può avere un restringimento del lume bronchiale o una stenosi dei tratti terminali dell’albero. In un soggetto normale le dimensioni delle varie componenti dell’albero sono quelle in Figura 3.69.\n\nFigura 3.69. Dimensione delle componenti dell’albero bronchiale per un soggetto sano.\n\nAttraverso immagini CT ad alta risoluzione è possibile misurare nel singolo paziente la struttura e le dimensioni dell’albero e verificare la presenza di patologie (Figura 3.70). A questo scopo è necessario ricavare la struttura dell’albero per verificare la lunghezza dei vari tratti (seconda colonna della tabella) e misurare il diametro dei vari tratti (prima colonna della tabella).\n\nFigura 3.70. Esempio di segmentazione dell’albero bronchiale.\n\nUna demo di un software di questo tipo è visibile a \n\nhttps://​www​.youtube​.com​/watch​?v​=​y0CB6Ko​_QF4.\nQuesto tipo di software si basa su operazioni di segmentazione dell’albero bronchiale e di estrazione della struttura dell’albero attraverso algoritmi di skeletonization.","type":"content","url":"/z-3-4-esercitazione#contesto-clinico","position":3},{"hierarchy":{"lvl1":"Esercitazione 6: segmentazione dell’albero bronchiale","lvl2":"Esercitazione"},"type":"lvl2","url":"/z-3-4-esercitazione#esercitazione","position":4},{"hierarchy":{"lvl1":"Esercitazione 6: segmentazione dell’albero bronchiale","lvl2":"Esercitazione"},"content":"L’esercitazione prevede lo sviluppo di una versione semplificata del software.\nIn particolare vogliamo ottenere due obiettivi:\n\nEstrarre dall’immagine l’albero bronchiale (quindi il lume dei bronchi)\n\nMisurare il diametro dell’albero scendendo lungo i bronchi.\n\nNaturalmente faremo delle ipotesi semplificative rispetto alle tecniche effettivamente utilizzate nei software clinici. In particolare le immagini fornite coprono solo la prima biforcazione.\n\nLa flow chart del software sarà costituita da una serie di blocchi come quelli riportati in Figura 3.71.\n\nFigura 3.71. Esempio di flowchart per la segmentazione dell’albero bronchiale.\n\nIl set di dati utilizzato proviene dal Cancer Imaging Archive (\n\nhttps://​www​.cancerimagingarchive​.net/), una repository di dati biomedici anonimizzati che possono essere utilizzati liberamente a scopi di ricerca. I dati sono “labellati” da operatori esperti e quindi possono anche costituire una base di conoscenza per la validazione di algoritmi di analisi dell’immagine. Nel nostro caso utilizziamo le immagini di un paziente dello studio LIDC-IDRI (\n\nhttps://​wiki​.cancerimagingarchive​.net​/pages​/viewpage​.action​?pageId​=​1966254) dedicato all’identificazioni dei noduli polmonari. Nel nostro caso invece l’obiettivo è studiare l’albero bronchiale del paziente.\nIl set di dati è una immagine 3D del torace composta da 148 slice ognuna memorizzata in un file DICOM. Rispetto al volume originale abbiamo la sezione che comprende solo la trachea e i bronchi. I dati DICOM sono memorizzati come numeri di Hounsfield, per cui per leggere correttamente i dati bisogna fare riferimento ai campi RescaleSlope ed RescaleIntercept che la funzione dicomread non utilizza correttamente.\nIl volume, una volta caricato in un array, può essere visualizzato in MATLAB tramite la\nfunzione sliceViewer. In Python, una visualizzazione equivalente di un volume può essere effettuata utilizzando\nviewer interattivi come napari (visualizzazione slice-by-slice lungo i diversi assi), oppure tramite strumenti di plotting come matplotlib per visualizzare singole slice.\n\nScorrendo le slides si individua nella prima la trachea (Figura 3.72), che esplorando il volume si suddivide nei due rami bronchiali principali. Il segnale all’interno dell’albero bronchiale è quello dell’aria quindi intorno al valore H = -1000.\n\nFigura 3.72. Trachea e rami bronchiali principale.\n\nIl volume ottenuto caricando i dati DICOM in generale non sarà isotropo. E quindi necessario verificare se il volume è isotropo (voxel cubici) ed in caso contrario interpolarlo in modo da ottenere dei voxel cubici. L’interpolazione si effettua mantenendo costante il FOV dell’immagine (dimensione voxel x numero voxel nelle tre dimensioni). Tipicamente si pone la dimensione del voxel del volume interpolato pari alla dimensione del pixel (massima risoluzione).\n\nVolendo segmentare il lume dei bronchi, è ragionevole utilizzare un algoritmo di tipo region growing che parta da un seed da definire al livello della trachea. MATLAB non implementa l’algoritmo di region growing in 3D, per cui è necessario\nimplementare questa funzione. Come esempio si fornisce la funzione regionGrowing3D.\nLa funzione prende in ingresso l’immagine 3D, la tolleranza dell’algoritmo e il seed e\nrestituisce la maschera dell’oggetto segmentato. In Python, un’implementazione analoga del region growing 3D può essere realizzata\nutilizzando funzioni della libreria scikit-image, ad esempio skimage.segmentation.flood,\nche supporta dati multidimensionali. In questo caso, la funzione riceve in ingresso il\nvolume 3D, la tolleranza (criterio di similarità) e le coordinate del seed, e restituisce\nla maschera binaria dell’oggetto segmentato.\n\nCome si osserva la funzione utilizza come riferimento per il valore di segnale la media del segnale sulla segmentazione effettuata fino ad un certo punto del ciclo. L’intorno può essere definito come un cubo (26 connessioni) o una sfera (6 connessioni). È naturalmente possibile modificare la funzione per ottimizzare le prestazioni. La funzione è lenta (qualche minuto) quindi una volta effettuata la segmentazione è opportuno salvare la maschera in modo da poterla caricare per ottimizzare le operazioni seguenti. Il risultato dell’algoritmo di region growing sarà una maschera del tipo in Figura 3.73.\n\nFigura 3.73. Risultato dell’algoritmo region growing sull’albero bronchiale.\n\nLa maschera ottenuta deve essere raffinata attraverso l’uso di filtri che eliminino eventuali “buchi” dovuti alla mancata inclusione di alcuni voxel. Infatti, pixel di background spuri all’interno della maschera farebbero fallire l’algoritmo di skeletonization. La funzione bwmorph3 di MATLAB implementa alcuni algoritmi di filtraggio morfologico,\ntra cui le operazioni 'fill' e 'majority', particolarmente adatte allo scopo.\nIn alternativa, è possibile eseguire un algoritmo di labeling sul background e\nidentificare i “buchi” nella maschera come tutti i blob eccetto quello di dimensione\nmaggiore. In Python, funzionalità analoghe possono essere ottenute utilizzando la libreria SciPy\ne scikit-image. In particolare, un filtraggio di tipo majority può essere implementato\ntramite la funzione scipy.ndimage.generic_filter, definendo opportunamente una funzione\ndi majority voting. Nella soluzione adottata, il filtraggio è stato realizzato utilizzando\ngeneric_filter con function=majority_filter. Dovedef majority_filter(window):\n    return 1 if np.sum(window) > (len(window) / 2) else 0\n\nOttenuta la maschera dell’albero bronchiale, la centerline può essere estratta tramite\nun algoritmo di skeletonization. In MATLAB tale operazione è implementata dalla funzione\nbwskel. Per ottenere uno scheletro più “pulito” è disponibile l’opzione\nMinBranchLength, che consente di definire la lunghezza minima dei rami dello scheletro.\nLo scheletro risultante è simile a quello mostrato in figura e la funzione restituisce\nuna maschera binaria dei voxel appartenenti allo scheletro. In Python, un’operazione equivalente di skeletonization 3D può essere effettuata\nutilizzando funzioni della libreria scikit-image, come skimage.morphology.skeletonize\n(per dati binari) oppure skimage.morphology.medial_axis, che restituisce anche informazioni\nsulla distanza dal contorno. La rimozione di rami corti (analoga a MinBranchLength)\npuò essere implementata tramite operazioni di labeling e filtraggio basato sulla lunghezza\ndei rami dello scheletro.\n\nUna volta ottenuto lo scheletro, in MATLAB la funzione bwmorph3 consente di eseguire\nulteriori elaborazioni. Le opzioni 'clean', 'fill' e 'remove' permettono di\nraffinare la forma dello scheletro, mentre l’opzione 'endpoints' restituisce i punti\nterminali dello scheletro. In Python, funzionalità analoghe possono essere realizzate\ncombinando operazioni di morfologia e labeling in SciPy e scikit-image; in\nparticolare, i punti terminali dello scheletro possono essere individuati analizzando\nla connettività locale dei voxel scheletrici e utilizzati per identificare l’inizio e\nla fine di un tratto dell’albero bronchiale.\n\nUn esempio dello scheletro dell’albero ottenuto è in Figura 3.74.\n\nFigura 3.74. Scheletro.\n\nPer la misura del diametro del lume si adopera il metodo della sfera (Figura 3.75), che viene utilizzato nella caratterizzazione di condotti cavi quali i vasi sanguigni. Nel metodo della sfera si considera il diametro del lume pari al diametro della massima sfera con centro situato nel centro di simmetria del vaso (nel nostro caso definito dallo scheletro) inscrivibile nel vaso stesso. In termini algoritmici la sfera è inscrivibile se tutti i suoi voxel appartengono alla maschera del lume.\n\nFigura 3.75. Metodo della sfera.\n\nL’algoritmo per ogni punto della ‘centerline’ prova ad inscrivere sfere di raggio via via maggiore fino a quando la sfera non oltrepassa la maschera del lume. A quel punto memorizza il diametro dell’ultima sfera inscritta.\n\nDefinire la centerline su cui fare la misura per una struttura ramificata come l’albero bronchiale non è banale visto che il numero di percorsi possibili è alto. Di solito si utilizza un approccio simile a quello di Canny per la costruzione dei contorni dalle maschere in cui si tiene conto della distanza dei punti e della direzione di propagazione. Può essere utile la funzione strel che genera un kernel 3D di forma sferica.\n\nNel nostro caso possiamo semplificare il problema partendo da un ramo terminale dello scheletro (coordinata z alta) ed andare ad includere punti dello scheletro andando a cercare in modo iterativo il punto più vicino e cancellando dalla lista i punti con valore della coordinata z minore di quella del punto inserito (Figura 3.76).\n\nFigura 3.76. Stima della centerline.\n\nLa variazione del diametro lungo la centerline partendo dalla trachea dovrebbe risultare un grafico di questo tipo, con un diametro della trachea dell’ordine dei 15-18 mm ed un diametro dei bronchi primari dell’ordine dei 10-12 mm. Un possibile risultato è riportato in Figura 3.77.\n\nFigura 3.77. Risultato.","type":"content","url":"/z-3-4-esercitazione#esercitazione","position":5},{"hierarchy":{"lvl1":"Capitolo 4: Validazione degli Algoritmi di Analisi dell’Immagine Biomedica"},"type":"lvl1","url":"/c14-imm-biom","position":0},{"hierarchy":{"lvl1":"Capitolo 4: Validazione degli Algoritmi di Analisi dell’Immagine Biomedica"},"content":"Un punto fondamentale nello sviluppo di algoritmi per l’analisi dell’immagine biomedica è la validazione di detti algoritmi. Per validazione si intende un processo che assicuri il “buon funzionamento” dell’algoritmo sviluppato nella pratica clinica, limitandosi ovviamente all’insieme di casi per cui l’algoritmo è stato sviluppato. Un buon algoritmo di analisi dell’immagine:\n\nDeve dare risultati corretti, cioè misurare correttamente i parametri oggetto dell’analisi.\n\nDeve dare risultati riproducibili, cioè se applicato più volte allo stesso set di immagini deve dare risultati molto simili tra loro.\n\nPer quanto riguarda il primo punto, nel processo di validazione si confrontano i risultati dell’algoritmo con un “gold standard”, “ground truth”, o “reference standard”, che rappresenta il risultato corretto da raggiungere. Le recenti CLAIM guidelines (\n\nTejani et al. (2024)) sviluppate nel campo dell’intelligenza artificiale applicata alla radiologia suggeriscono fortemente l’uso del termine “reference standard”. La motivazione è che “ground truth” and “gold standard” suggeriscono erroneamente che il riferimento (reference) sia noto con precisione, cosa che in medicina è estremamente raro. Il termine reference standard è quindi più appropriato.\n\nMeno il risultato fornito dall’algoritmo si discosta dal reference standard, migliore è il suo funzionamento. L’errore accettabile dipende dal quesito clinico. Per la misura della temperatura corporea un errore del 10% è incettabile, in quanto a un paziente con temperatura normale (37 C°) verrebbe diagnosticato uno stato patologico (40.7 C°) che richiede l’adozione di terapie salvavita. Nel caso della misura del volume ventricolare sinistro in un paziente maschio (range di normalità 67-155 ml), un errore del 10% in un paziente bordeline (155 ml) porta ad un valore di 170 ml che corrisponde ad una diagnosi di lieve anomalia (range 156-178 ml), che può essere accettabile. Infatti un errore intorno al 10% è considerato accettabile in ecografia cardiaca. Quindi non è possibile dare una soglia accettabile di errore ma questa va stabilita sulla base del quesito clinico al quale la procedura deve rispondere. Tipicamente, come sarà dettagliato nel seguito, un software può essere utilizzato clinicamente se l’errore commesso dal software è comparabile a quello commesso da un operatore esperto seguendo le linee guida approvate.\n\nUn punto fondamentale nella valutazione dell’errore è quindi la definizione del “reference standard”. In teoria il reference standard dovrebbe rappresentare una misura oggettiva e certa della quantità da misurare. Tuttavia, nella maggioranza dei casi detta quantità non sarà nota con precisione ma con un certo errore sperimentale.\n\nUn primo approccio per risolvere il problema è quello di utilizzare per la validazione dell’algoritmo delle immagini ottenuta da oggetti noti, cioè fantocci (phantom), oppure generare immagini sintetiche simili alle immagini reali (fantoccio sintetico o in silico). Nel primo caso le caratteristiche fisiche del fantoccio saranno note dai suoi dati costruttivi, nel secondo dai parametri noti della procedura di simulazione.\nIl limite di questo approccio è evidentemente il fatto che un fantoccio reale o sintetico non sarà in grado di rappresentare pienamente immagini reali. L’uso di fantocci è comunque estremamente utile nella fase di sviluppo dell’algoritmo per affinarne le potenzialità.\n\nNel caso di immagini reali, possiamo utilizzare come riferimento una misura ottenuta con la tecnica che rappresenta il meglio della tecnologia disponibile. Ad esempio, se lo scopo dell’algoritmo sotto esame è diagnosticare l’estensione dell’infarto miocardico, potremmo utilizzare come riferimento una misura istologica dell’estensione dell’infarto sul cuore espiantato (Figura 4.1).\n\nFigura 4.1. Esempio di confronto tra dati MRI (LGE-CMR) e istologia nel topo.\n\nIn Figura 4.1 (Bohl S et al Am J Physiol Heart Circ Physiol. 2009 April; 296(4): H1200–H1208) è mostrato un esempio di confronto tra dati MRI (LGE-CMR) e istologia nel topo.\nQuesto tipo di valutazione comporta una serie di problemi di tipo tecnico, quali la contrarietà del paziente a farsi espiantare l’organo oggetto dell’esame e la difficoltà di individuare una corrispondenza precisa tra localizzazione dei tessuti ex-vivo ed in-vivo. Il suo utilizzo è di solito limitato al modello animale, per quanto siano possibili applicazioni sull’uomo, ad esempio nell’imaging di organi o tessuti che vengono poi rimossi per via chirurgica.\n\nIl reference standard può essere anche rappresentato da una modalità di imaging “invasiva” rispetto ad una modalità di imaging “non invasiva”. Un esempio tipico è la valutazione della stenosi coronarica attraverso misure di perfusione miocardica. Le misure di perfusione rappresentano una valutazione indiretta del rischio coronarico, in quando una coronaria stenotica è associata ad una riduzione del flusso e quindi della perfusione miocardica, soprattutto in condizione di stress indotto. In questo caso i pazienti che vengono valutati con un significativo livello di rischio nella modalità di imaging non invasiva (MRI, SPECT, PET) che valuta la perfusione vengono sottoposti ad angiografia digitale ed eventualmente se la diagnosi è confermata ad angioplastica, ed è quindi disponibile un set di dati comparati dove l’angiografia rappresenta il reference standard.\nInfine, il reference standard può essere rappresentato da una modalità di imaging non invasiva che allo stato dell’arte fornisce la migliore qualità diagnostica. Ad esempio, può essere di interesse valutare la capacità della ecocardiografia di ottenere a costi significativamente minori la stessa qualità diagnostica nella valutazione dei parametri di funzione cardiaca della risonanza magnetica.\nInfine, è possibile prendere come riferimento l’analisi manuale fatta da un operatore esperto, tipicamente un radiologo. In questo scenario, il radiologo effettua un’analisi di tipo manuale ad esempio definendo i contorni di un organo con il mouse su di una workstation. La regione segmentata viene presa come riferimento per la valutazione dell’algoritmo di segmentazione sviluppato. Il confronto con l’analisi manuale è la tecnica di validazione più diffusa per la sua facilità di implementazione, infatti spesso basta usare come riferimento l’analisi manuale fatta a fini diagnostici che è a costo zero. Ha tuttavia due limiti fondamentali:\n\nL’analisi non è veramente manuale, visto che comunque viene utilizzato un software che condiziona l’operato dell’utente esperto;\n\nI risultati dell’analisi manuale, e quindi il reference standard, dipendono dall’operatore.\n\nQuindi, se il reference standard è rappresentato da una segmentazione manuale, esso dipenderà dell’osservatore che compie la misura. Il reference standard avrà quindi una variabilità dovuta alla variabilità inter- ed intra-osservatore. Supponiamo di avere un indice SIM che calcola la similarità (o la differenza) tra due misure. Avendo due osservatori avremo quindi due “reference standard” G_1 e G_2, per i quali sarà definito ad esempio un indice SIM(G_1,G_2), che rappresenta una misura della variabilità inter-osservatore. Confrontando il risultato della misura automatica S con il reference standard avremo due valori SIM(S,G_1) e  SIM(S,G_2) che rappresentano la differenza tra l’algoritmo automatico e i due osservatori umani. L’algoritmo di segmentazione sarà efficace se i tre indici sono tra loro simili, cioè se la procedura automatica non è distinguibile da un osservatore umano. Vedendo la cosa dal punto di vista grafico, considerando l’indice SIM come una distanza spaziale l’algoritmo e i due osservatori devono trovarsi ai vertici di un triangolo equilatero (Figura 4.2).\nLa validazione viene sempre effettuata su base statistica, quindi non su di una sola immagine ma su di una serie di immagini dello stesso tipo, in modo da estendere al valutazione stessa ad un set di immagini rappresentativo della pratica clinica in cui ci si aspetta di utilizzare l’algoritmo.\n\nFigura 4.2. Rappresentazione grafica di un’indice di similarità.","type":"content","url":"/c14-imm-biom","position":1},{"hierarchy":{"lvl1":"Capitolo 4: Validazione degli Algoritmi di Analisi dell’Immagine Biomedica","lvl2":"Valutazione degli algoritmi di segmentazione"},"type":"lvl2","url":"/c14-imm-biom#valutazione-degli-algoritmi-di-segmentazione","position":2},{"hierarchy":{"lvl1":"Capitolo 4: Validazione degli Algoritmi di Analisi dell’Immagine Biomedica","lvl2":"Valutazione degli algoritmi di segmentazione"},"content":"Come detto in precedenza, per valutare l’efficacia di un algoritmo di segmentazione il risultato dell’algoritmo, tipicamente una maschera, viene confrontato con un riferimento “reference standard”. Il reference standard è tipicamente ottenuto attraverso una segmentazione manuale operata da un utente esperto. Date le due maschere da confrontare, è possibile valutare degli indici numerici che misurano la bontà della segmentazione. Gli indici più usati sono l’indice di Dice, introdotto da Lee Raymond Dice nel 1945 nel campo dell’ecologia, e l’indice di Jaccard (1901).Detta S la maschera ottenuta dalla segmentazione e G la maschera che rappresenta il reference standard, l’indice di Jaccard è definito come:J = \\frac{|S \\cap G|}{|S \\cup G|}\n\n|S \\cap G| indica la numerosità dell’intersezione tra le due maschere, ed è quindi uguale al numero di pixel in comune tra le due maschere. È detta anche overlapping area (OA). |S \\cup G| è il numero di pixel comuni ottenuto dall’unione delle due maschere.L’indice di Jaccard è uguale a 1 se le due maschere sono perfettamente coincidenti e 0 se le due maschere sono disgiunte. L’indice può essere naturalmente esteso al caso 3D in modo immediato.\nL’indice di Dice è definito come:DSC = \\frac{2|S \\cap G|}{|S| + |G|}\n\ned è quindi uguale al numero di pixel in comune tra le due maschere, diviso per la media della numerosità delle maschere. Anche in questo caso si ha DSC=1 per la perfetta segmentazione e DCS=0 per OA=0. La relazione tra i due indici è definita come:J = \\frac{DSC}{2-DSC}DSC = \\frac{2J}{1+J}\n\nI due indici si possono anche esprimere in termini delle quattro categorie:\n\nTP/VP (Veri Positivi). \tUn pixel è incluso sia in S che in G\n\nTN/VN (Veri negativi). \tUn pixel non è incluso sia in S che in $G\n\nFP/FP (Falsi Positivi). \tUn pixel è incluso in S ma non in G\n\nFP/FN (Falsi Negativi). \tUn pixel non è incluso in S ma è incluso in GJ = \\frac{TP}{TP+FP+FN}DSC = \\frac{2TP}{2TP+FP+FN}\n\nCome illustrato in Figura 4.3, l’indice di Jaccard è più “severo” rispetto all’indice di Dice, in pratica l’indice di Dice assegna un valore doppio ai pixel True Positive (TP) dove l’algoritmo sotto esame ed il Gold Standard coincidono. La cosa ha senso perché l’utente tipicamente concentra la sua attenzione sull’oggetto da segmentare e non sullo sfondo.\n\nFigura 4.3. Confronto tra l’indice di Jaccard e l’indice di Dice.\n\nGli indici così definiti sono validi per una singola maschera. Se l’obiettivo della segmentazione è individuare più pattern, è possibile utilizzare l’object-level DICE/Jaccard index. Supponiamo di voler segmentare N_G oggetti di cui sono note le maschere “gold standard” G_k \\quad (k=1,...,N_G) e che l’algoritmo di segmentazione restituisca N_S maschere S_q \\quad (q=1,...,N_S). Per ogni S_i definiamo G_i come la maschera appartenente all’insieme G di tutte le maschere gold standard che ha il massimo overlap con S_i.  In pratica G_i è la maschera gold standard che assomiglia di più a S_i. Definiamo poi in modo similare \\hat{S}_i  come l’elemento di S che ha il massimo overlap con \\hat{G}_i. Si ha quindi:DICE_{object}(G,S) = \\frac{1}{2} \\left[ \\sum_{i=1}^{N_S} \\omega_i DICE(G_i, S_i) + \\sum_{i=1}^{N_G} \\varphi_i DICE(\\hat{G}_i, \\hat{S}_i) \\right]\n\nL’indice DICE object-level è ottenuto come media della somma pesata degli indici DICE tra gli oggetti segmentati e gli oggetti gold standard più somiglianti e la somma pesata degli indici DICE tra gli oggetti gold standard e gli oggetti segmentati più somiglianti. Si noti che se N_S=N_G , cioè se l’algoritmo di segmentazione riconosce il giusto numero di oggetti, la formula diventa:DICE_{object}(G,S) = \\frac{1}{2} \\left[ \\sum_{i=1}^{N_S} \\omega_i DICE(G_i, S_i) + \\sum_{i=1}^{N_G} \\varphi_i DICE(\\hat{G}_i, \\hat{S}_i) \\right]\n\ni pesi \\omega _i vengono calcolati come:\\omega_i = \\frac{|S_i|}{|S|} \\qquad \\varphi_i = \\frac{|\\widehat{G}_i|}{|G|}\n\nCioè il peso è il numero di pixel dell’oggetto diviso il totale dei pixel da segmentare, in modo da pesare maggiormente l’indice DICE degli oggetti più grandi. La formulazione per J è analoga.\nIn MATLAB gli indici di Dice e Jaccard possono essere calcolati con le funzioni dice e jaccard.\nIn python il calcolo degli indici può essere fatto con le funzioni dice e jaccard nel pacchetto scipy.spatial.distance.\n\nPer una dimostrazione delle funzioni Python: \n\ndice​-jaccard​-demonstration​.ipynb\n\nIn alternativa al confronto delle maschere, è possibile confrontare i contorni delle regioni segmentate. In questo caso, detto CS il contorno prodotto dall’algoritmo di segmentazione e CG il contorno preso come gold standard, si definiscono la precisione (P, precision) e il richiamo (R, recall) come:P = \\frac{1}{|C_S|} \\sum_{i=1}^{N_S} \\llbracket d(C_S(i), C_G) < \\vartheta \\rrbracket, \\quad R = \\frac{1}{|C_G|} \\sum_{i=1}^{N_G} \\llbracket d(C_G(i), C_S) < \\vartheta \\rrbracket\n\ndove d() è la distanza euclidea tra un punto i del contorno (ad esempio C_S) e l’altro contorno (ad esempio C_G), intesa come il minimo delle distanze tra il punto C_S(i) e tutti i punti di C_G.  \\vartheta è un valore di soglia, pari allo 0.75\\% della diagonale dell’immagine nella formulazione originale dell’algoritmo (Csurka et al Proceedings of the British Machine Vision Conference, 2013, pp. 32.1-32.11). In pratica P è la percentuale di punti del contorno segmentato con una distanza dal gold standard minore di \\vartheta, mentre R è la percentuale di punti del contorno gold standard con una distanza dal contorno segmentato minore di \\vartheta. La misura F_1 che definisce la simiglianza dei due contorni è:F_1 = \\frac{2PR}{P + R}\n\nQuesta metrica detta BF (Boundary F_1) contour matching score è implementata in MATLAB dalla funzione bfscore. L’implementazione python è disponibile a \n\nhttps://​github​.com​/minar09​/bfscore​_python.\n\nCome si vedrà in seguito, gli indici di DICE, Jaccard e F_1 trovano uso negli algoritmi di machine learning supervisionati come funzioni loss e come indici di valutazione della bontà del modello sviluppato.","type":"content","url":"/c14-imm-biom#valutazione-degli-algoritmi-di-segmentazione","position":3},{"hierarchy":{"lvl1":"Capitolo 4: Validazione degli Algoritmi di Analisi dell’Immagine Biomedica","lvl2":"Misure statistiche di precisione e riproducibilità"},"type":"lvl2","url":"/c14-imm-biom#misure-statistiche-di-precisione-e-riproducibilit","position":4},{"hierarchy":{"lvl1":"Capitolo 4: Validazione degli Algoritmi di Analisi dell’Immagine Biomedica","lvl2":"Misure statistiche di precisione e riproducibilità"},"content":"Gli indici definiti in precedenza valutano la qualità della segmentazione, che però nella pratica clinica rappresenta quasi sempre uno step intermedio per valutare un indice diagnostico. Ad esempio la segmentazione del ventricolo sinistro del cuore da immagini US, MR o CT permette di valutare il volume ventricolare in diastole e sistole e quindi la frazione di eiezione (EF) che misura l’efficienza della pompa cardiaca. E’ quindi necessario valutare l’efficacia dell’algoritmo di segmentazione nel misurare questi indici.\nCome detto in precedenza, la valutazione di un algoritmo di segmentazione si basa su prove ripetute su di un certo numero di immagini considerate rappresentative delle immagini su cui verrà utilizzato l’algoritmo. Supponiamo di voler valutare la congruenza di due metodi nell’analisi di un certo tipo di immagini. Prendiamo K immagini e utilizziamo il metodo 1 per estrarre dalle K immagini K indici ed un secondo metodo per effettuare la stessa procedura senza conoscere i risultati del primo metodo. Avremo una prima serie di indici x_1,x_2,...,x_K ed una seconda serie y_1,y_2,...,y_K. In teoria, gli indici dovrebbero essere uguali a coppie (x_1=y_1, x_2=y_2,...).\nI due metodi possono corrispondere a due operatori umani, ed in questo caso il confronto delle misure fornisce una stima della variabilità (o riproducibilità) inter-osservatore. Se i due metodi rappresentano le misure fatte in tempi diversi dallo stesso osservatore umano viene stimata la variabilità intra-osservatore. Se il confronto è tra un algoritmo ed un reference standard il confronto delle misure fornisce una stima della correttezza o precisione dell’algoritmo.\n\nOsservatore 1 vs Osservatore 1\t--> Riproducibilità Intra-osservatore\n\nOsservatore 1 vs Osservatore 2\t--> Riproducibilità Inter-osservatore\n\nAlgoritmo vs Osservatore\t--> Precisione dell’algoritmo\n\nAlgoritmo (test 1) vs Algoritmo (test 2)\t-- > Riproducibilità dell’algoritmo\n\nSe il funzionamento dell’algoritmo dipende da un input dell’osservatore (algoritmo semi-automatico) o da fattori casuali (condizioni iniziali random) si definisce la riproducibilità dell’algoritmo confrontando i risultati di due o più test successivi sugli stessi dati di ingresso. Un algoritmo totalmente automatico (unsupervised), cioè che non dipende in nessun modo dall’input dell’utente può essere perfettamente riproducibile.\nPer confrontare le misure si possono utilizzare diversi indici.\n\nIl primo indice utilizzato è il cosiddetto Coefficente di Variazione (Coefficient of Variation, CoV o CV). Il CoV valuta quanto pesa percentualmente la differenza tra due misure ed è misurato come la deviazione standard del vettore degli errori percentuali assoluti sulle singole misure.Valuteremo quindi l’errore percentuale assoluto come epi = 100*|x_i-y_i|/MEAN(x_i,y_i) e poi calcoleremo la deviazione standard del vettore epi. MEAN(x_i,y_i) rappresenta una stima del valore “vero” dell’indice che viene misurato. Se una delle due misure è un reference standard, al posto della media delle misure avremo la misura effettuata con il reference standard che si considera quella più vicina al valore “vero”. Una CoV< 10\\% è di solito considerata accettabile in campo medico.\nQuesto tipo di misura ci fornisce l’errore percentuale atteso che commette un osservatore o un algoritmo quando esamina due volte la stessa immagine. Non ci dice però quanto è grave l’errore da un punto di vista diagnostico, in quanto ad esempio un errore del 10% può essere trascurabile su di un indice che varia del 100% tra un caso normale ed uno patologico mentre è grave se la variazione è ad esempio del 20%. Inoltre la CoV non tiene conto della differenza tra errore casuale ed errore sistematico. Per errore sistematico intendiamo il fatto che uno dei due operatori sottostimi o sovrastimi sempre l’indice estratto. L’errore sistematico è meno grave dell’errore casuale, in quanto mantiene l’ordine degli indici estratti che è un fattore diagnostico importante.\n\nUn altro indice spesso utilizzato nella comparazione di indici diagnostici è la correlazione tra le due misure. Ipotizziamo che esista una relazione lineare tra le osservazioni X=(x_1,x_2,...,x_K) e Y=(y_1,y_2,...,yK)$.Avremo quindi:y = a + bx_i + \\epsilon _i\n\nQuindi la variabile dipendente y viene “spiegata” attraverso una relazione lineare della variabile indipendente x (cioè: a+bx) e da una quantità casuale \\epsilon _i  che rappresenta il rumore della misura. L’indice a rappresenta il fatto che un operatore aggiunge una certa quantità rispetto all’altro, il fattore b il fatto che un operatore sovrastima l’indice in una certa misura. Il ben noto problema della regressione si traduce nella determinazione di a e b in modo da esprimere al ‘meglio’ la relazione funzionale tra Y e X. Tipicamente si calcolano i coefficienti a e b secondo il metodo dei minimi quadrati utilizzando opportuni software statistici. E’ possibile estendere il metodo a più variabili (caso multivariato).\n\nSi deriva quindi una retta che interpola uno scatter di punti minimizzando la somma dei quadrati delle distanze  \\epsilon _i dei punti stessi dalla retta (Figura 4.4).\n\n*Figura 4.4. Esempio di retta di regressione. *\n\nIn questo esempio il confronto è tra due osservatori, uno umano (manual) ed uno automatico (auto). Il fatto che le misure siano vicine alla retta di regressione da una informazione visuale sul fatto che i due osservatori forniscono risultati simili. In questo caso la misura manuale funge da “gold standard”. La variabilità inter- e intra-osservatore si calcolano in modo analogo. Chiaramente oltre alla rappresentazione grafica è utile avere un indice numerico della bontà del fitting. Il coefficiente di determinazione (R^2), è una misura della bontà dell’adattamento (in inglese fitting) della regressione lineare stimata ai dati osservati. R^2 misura la frazione della variabilità delle osservazioni yi che siamo in grado di spiegare tramite il modello lineare. Bisogna notare che R^2 non misura se effettivamente sussista una relazione (di qualsiasi tipo) tra le y_i e i regressori, ma soltanto fino a che punto un modello lineare consente di approssimare la realtà dei dati osservati; un modello non lineare, ad esempio, potrebbe meglio rappresentare la relazione tra variabile dipendente e regressori, e presentare un buon potere esplicativo, anche in presenza di un R^2 prossimo allo zero. Nella retta di regressione in Figura 4.4 ad esempio r=0.93. Un altro parametro importante è la pendenza della retta b, che idealmente deve essere uguale ad 1. Se b è diverso da uno, uno dei due metodi sottostimerà o sovrastimerà la misura rispetto all’altro. L’intercetta a dovrebbe essere nulla, se non lo è esiste una differenza sistematica tra le misure.\n\nUn’altra rappresentazione grafica molto popolare è il cosiddetto Bland-Altman plot. In Figura 4.5 vediamo il grafico per le due serie di dati per cui era stata tracciata la retta di regressione. Nel Bland-Altmann plot, in ascisse abbiamo la media delle due misure, ed in ordinata la differenza delle stesse. La media delle differenze è riportata come una riga continua, e permette di stimare se una delle due metodiche sottostima o sovrastima l’indice rispetto all’altra (bias). In assenza di bias, la riga continua dovrebbe coincidere con lo zero dell’asse y. In questo caso vediamo che la metodica automatica da una stima inferiore rispetto a quella manuale. Le due righe indicate con 1.96SD e -1.96SD sono ottenute calcolando la deviazione standard della differenza tra le misure. Se la distribuzione delle differenze è gaussiana il 95\\% dei dati cade nell’area tra le due linee. In questo caso (errore gaussiano) ci aspettiamo che l’errore cada nell’intervallo di confidenza. La distanza tra le due linee da una misura della congruenza delle due misure fatta la correzione per l’errore sistematico. Minore è la distanza tra le due linee, migliore è la concordanza tra le due misure.\n\nFigura 4.5. Bland-Altman plot.\n\nIl Bland-Altman plot può essere usato solo per confrontare misure della stessa natura. Nel caso il confronto sia fatto con un reference standard, sull’asse x viene riportato il reference standard e non la stima fatta con la media delle misure.","type":"content","url":"/c14-imm-biom#misure-statistiche-di-precisione-e-riproducibilit","position":5},{"hierarchy":{"lvl1":"Capitolo 4: Validazione degli Algoritmi di Analisi dell’Immagine Biomedica","lvl2":"Test di significatività"},"type":"lvl2","url":"/c14-imm-biom#test-di-significativit","position":6},{"hierarchy":{"lvl1":"Capitolo 4: Validazione degli Algoritmi di Analisi dell’Immagine Biomedica","lvl2":"Test di significatività"},"content":"Di larghissimo uso nella valutazione delle metodiche per l’estrazione di indici clinici è l’indice p, che rappresenta la probabilità di accettazione dell’ipotesi nulla, e cioè che non esista alcuna differenza tra le due misure riguardo al parametro considerato. In altre parole, secondo l’ipotesi nulla le due misure sono fra loro uguali e le eventuali differenze osservate vanno attribuite al solo caso. Si comprende l’importanza da un punto di vista diagnostico di questa misura, in quando due misure statisticamente uguali possono essere usate in modo interscambiabile nella valutazione clinica.\nL’ipotesi zero può essere accettata o respinta applicando un test statistico di significatività, il cui risultato - in genere - va confrontato con un valore critico tabulato in apposite tabelle. Se il risultato del test di significatività supera il valore critico, allora la differenza fra i gruppi viene dichiarata statisticamente significativa e, quindi, l’ipotesi zero viene respinta. In caso contrario l’ipotesi zero viene accettata. Questo fatto viene espresso dal parametro p che esprime la probabilità che l’ipotesi zero sia falsa. Se p è molto basso (valori tipici sono p<0.01, p<0.05) si può concludere che i due gruppi sono equivalenti. Nel caso delle misure di riproducibilità e di validazione, l’ipotesi zero sarà che la differenza tra gli indici misurati dai due osservatori sia dovuta solo al caso, e questa ipotesi sarà confermata da un p<0.05 o p<0.01 a seconda della soglia scelta.\nNumerosi test statistici vengono usati per determinare con un certo grado di probabilità l’esistenza (o l’assenza) di differenze significative nei dati in esame o meglio, più in generale, di accettare o rigettare una ipotesi zero.\n\nUn test molto usato è il paired-t-test, che si applica quando la misura è effettuata da due osservatori sulla stessa immagine (o sullo stesso paziente). Il paired-t-test valuta se la differenza tra le due misure è statisticamente significativa rispetto al valore della misura stessa. I casi visti fino a ora (valutazione vs. gold standard e valutazione dell’intra- e inter-observer variability) possono essere oggetto di una analisi con il paired-t-test.\n\nIn Figura 4.6 i risultati di un paired-t-test su due set di dati (misura della variabilità tra due programmi software per la misura dell’accumulo di ferro nel paziente talassemico).\n\nFigura 4.6. Esempio di analisi tramite paired t-test.\n\nLe misure sono state effettuate su un set di 20 immagini con i due programmi, quindi viene applicato il paired-t-test. Nei pazienti senza accumulo p=0.0776, quindi non c’è differenza significativa tra le due misure. Notiamo che il paired-t-test è molto sensibile, in quanto una differenza di 0.55 su valori medi di 38.5 (1.5%) rischia di creare una significatività. Questo perché il test è sensibile all’errore sistematico. Per confronto la correlazione lineare tra gli stessi dati da r=0.98, \\quad p< 0.0001. Nei pazienti con accumulo severo c’è differenza significativa (p<0.0001).","type":"content","url":"/c14-imm-biom#test-di-significativit","position":7},{"hierarchy":{"lvl1":"Capitolo 4: Validazione degli Algoritmi di Analisi dell’Immagine Biomedica","lvl2":"Misura dell’efficacia diagnostica"},"type":"lvl2","url":"/c14-imm-biom#misura-dellefficacia-diagnostica","position":8},{"hierarchy":{"lvl1":"Capitolo 4: Validazione degli Algoritmi di Analisi dell’Immagine Biomedica","lvl2":"Misura dell’efficacia diagnostica"},"content":"La validità di un metodo di analisi può anche essere valutata in base all’efficacia nell’identificare una malattia. In questo caso avremo K immagini, acquisite da K pazienti di cui un certo numero M presentano una certa malattia e gli altri (S) sono sani. L’operatore esaminerà le varie immagini e fornirà un giudizio binario sulla presenza o meno della malattia. Avremo quattro possibilità:\n\nVP (Veri Positivi). \tViene identificata la malattia ed il soggetto è effettivamente malato.\n\nVN (Veri negativi). \tNon viene identificata la malattia ed il soggetto è sano.\n\nFP (Falsi Positivi). \tViene identificata la malattia ma il soggetto è sano.\n\nFN (Falsi Negativi). \tNon viene identificata la malattia ma il soggetto è malato.\n\nO come in Figura 4.7, attraverso una confusion matrix. In questo esempio abbimo VP=100, VN=80, FP=21, FN=11.\n\nFigura 4.7. Confusion matrix.\n\nNel caso ideale dovremmo avere VP=M, VN=S, FN=FP=0.\nLa rappresentazione tabellare è anche detta “confusion matrix” o “matrice di confusione” ed è correntemente usata per la valutazione di decisori sviluppati con tecniche AI di machine learning.\n\nSi definiscono:Sensitività = \\frac{VP}{VP+FN}\n\nE’ la capacità del metodo di scoprire la patologia. In altri termini è la percentuale di diagnosi giuste fatte sui soggetti malati.Specificità = \\frac{VN}{VN+FP}\n\nE’ la capacità dell’immagine di fare una diagnosi corretta sui pazienti sani. Infine:Accuratezza = \\frac{VN+VP}{VN+VP+FN+FP}\n\nDescrive il numero di diagnosi corrette.\n\nUn modo alternativo di visualizzare queste misure sono le curve ROC (Figura 4.8). Una curva ROC è una curva che lega la sensitività alla specificità in base ad una soglia scelta per l’indice considerato.\n\nFigura 4.8. Curva ROC.\n\nPer tracciare una curva ROC prenderemo un vettore X=(x_1,x_2,...,x_N) continuo. Ad esempio x potrebbe essere una misura di perfusione nel miocardio. Avremo poi un indice I=(i_1,i_2,...,i_N) binario, che indica se il soggetto ha o non ha una certa patologia (ad esempio una stenosi coronarica). Facciamo variare una soglia T e definiamo patologici i soggetti con X<T e sani quelli con X>T. Avremo dei valori di sensitività e specificità che riporteremo nel grafico tracciando la curva ROC. Notiamo che avremo sempre un punto nell’origine, corrispondente a sensitività 0 e specificità 1. Questo punto equivale ad un osservatore che risponde sempre N_O. L’osservatore che risponde sempre S_I corrisponde al punto in alto a destra. Tutti i punti della curva corrispondono ad un compromesso tra sensitività e specificità, il punto migliore in generale dipende dal quesito clinico e dal rapporto costo/beneficio. E’ possibile comunque calcolare il punto con la massima accuratezza. Una curva corrispondente alla diagonale indica che l’indice non dà informazioni sulla patologia. L’area della curva può essere usato come fattore di merito per la metodologia, e quindi in questo caso per il metodo di analisi dell’immagine.Anche in questo caso il ground truth, cioè le diagnosi sano/malato potranno essere affette da variabilità inter- ed intra-osservatore. I valori di accuratezza, sensitività e specificità ottenuti da un software automatico dovranno quindi essere confrontati con la riproducibilità del riferimento.","type":"content","url":"/c14-imm-biom#misura-dellefficacia-diagnostica","position":9},{"hierarchy":{"lvl1":"Capitolo 4: Validazione degli Algoritmi di Analisi dell’Immagine Biomedica","lvl2":"Valutazione di un software ad uso diagnostico"},"type":"lvl2","url":"/c14-imm-biom#valutazione-di-un-software-ad-uso-diagnostico","position":10},{"hierarchy":{"lvl1":"Capitolo 4: Validazione degli Algoritmi di Analisi dell’Immagine Biomedica","lvl2":"Valutazione di un software ad uso diagnostico"},"content":"Come detto in precedenza, la validazione di un metodo di analisi automatico o semi-automatico comporta il confronto delle misure ottenute con il metodo in oggetto con un “gold standard” clinico, tipicamente la segmentazione manuale effettuata da un operatore esperto. Un altro punto importante nella valutazione di un software di analisi dell’immagine biomedica è la misura della riproducibilità, cioè della capacità del software di assicurare un bassa variabilità inter- e intra-osservatore. Se il software è completamente automatico la variabilità inter e intra-osservatore è zero, a meno della possibile variabilità indotta dalla componente random dell’algoritmo, ad esempio a causa dell’uso di un algoritmo di clustering.\n\nCome esempio consideriamo il programma HIPPO FAT per la valutazione del grasso addominale. Il programma permette di misurare in modo automatico i valori di grasso viscerale (VAT) e subcutaneo (SAT) da immagini di risonanza T1-pesate (Positano V, JMRI 2004). Per la validazione sono stati selezionati 20 pazienti con vario livello di obesità. Per ogni paziente sono state acquisite 32 slice assiali con una sequenza pesata T1. I volumi di SAT e VAT sono stati valutati con il software e manualmente da un operatore esperto. In tutto quindi sono state valutate 640 immagini (32x20). L’analisi con Bland-Altman plot (Figura 4.9) mostra come la correlazione tra misura automatica e manuale sia buona. La correlazione risulta r=0.99, \\quad p<0.0001 per il SAT e r=0.96, \\quad p<0.0001 per il VAT.\n\nFigura 4.9. Bland-Altman plot della stima dei volumi di SAT e VAT.\n\nDai BA plot si nota come ci sia una leggera sovrastima del SAT (intorno al 6%). Questo può essere dovuto all’inclusione errata di parti di VAT da parte dello snake interno o al fatto che lo snake esterno converge leggermente all’esterno del SAT per lo smoothing dei contorni introdotto dall’edge map. Per il VAT abbiamo una sottostima intorno al 8% ma una maggiore ampiezza dell’intervallo di confidenza, che conferma come la misura del VAT sia più critica. La sottostima è ragionevolmente dovuta al fatto che l’algoritmo automatico non tiene conto dell’effetto volume parziale, in quanto il fitting dell’istogramma viene effettuato con una gaussiana. Il problema potrebbe essere risolto introducendo l’effetto PV nel modello di fitting.\nUna cosa positiva è che l’errore non è dipendente dal valore del VAT o del SAT.\nLa validazione effettuata presenta alcuni limiti intrinseci:\n\nÈ effettuata dagli autori del metodo, che possono avere un bias nella valutazione\n\nÈ limitata ad immagini acquisite in un singolo centro e con una singola macchina MR, non c’è garanzia che i risultati ottenuti siano validi su altri tipi di immagini.\n\nÈ quindi opportuno procedere ad una valutazione che sia indipendente, cioè effettuata da valutatori esterni, e inter-centro, cioè che coinvolga immagini acquisite in centri diversi e con apparecchiature diverse. Una validazione di questo tipo è richiesta per la certificazione del software per uso clinico.\n\nEsistono varie validazioni indipendenti del software.  In quella eseguita presso la Washington University School of Medicine, St. Louis, Missouri, USA  sono state esaminate le immagini di 10 soggetti, analizzate con HIPPO FAT e ANALYZE (Figura 4.10). Le immagini sono state acquisite con scanner Siemens. ANALYZE è un tool generico per l’analisi delle immagini biomediche che permette di definire manualmente le regioni di interesse e di effettuare all’interno delle stesse una segmentazione a soglia modificando la soglia stessa e osservando la maschera risultante. L’analisi è stata effettuata da due operatori per valutare la variabilità inter- e intra-osservatore. Se necessario, i risultati della segmentazione tramite HIPPO sono stati corretti manualmente.\n\nFigura 4.10. Confronto HIPPO vs ANALYZE (1).\n\nUn primo set di risultati mostra la variabilità inter-osservatore (qui chiamata  inter-rater agreement) per i due software, valutata con l’ICC (inter-class correlation coefficient). L’ICC sostanzialmente misura la correlazione tra l’ordinamento delle misure fatto dai due osservatori, piuttosto che la correlazione tra le misure stesse. Un ICC=1 indica che l’ordinamento delle misure è identico. Vediamo come l’ICC sia abbastanza alto (> 0.9) in tutti i casi tranne la misura del VAT con ANALYZE, mentre l’ICC dell’HIPPO FAT nella valutazione del VAT è molto vicino ad uno. Questo è un risultato abbastanza generale, nel senso che tipicamente un programma con un livello alto di automazione riduce la variabilità inter e intra osservatore, in quanto funge da “guida” per l’operatore. Un programma manuale lascia più libero l’operatore e quindi aumenta la variabilità tra operatori.\nViene poi valutata la riproducibilità intra-operatore (intra rater agreement), che risulta sempre elevata (ICC>0.98). Anche questo è un risultato atteso, in quanto la variabilità intra-osservatore è tipicamente minore della variabilità inter-osservatore (Figura 4.11).\n\nFigura 4.11. Confronto HIPPO vs ANALYZE (2).\n\nInfine viene valutata la concordanza tra i due software. Dalla tabella si vede come per il SAT e per il VAT l’HIPPO presenti una sottostima rispetto ad ANALYZE del 10% circa. Le misure effettuate con i due software non sono comunque diverse da un punto di vista statistico.Il risultato di questo studio indipendente conferma la valutazione precedente per il VAT, mentre da un risultato di segno opposto per il SAT.  Un risultato importante è che il tempo di analisi con HIPPO FAT è la metà di quello richiesto da ANALYZE, cosa che porta gli autori a suggerirne l’uso.\n\nUn altro studio dell’università dell’Ohio, USA è stato condotto su 40 pazienti . Oltre al numero di pazienti più elevato, che assicura una migliore potenza statistica, è interessante notare che il confronto è fatto con il programma Slice-O-Matic, uno strumento commerciale che utilizza un algoritmo di tipo region growing. Il programma Slice-O-Matic è stato validato per la misura del grasso addominale in molti studi anche rispetto a campioni istologici, e rappresenta quindi un ottimo “gold standard”.\nCome si osserva dalla figura, i risultati dello studio confermano quelli della validazione con una leggera sovrastima del SAT (4%) e una sottostima più rilevante del VAT (9.4%). Anche qui il tempo di analisi è circa la metà di quello richiesto utilizzando Slice-O-Matic (Figura 4.12).\n\nFigura 4.12. Confronto HIPPO vs ANALYZE (3).\n\nInfine uno studio della Johns Hopkins University di Baltimora (USA)   confronta cinque software per la misura del grasso viscerale: NIHImage, slice-O-matic, Analyze, Philips Easy Vision e Hippo Fat.\n\nNIHImage è il precursore di imagej, il software di analisi dell’immagine sviluppato presso NIH (National Institute of Health) degli USA, Philips Easy Vision è un software proprietario installato nelle workstation Philips, tipicamente disponibili insieme agli scanner MR Philips.\nAbbastanza interessante è la tabella comparativa tra i software (Figura 4.13), che rappresenta un buon esempio dei parametri da tenere in conto nella valutazione clinica di questo tipo di strumenti. Vediamo i principali:\n\nFigura 4.13. Confronto fra diversi software.\n\nAvalaibility: Qui viene riportato il costo del software. L’uso dell’Easy Vision è limitato agli utenti di uno scanner Philips. SliceOMatic e Analyze sono software commerciali. Gli ultimi due sono gratuiti per uso di ricerca. Importante anche valutare i sistemi operativi supportati, in particolare Easy Vision è in bundle con la workstation Philips, gli altri sono multi piattaforma con l’eccezione di HIHimage (in realtà la versione corrente imagej è multipiattaforma) e SliceOMatic. Questa ultima informazione è importante per valutare l’uso del software nella configurazione della rete informatica disponibile in un data centro. Viene anche valutata la facilità di installazione.\n\nDocumentation: Viene poi valutata la qualità della manualistica e la facilità di apprendimento del software. Questi parametri sono importanti per definire la curva di apprendimento, cioè il tempo previsto per un uso a regime del software nel centro.\n\nData Input: ad oggi è necessaria e sufficiente la compatibilità DICOM, qui si poteva utilmente aggiungere la presenza o meno di un DICOM conformance statement. Software più “antichi” accedono ad altri formati in uso precedente al DICOM.\n\nAverage time: un altro parametro importante è il tempo di analisi richiesto. Questo permette di valutare il costo in ore/uomo per il centro per analizzare i dati.\n\nAl solito viene poi valutata la riproducibilità inter-osservatore (Figura 4.14) e inter-software (Figura 4.15).\n\nFigura 4.14. Variabilità inter-osservatore.\n\nFigura 4.15. Variabilità inter-software.\n\nQui l’ICC risulta elevato per tutti i software. Per quanto riguarda il SAT i risultati dei software sono sovrapponibili. Per quanto riguarda il VAT NIHImage, SliceOmatic e Analyze danno risultati simili, mentre HippoFat e EasyVision divergono.  Probabilmente questo è dovuto all’approccio simile per la segmentazione del VAT seguito da  NIHImage, SliceOmatic e Analyze che utilizzano un approccio di tipo topologico mentre HippoFat usa un approccio basato sull’analisi dell’istogramma.\n\nIl caso di studio esaminato permette di trarre alcune conclusioni di carattere generale. La valutazione di un pacchetto software per l’analisi di immagini biomediche deve basarsi su una serie di considerazioni, che possono essere schematizzate come segue.\n\nAccessibilità al software. Si valuta la possibilità di utilizzare il software presso il proprio centro. I parametri di interesse sono:1a. Costo (Compatibilità economica). Il costo deve comprendere il costo di acquisto e quello stimato per gli aggiornamenti periodici nel tempo.1b. Compatibilità software. Compatibilità del software con l’hardware e il/i sistemi operativi disponibili nel centro. Tempo di installazione.1c. Supporto DICOM. Certificato da conformance statement.\n\nUsabilità. La possibilità di utilizzare in modo proficuo il software. Questo comprende: 2a. Curva di apprendimento: il tempo che un utente medio impiega a divenire produttivo nell’uso del software. Dipende dalla semplicità d’uso del software, dalla qualità della manualistica, dal servizio di assistenza (helpdesk, corsi di formazione, etc).2b. Tempo di analisi delle immagini a regime.\n\nRiproducibilità.  La capacità del software di assicurare risultati riproducibili in modo indipendente dall’operatore. Valutata attraverso la variabilità inter- e intra- osservatore.\n\nValidazione clinica. La capacità del software di produrre risultati corretti. Si valuta attraverso gli studi di validazione disponibili. I parametri di interesse sono:4a. Numero di pazienti e range di patologie esaminate. 4b. Indipendenza della validazione4c. Validazione multi-centro\n\nCertificazione.  Un punto importante che esula dalla componente tecnica è la certificazione del software come dispositivo medico. Un software utilizzato in ambito clinico deve essere a norma di legge certificato da un ente certificatore. Nella concessione della certificazione i punti fondamentali sono la validazione clinica e la presenza di una manualistica adeguata. Inoltre è necessario un documento di gestione del rischio che contempli gli eventuali malfunzionamenti del software e le loro conseguenze.\n\nPer quanto riguarda la certificazione, il software per l’analisi di immagini mediche è a tutti gli effetti un “dispositivo medico”, e rientra quindi nella normativa corrente che riguarda questi dispositivi. Tale normativa impone al fabbricante del software (cioè a chi commercializza o distribuisce il software) di provvedere ad una particolare marcatura CE, specifica per i dispositivi medici, che deve essere rilasciata da un ente certificato. Il software se è integrato con un dispositivo medico (ad esempio un ecografo o uno scanner MR) di cui è parte imprescindibile viene certificato insieme al dispositivo.  Il software “stand-alone” viene classificato come “dispositivo medico attivo” in quanto il suo funzionamento dipende da una fonte di energia esterna. La certificazione richiede una complessa serie di attività da parte del produttore:\n\nStabilire se il software è o meno un dispositivo medico (per essere un dispositivo medico il software deve produrre informazioni diagnostiche. Una cartella clinica ad esempio non è di per se un dispositivo medico. La semplice visualizzazione di immagini rende invece un software un dispositivo medico.\n\nIndividuare la destinazione d’uso (quindi quando è appropriato usare il software). Il produttore dovrà quindi individuare ad esempio i valori limite di qualità dell’immagine che permettono l’uso del software stesso.\n\nAttribuire la classe di rischio del dispositivo, in base al danno che un malfunzionamento del software può produrre sul paziente o sugli operatori. La classe di rischio viene assegnata sulla base di tabelle prefissate. Le tre classi principali sono A: nessun danno possibile; B: lesioni non gravi; C: morte o lesioni gravi.\n\nProgettare e fabbricare il dispositivo secondo i parametri stabiliti dalla normativa. Sostanzialmente questo punto implica l’adozione di un controllo interno di qualità secondo le specifiche ISO e la predisposizione di un fascicolo tecnico che documenti la struttura del software.\n\nAttivare una procedura di valutazione di conformità in base alla classe di rischio individuata.\n\nDefinire le istruzioni per l’installazione del software e gli eventuali requisiti di competenza /qualifica dell’installatore (manuale di installazione).\n\nDefinire e documentare le istruzioni per l’utilizzo del software (manuale d’uso del software).\n\nDefinire le procedure per la manutenzione del software (aggiornamenti, etc).\n\nArif, Hassan, Susan B. Racette, Dennis T. Villareal, John O. Holloszy, And Edward P. Weiss. Comparison of methods for assessing abdominal adipose tissue from magnetic resonance images. Obesity. 2007;15:2240 –2244.\n\nDemerath EW, Ritter KJ, Couch WA, et al. Validity of a new automated software program for visceral adipose tissue estimation. Int J Obes (Lond). 2007;31:285–91\n\nBonekamp S, Ghosh P, Crawford S, et al. Quantitative comparison and evaluation of software packages for assessment of abdominal adipose tissue distribution by magnetic resonance imaging. Int J Obes (Lond) 2008;32:100–111.","type":"content","url":"/c14-imm-biom#valutazione-di-un-software-ad-uso-diagnostico","position":11},{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/dice-jaccard-demonstration","position":0},{"hierarchy":{"lvl1":""},"content":"Esempio di calcolo degli indici di Jaccard e Dice\n\nImportiamo le librerie necessarie\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import jaccard_score\nfrom scipy.spatial.distance import dice\n\n\n\nCreiamo due maschere parzialmente sovrapposte\n\ndim=256  # image dimension\n\n# create masks\nA = np.zeros([dim,dim])  # background images\nB = np.zeros([dim,dim])\ncenter_mask_A = [int(dim/2),int(dim/2)] # center of A mask\ndim_mask_A = [int(dim/4),int(dim/4)]            # dimension of A mask\nA[center_mask_A[0]-dim_mask_A[0]:center_mask_A[0]+dim_mask_A[0],center_mask_A[1]-dim_mask_A[1]:center_mask_A[1]+dim_mask_A[1]]=1\n\nshift_mask=20  # shift between masks\ncenter_mask_B = [int(dim/2)+shift_mask,int(dim/2)+shift_mask] # center of B mask\ndim_mask_B=dim_mask_A # dimension of B mask\nB[center_mask_B[0]-dim_mask_B[0]:center_mask_B[0]+dim_mask_B[0],center_mask_B[1]-dim_mask_B[1]:center_mask_B[1]+dim_mask_B[1]]=1\n\n\n\nVisualizziamo le maschere risultanti, l’overlapping area dovrebbe apparire in bianco (tutti i canali uguali)\n\nfig = plt.figure(figsize=(10,4)) \naxs0 = fig.add_subplot(1, 3, 1)\naxs1 = fig.add_subplot(1, 3, 2)\naxs2 = fig.add_subplot(1, 3, 3)\nimgplot = axs0.imshow(A,cmap='gray')\naxs0.set_title(\"Image A\",fontsize=12)\naxs0.set_axis_off()\nimgplot = axs1.imshow(B,cmap='gray')\naxs1.set_title(\"Image B\",fontsize=12)\naxs1.set_axis_off()\nRGB_image=np.zeros([dim,dim,3])  # create RGB image\nRGB_image[:,:,0]=A\nRGB_image[:,:,1]=B\nRGB_image[:,:,2]=B\nimgplot = axs2.imshow(RGB_image,cmap='viridis')\naxs2.set_title(\"Image RGB\",fontsize=12)\naxs2.set_axis_off()\n\n\n\nCalcoliamo la sovrapposizione tra le maschere (overlapping area)\n\noverlap_id = np.array(np.where((A ==1)&(B==1)))\noverlap=len(overlap_id[0])\nprint('Overlap area = ',overlap,' (number of pixels)')\n\n\n\nCalcoliamo l’indice di Jaccard\n\nAB_mask_id = np.array(np.where((A ==1) | (B==1)))\nAB_mask_size=len(AB_mask_id[0])\nJaccard_index = overlap/AB_mask_size\nprint('Jaccard index = ',Jaccard_index,' (number of pixels)')\n\n\n\nCalcoliamo l’indice di Jaccard attraverso la libreria sklearn\n\nJaccard_lib=jaccard_score(A,B,average=\"micro\")\nprint('Jaccard index (library) = ',Jaccard_lib,' (number of pixels)')\n\n\n\nCalcoliamo l’indice di Dice\n\nA_mask_id = np.array(np.where(A ==1))\nA_mask_size=len(A_mask_id[0])\nB_mask_id = np.array(np.where(B ==1))\nB_mask_size=len(B_mask_id[0])\nDice_index=2*overlap/(A_mask_size+B_mask_size)\nprint('Dice index = ',Dice_index,' (number of pixels)')\nDice_lib1 = 2*Jaccard_lib / (1 + Jaccard_lib)\nDice_lib2 = 1-dice(A.flatten(),B.flatten())\nprint('Dice index (library) = ',Dice_lib1,Dice_lib2,' (number of pixels)')\n\n\n","type":"content","url":"/dice-jaccard-demonstration","position":1},{"hierarchy":{"lvl1":"Elaborazione delle Bioimmagini"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Elaborazione delle Bioimmagini"},"content":"Benvenuto!\n\nUsa il menu a sinistra per navigare i capitoli.","type":"content","url":"/","position":1}]}